{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep - Invoice Entity\n",
    "I will compose a view from the raw dataset that is a summary of the multiple characteristics of a single invoice. Thus, we will have the `invoice_no` as the primary key of this dataset.\n",
    "\n",
    "Throughout this notebook, I will use both Spark's standard API and the `SQLContext` to illustrate when the SQLContext can make grouping structures more readable and portable. Another good reason for using it is that it can be more easily communicated to business and less technical stakeholders that are more used to SQL syntax.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install inflection nb_black >> ../configs/package_installation.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"# loading magic commands:\\n%load_ext nb_black\\n%load_ext autoreload\\n%autoreload 2\";\n",
       "                var nbb_formatted_code = \"# loading magic commands:\\n%load_ext nb_black\\n%load_ext autoreload\\n%autoreload 2\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loading magic commands:\n",
    "%load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# PySpark dependencies:\\nimport pyspark\\nfrom pyspark import SparkConf\\nfrom pyspark import SparkContext\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql import SQLContext\\nimport pyspark.sql.functions as F\\nfrom pyspark.sql.functions import udf\\nfrom pyspark.sql.types import *\\nfrom pyspark.sql.window import Window\\n\\n# database utilities\\nimport pandas as pd\\n\\n# other relevant libraries:\\nimport warnings\\nimport inflection\\nimport unicodedata\\nfrom datetime import datetime, timedelta\\nimport json \\nimport re\\nimport os\\nfrom glob import glob\\nimport shutil\\nimport itertools\\n\\n# setting global parameters for visualizations:\\nwarnings.filterwarnings('ignore')\\npd.set_option(\\\"display.precision\\\", 4)\\npd.set_option(\\\"display.float_format\\\", lambda x: '%.2f' % x)\";\n",
       "                var nbb_formatted_code = \"# PySpark dependencies:\\nimport pyspark\\nfrom pyspark import SparkConf\\nfrom pyspark import SparkContext\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql import SQLContext\\nimport pyspark.sql.functions as F\\nfrom pyspark.sql.functions import udf\\nfrom pyspark.sql.types import *\\nfrom pyspark.sql.window import Window\\n\\n# database utilities\\nimport pandas as pd\\n\\n# other relevant libraries:\\nimport warnings\\nimport inflection\\nimport unicodedata\\nfrom datetime import datetime, timedelta\\nimport json\\nimport re\\nimport os\\nfrom glob import glob\\nimport shutil\\nimport itertools\\n\\n# setting global parameters for visualizations:\\nwarnings.filterwarnings(\\\"ignore\\\")\\npd.set_option(\\\"display.precision\\\", 4)\\npd.set_option(\\\"display.float_format\\\", lambda x: \\\"%.2f\\\" % x)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PySpark dependencies:\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# database utilities\n",
    "import pandas as pd\n",
    "\n",
    "# other relevant libraries:\n",
    "import warnings\n",
    "import inflection\n",
    "import unicodedata\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from glob import glob\n",
    "import shutil\n",
    "import itertools\n",
    "\n",
    "# setting global parameters for visualizations:\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "pd.set_option(\"display.float_format\", lambda x: \"%.2f\" % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Configuring Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# loading the configurations needed for Spark\\ndef init_spark(app_name):\\n\\n    spark = (\\n        SparkSession.builder.appName(app_name)\\n        .config(\\\"spark.files.overwrite\\\", \\\"true\\\")\\n        .config(\\\"spark.sql.repl.eagerEval.enabled\\\", True)\\n        .config(\\\"spark.sql.repl.eagerEval.maxNumRows\\\", 5)\\n        .config(\\\"spark.sql.legacy.timeParserPolicy\\\", \\\"LEGACY\\\")\\n        .config(\\\"spark.sql.parquet.compression.codec\\\", \\\"gzip\\\")\\n        .enableHiveSupport()\\n        .getOrCreate()\\n    )\\n\\n    return spark\\n\\n\\n# init the spark session:\\nspark = init_spark(\\\"Invoice Preparation\\\")\";\n",
       "                var nbb_formatted_code = \"# loading the configurations needed for Spark\\ndef init_spark(app_name):\\n\\n    spark = (\\n        SparkSession.builder.appName(app_name)\\n        .config(\\\"spark.files.overwrite\\\", \\\"true\\\")\\n        .config(\\\"spark.sql.repl.eagerEval.enabled\\\", True)\\n        .config(\\\"spark.sql.repl.eagerEval.maxNumRows\\\", 5)\\n        .config(\\\"spark.sql.legacy.timeParserPolicy\\\", \\\"LEGACY\\\")\\n        .config(\\\"spark.sql.parquet.compression.codec\\\", \\\"gzip\\\")\\n        .enableHiveSupport()\\n        .getOrCreate()\\n    )\\n\\n    return spark\\n\\n\\n# init the spark session:\\nspark = init_spark(\\\"Invoice Preparation\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loading the configurations needed for Spark\n",
    "def init_spark(app_name):\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder.appName(app_name)\n",
    "        .config(\"spark.files.overwrite\", \"true\")\n",
    "        .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "        .config(\"spark.sql.repl.eagerEval.maxNumRows\", 5)\n",
    "        .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "        .config(\"spark.sql.parquet.compression.codec\", \"gzip\")\n",
    "        .enableHiveSupport()\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "    return spark\n",
    "\n",
    "\n",
    "# init the spark session:\n",
    "spark = init_spark(\"Invoice Preparation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://84a2e6cc83b5:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Invoice Preparation</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f33fd248250>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# verifying the spark session:\\nspark\";\n",
       "                var nbb_formatted_code = \"# verifying the spark session:\\nspark\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# verifying the spark session:\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"def save_to_filesystem(df, target_path, parquet_path, filename):\\n    \\\"\\\"\\\"Helper function to save pyspark dataframes as parquets in a way that is similar to writing to local files\\n\\n    Args:\\n        df (pyspark.sql.dataframe.DataFrame): dataframe to be saved\\n        target_path (str): path that will store the file\\n        filename (str): name of the resulting file\\n\\n    Returns:\\n        None\\n    \\\"\\\"\\\"\\n    PARQUET_FILE = f\\\"{target_path}/{parquet_path}\\\"\\n    OUTPUT_FILE = f\\\"{target_path}/{filename}\\\"\\n\\n    if os.path.exists(PARQUET_FILE):\\n        shutil.rmtree(\\n            PARQUET_FILE\\n        )  # if the directory already exists, remove it (throws error if not)\\n\\n    # saves the dataframe:\\n    df.coalesce(1).write.save(PARQUET_FILE)\\n\\n    # retrieves file resulting from the saving procedure:\\n    original_file = glob(f\\\"{PARQUET_FILE}/*.parquet\\\")[0]\\n\\n    # renames the resulting file and saves it to the target directory:\\n    os.rename(original_file, OUTPUT_FILE)\\n\\n    shutil.rmtree(PARQUET_FILE)\\n\\n    return True\\n\\n\\ndef apply_category_map(category_map):\\n    \\\"\\\"\\\"Helper function to convert strings given a map\\n\\n    Note:\\n        This function uses the function generator scheme, much like the PySpark code\\n\\n    Args:\\n        original_category (str): the original category name\\n        category_map (dict): the hash table or dictionary for converting the values:\\n\\n    Returns:\\n        new_category (str): the resulting category\\n\\n    \\\"\\\"\\\"\\n\\n    def func(row):\\n        try:\\n            result = category_map[row]\\n        except:\\n            result = None\\n        return result\\n\\n    return F.udf(func)\\n\\n\\ndef get_datetime_features(df, time_col):\\n    \\\"\\\"\\\"Function to extract time-based features from pyspark dataframes\\n\\n    Args:\\n        df (pyspark.sql.dataframe.DataFrame): the original dataframe that needs to be enriched\\n        time_col (str): the string name of the column containing the date object\\n\\n    Returns:\\n        df (pyspark.sql.dataframe.DataFrame): resulting pyspark dataframe with the added features\\n            -> See list of attribute the source code for the attributes\\n\\n    \\\"\\\"\\\"\\n\\n    # applying date-related functions:\\n\\n    # day-level attributes:\\n    df = df.withColumn(\\\"day_of_week\\\", F.dayofweek(F.col(time_col)))\\n\\n    df = df.withColumn(\\\"day_of_month\\\", F.dayofmonth(F.col(time_col)))\\n\\n    df = df.withColumn(\\\"day_of_year\\\", F.dayofyear(F.col(time_col)))\\n\\n    # week-level attributes:\\n    df = df.withColumn(\\\"week_of_year\\\", F.weekofyear(F.col(time_col)))\\n\\n    # month-level attributes:\\n    df = df.withColumn(\\\"month\\\", F.month(F.col(time_col)))\\n\\n    df = df.withColumn(\\\"quarter\\\", F.quarter(F.col(time_col)))\\n\\n    # year-level attributes:\\n    df = df.withColumn(\\\"year\\\", F.year(F.col(time_col)))\\n\\n    return df\\n\\n\\ndef bulk_aggregate(df, group_col, aggs, target_cols):\\n    \\\"\\\"\\\"Wrapper function to apply multiple aggregations when performing group bys\\n\\n    It utilizes the spark's SQL Context and string interpolation to perform the aggregation using SQL syntax.\\n\\n    Args:\\n        df (pyspark.sql.dataframe.DataFrame): dataframe with raw data\\n        group_col (str): the column that will be used for grouping\\n        aggs (list): list of aggregations that want to be made (must be the same name as pyspark.sql.functions)\\n        target_cols (str): columns in which aggregations will be performed\\n\\n    Returns:\\n        df_grouped (pyspark.sql.dataframe.DataFrame): dataframe with the grouped data\\n    \\\"\\\"\\\"\\n\\n    # buils the cartersian product of the lists\\n    aggs_to_perform = itertools.product(aggs, target_cols)\\n\\n    Q_LAYOUT = \\\"\\\"\\\"\\n    SELECT\\n        {},\\n        {}\\n        FROM df\\n        GROUP BY {}\\n    \\\"\\\"\\\"\\n\\n    aggregations = []\\n    for agg, col in aggs_to_perform:\\n\\n        # builds the string for aggregation\\n        statement = f\\\"{agg.upper()}({col}) as {agg}_{col}\\\"\\n        aggregations.append(statement)\\n\\n    full_statement = \\\",\\\\n\\\".join(aggregations)\\n\\n    # uses string interpolation to build the full query statement\\n    QUERY = Q_LAYOUT.format(group_col, full_statement, group_col)\\n\\n    # registers the dataframe as temporary table:\\n    df.registerTempTable(\\\"df\\\")\\n    df_grouped = spark.sql(QUERY)\\n\\n    # rounds values:\\n    for column in df_grouped.columns:\\n        df_grouped = df_grouped.withColumn(column, F.round(F.col(column), 1))\\n\\n    return df_grouped\\n\\n\\n######### Text Processing Functions ########\\n@udf(\\\"string\\\")\\ndef normalize_text(text):\\n    \\\"\\\"\\\"Helper function to normalize text data to ASCII and lower case, removing spaces\\n\\n    Args:\\n        text (string): the string that needs to be normalized\\n\\n    Returns:\\n        text (string): cleaned up string\\n\\n    \\\"\\\"\\\"\\n    regex = r\\\"[^a-zA-Z0-9]+\\\"\\n\\n    if text is not None:\\n\\n        text = str(text)\\n        text = text.lower()\\n        text = re.sub(regex, \\\" \\\", text)\\n        text = text.strip()\\n        text = str(\\n            unicodedata.normalize(\\\"NFKD\\\", text).encode(\\\"ASCII\\\", \\\"ignore\\\"), \\\"utf-8\\\"\\n        )\\n\\n    return text\\n\\n\\ndef get_null_columns(df, normalize=False):\\n    \\\"\\\"\\\"Helper function to print the number of null records for each column of a PySpark DataFrame.\\n\\n    Args:\\n        df (pyspark.sql.dataframe.DataFrame): a PySpark Dataframe object\\n\\n    Returns:\\n        None -> prints to standard out\\n\\n    \\\"\\\"\\\"\\n\\n    if normalize:\\n        total = df.count()\\n\\n        df_nulls = df.select(\\n            [\\n                (F.sum(F.when(F.col(column).isNull(), 1).otherwise(0)) / total).alias(\\n                    column\\n                )\\n                for column in df.columns\\n            ]\\n        )\\n\\n    else:\\n        df_nulls = df.select(\\n            [\\n                F.sum(F.when(F.col(column).isNull(), 1).otherwise(0)).alias(column)\\n                for column in df.columns\\n            ]\\n        )\\n\\n    # displaying the results to standard out\\n    df_nulls.show(1, truncate=False, vertical=True)\\n\\n\\n@udf(\\\"boolean\\\")\\ndef is_set_or_pack(text):\\n\\n    # description entries to match:\\n    set_descriptions = {\\\"set\\\", \\\"set of\\\", \\\"pack\\\", \\\"pack of\\\", \\\"box\\\", \\\"box of\\\"}\\n\\n    if text is not None:\\n        text = str(text)\\n\\n        if text in set_descriptions:\\n            return True\\n\\n        else:\\n            return False\\n\\n    else:\\n        return False\\n\\n\\n@udf(\\\"integer\\\")\\ndef get_unit_size(text):\\n\\n    if text is not None:\\n        check_if_digit = len(re.findall(r\\\"(\\\\d+)\\\", text)) > 0\\n\\n        if check_if_digit:\\n            set_size = int(re.findall(r\\\"(\\\\d+)\\\", text)[0])\\n            return set_size\\n\\n        else:\\n            return 1\\n\\n    else:\\n        return 1\\n\\n\\n@udf(\\\"boolean\\\")\\ndef has_non_digits_only(text):\\n    \\\"\\\"\\\"Function to match entries in the dataset that are purely non-digit characters\\n\\n    Args:\\n        text (str): string containing the invoice code\\n\\n    Returns:\\n        boolean: whether the text contains non-digit characters and is not related to cancellations\\n\\n    \\\"\\\"\\\"\\n\\n    if text is not None:\\n        condition = all(character.isalpha() for character in text)\\n\\n        if condition:\\n            return True\\n\\n        else:\\n            return False\\n\\n    else:\\n        return False\";\n",
       "                var nbb_formatted_code = \"def save_to_filesystem(df, target_path, parquet_path, filename):\\n    \\\"\\\"\\\"Helper function to save pyspark dataframes as parquets in a way that is similar to writing to local files\\n\\n    Args:\\n        df (pyspark.sql.dataframe.DataFrame): dataframe to be saved\\n        target_path (str): path that will store the file\\n        filename (str): name of the resulting file\\n\\n    Returns:\\n        None\\n    \\\"\\\"\\\"\\n    PARQUET_FILE = f\\\"{target_path}/{parquet_path}\\\"\\n    OUTPUT_FILE = f\\\"{target_path}/{filename}\\\"\\n\\n    if os.path.exists(PARQUET_FILE):\\n        shutil.rmtree(\\n            PARQUET_FILE\\n        )  # if the directory already exists, remove it (throws error if not)\\n\\n    # saves the dataframe:\\n    df.coalesce(1).write.save(PARQUET_FILE)\\n\\n    # retrieves file resulting from the saving procedure:\\n    original_file = glob(f\\\"{PARQUET_FILE}/*.parquet\\\")[0]\\n\\n    # renames the resulting file and saves it to the target directory:\\n    os.rename(original_file, OUTPUT_FILE)\\n\\n    shutil.rmtree(PARQUET_FILE)\\n\\n    return True\\n\\n\\ndef apply_category_map(category_map):\\n    \\\"\\\"\\\"Helper function to convert strings given a map\\n\\n    Note:\\n        This function uses the function generator scheme, much like the PySpark code\\n\\n    Args:\\n        original_category (str): the original category name\\n        category_map (dict): the hash table or dictionary for converting the values:\\n\\n    Returns:\\n        new_category (str): the resulting category\\n\\n    \\\"\\\"\\\"\\n\\n    def func(row):\\n        try:\\n            result = category_map[row]\\n        except:\\n            result = None\\n        return result\\n\\n    return F.udf(func)\\n\\n\\ndef get_datetime_features(df, time_col):\\n    \\\"\\\"\\\"Function to extract time-based features from pyspark dataframes\\n\\n    Args:\\n        df (pyspark.sql.dataframe.DataFrame): the original dataframe that needs to be enriched\\n        time_col (str): the string name of the column containing the date object\\n\\n    Returns:\\n        df (pyspark.sql.dataframe.DataFrame): resulting pyspark dataframe with the added features\\n            -> See list of attribute the source code for the attributes\\n\\n    \\\"\\\"\\\"\\n\\n    # applying date-related functions:\\n\\n    # day-level attributes:\\n    df = df.withColumn(\\\"day_of_week\\\", F.dayofweek(F.col(time_col)))\\n\\n    df = df.withColumn(\\\"day_of_month\\\", F.dayofmonth(F.col(time_col)))\\n\\n    df = df.withColumn(\\\"day_of_year\\\", F.dayofyear(F.col(time_col)))\\n\\n    # week-level attributes:\\n    df = df.withColumn(\\\"week_of_year\\\", F.weekofyear(F.col(time_col)))\\n\\n    # month-level attributes:\\n    df = df.withColumn(\\\"month\\\", F.month(F.col(time_col)))\\n\\n    df = df.withColumn(\\\"quarter\\\", F.quarter(F.col(time_col)))\\n\\n    # year-level attributes:\\n    df = df.withColumn(\\\"year\\\", F.year(F.col(time_col)))\\n\\n    return df\\n\\n\\ndef bulk_aggregate(df, group_col, aggs, target_cols):\\n    \\\"\\\"\\\"Wrapper function to apply multiple aggregations when performing group bys\\n\\n    It utilizes the spark's SQL Context and string interpolation to perform the aggregation using SQL syntax.\\n\\n    Args:\\n        df (pyspark.sql.dataframe.DataFrame): dataframe with raw data\\n        group_col (str): the column that will be used for grouping\\n        aggs (list): list of aggregations that want to be made (must be the same name as pyspark.sql.functions)\\n        target_cols (str): columns in which aggregations will be performed\\n\\n    Returns:\\n        df_grouped (pyspark.sql.dataframe.DataFrame): dataframe with the grouped data\\n    \\\"\\\"\\\"\\n\\n    # buils the cartersian product of the lists\\n    aggs_to_perform = itertools.product(aggs, target_cols)\\n\\n    Q_LAYOUT = \\\"\\\"\\\"\\n    SELECT\\n        {},\\n        {}\\n        FROM df\\n        GROUP BY {}\\n    \\\"\\\"\\\"\\n\\n    aggregations = []\\n    for agg, col in aggs_to_perform:\\n\\n        # builds the string for aggregation\\n        statement = f\\\"{agg.upper()}({col}) as {agg}_{col}\\\"\\n        aggregations.append(statement)\\n\\n    full_statement = \\\",\\\\n\\\".join(aggregations)\\n\\n    # uses string interpolation to build the full query statement\\n    QUERY = Q_LAYOUT.format(group_col, full_statement, group_col)\\n\\n    # registers the dataframe as temporary table:\\n    df.registerTempTable(\\\"df\\\")\\n    df_grouped = spark.sql(QUERY)\\n\\n    # rounds values:\\n    for column in df_grouped.columns:\\n        df_grouped = df_grouped.withColumn(column, F.round(F.col(column), 1))\\n\\n    return df_grouped\\n\\n\\n######### Text Processing Functions ########\\n@udf(\\\"string\\\")\\ndef normalize_text(text):\\n    \\\"\\\"\\\"Helper function to normalize text data to ASCII and lower case, removing spaces\\n\\n    Args:\\n        text (string): the string that needs to be normalized\\n\\n    Returns:\\n        text (string): cleaned up string\\n\\n    \\\"\\\"\\\"\\n    regex = r\\\"[^a-zA-Z0-9]+\\\"\\n\\n    if text is not None:\\n\\n        text = str(text)\\n        text = text.lower()\\n        text = re.sub(regex, \\\" \\\", text)\\n        text = text.strip()\\n        text = str(\\n            unicodedata.normalize(\\\"NFKD\\\", text).encode(\\\"ASCII\\\", \\\"ignore\\\"), \\\"utf-8\\\"\\n        )\\n\\n    return text\\n\\n\\ndef get_null_columns(df, normalize=False):\\n    \\\"\\\"\\\"Helper function to print the number of null records for each column of a PySpark DataFrame.\\n\\n    Args:\\n        df (pyspark.sql.dataframe.DataFrame): a PySpark Dataframe object\\n\\n    Returns:\\n        None -> prints to standard out\\n\\n    \\\"\\\"\\\"\\n\\n    if normalize:\\n        total = df.count()\\n\\n        df_nulls = df.select(\\n            [\\n                (F.sum(F.when(F.col(column).isNull(), 1).otherwise(0)) / total).alias(\\n                    column\\n                )\\n                for column in df.columns\\n            ]\\n        )\\n\\n    else:\\n        df_nulls = df.select(\\n            [\\n                F.sum(F.when(F.col(column).isNull(), 1).otherwise(0)).alias(column)\\n                for column in df.columns\\n            ]\\n        )\\n\\n    # displaying the results to standard out\\n    df_nulls.show(1, truncate=False, vertical=True)\\n\\n\\n@udf(\\\"boolean\\\")\\ndef is_set_or_pack(text):\\n\\n    # description entries to match:\\n    set_descriptions = {\\\"set\\\", \\\"set of\\\", \\\"pack\\\", \\\"pack of\\\", \\\"box\\\", \\\"box of\\\"}\\n\\n    if text is not None:\\n        text = str(text)\\n\\n        if text in set_descriptions:\\n            return True\\n\\n        else:\\n            return False\\n\\n    else:\\n        return False\\n\\n\\n@udf(\\\"integer\\\")\\ndef get_unit_size(text):\\n\\n    if text is not None:\\n        check_if_digit = len(re.findall(r\\\"(\\\\d+)\\\", text)) > 0\\n\\n        if check_if_digit:\\n            set_size = int(re.findall(r\\\"(\\\\d+)\\\", text)[0])\\n            return set_size\\n\\n        else:\\n            return 1\\n\\n    else:\\n        return 1\\n\\n\\n@udf(\\\"boolean\\\")\\ndef has_non_digits_only(text):\\n    \\\"\\\"\\\"Function to match entries in the dataset that are purely non-digit characters\\n\\n    Args:\\n        text (str): string containing the invoice code\\n\\n    Returns:\\n        boolean: whether the text contains non-digit characters and is not related to cancellations\\n\\n    \\\"\\\"\\\"\\n\\n    if text is not None:\\n        condition = all(character.isalpha() for character in text)\\n\\n        if condition:\\n            return True\\n\\n        else:\\n            return False\\n\\n    else:\\n        return False\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def save_to_filesystem(df, target_path, parquet_path, filename):\n",
    "    \"\"\"Helper function to save pyspark dataframes as parquets in a way that is similar to writing to local files\n",
    "\n",
    "    Args:\n",
    "        df (pyspark.sql.dataframe.DataFrame): dataframe to be saved\n",
    "        target_path (str): path that will store the file\n",
    "        filename (str): name of the resulting file\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    PARQUET_FILE = f\"{target_path}/{parquet_path}\"\n",
    "    OUTPUT_FILE = f\"{target_path}/{filename}\"\n",
    "\n",
    "    if os.path.exists(PARQUET_FILE):\n",
    "        shutil.rmtree(\n",
    "            PARQUET_FILE\n",
    "        )  # if the directory already exists, remove it (throws error if not)\n",
    "\n",
    "    # saves the dataframe:\n",
    "    df.coalesce(1).write.save(PARQUET_FILE)\n",
    "\n",
    "    # retrieves file resulting from the saving procedure:\n",
    "    original_file = glob(f\"{PARQUET_FILE}/*.parquet\")[0]\n",
    "\n",
    "    # renames the resulting file and saves it to the target directory:\n",
    "    os.rename(original_file, OUTPUT_FILE)\n",
    "\n",
    "    shutil.rmtree(PARQUET_FILE)\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def apply_category_map(category_map):\n",
    "    \"\"\"Helper function to convert strings given a map\n",
    "\n",
    "    Note:\n",
    "        This function uses the function generator scheme, much like the PySpark code\n",
    "\n",
    "    Args:\n",
    "        original_category (str): the original category name\n",
    "        category_map (dict): the hash table or dictionary for converting the values:\n",
    "\n",
    "    Returns:\n",
    "        new_category (str): the resulting category\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def func(row):\n",
    "        try:\n",
    "            result = category_map[row]\n",
    "        except:\n",
    "            result = None\n",
    "        return result\n",
    "\n",
    "    return F.udf(func)\n",
    "\n",
    "\n",
    "def get_datetime_features(df, time_col):\n",
    "    \"\"\"Function to extract time-based features from pyspark dataframes\n",
    "\n",
    "    Args:\n",
    "        df (pyspark.sql.dataframe.DataFrame): the original dataframe that needs to be enriched\n",
    "        time_col (str): the string name of the column containing the date object\n",
    "\n",
    "    Returns:\n",
    "        df (pyspark.sql.dataframe.DataFrame): resulting pyspark dataframe with the added features\n",
    "            -> See list of attribute the source code for the attributes\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # applying date-related functions:\n",
    "\n",
    "    # day-level attributes:\n",
    "    df = df.withColumn(\"day_of_week\", F.dayofweek(F.col(time_col)))\n",
    "\n",
    "    df = df.withColumn(\"day_of_month\", F.dayofmonth(F.col(time_col)))\n",
    "\n",
    "    df = df.withColumn(\"day_of_year\", F.dayofyear(F.col(time_col)))\n",
    "\n",
    "    # week-level attributes:\n",
    "    df = df.withColumn(\"week_of_year\", F.weekofyear(F.col(time_col)))\n",
    "\n",
    "    # month-level attributes:\n",
    "    df = df.withColumn(\"month\", F.month(F.col(time_col)))\n",
    "\n",
    "    df = df.withColumn(\"quarter\", F.quarter(F.col(time_col)))\n",
    "\n",
    "    # year-level attributes:\n",
    "    df = df.withColumn(\"year\", F.year(F.col(time_col)))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def bulk_aggregate(df, group_col, aggs, target_cols):\n",
    "    \"\"\"Wrapper function to apply multiple aggregations when performing group bys\n",
    "\n",
    "    It utilizes the spark's SQL Context and string interpolation to perform the aggregation using SQL syntax.\n",
    "\n",
    "    Args:\n",
    "        df (pyspark.sql.dataframe.DataFrame): dataframe with raw data\n",
    "        group_col (str): the column that will be used for grouping\n",
    "        aggs (list): list of aggregations that want to be made (must be the same name as pyspark.sql.functions)\n",
    "        target_cols (str): columns in which aggregations will be performed\n",
    "\n",
    "    Returns:\n",
    "        df_grouped (pyspark.sql.dataframe.DataFrame): dataframe with the grouped data\n",
    "    \"\"\"\n",
    "\n",
    "    # buils the cartersian product of the lists\n",
    "    aggs_to_perform = itertools.product(aggs, target_cols)\n",
    "\n",
    "    Q_LAYOUT = \"\"\"\n",
    "    SELECT\n",
    "        {},\n",
    "        {}\n",
    "        FROM df\n",
    "        GROUP BY {}\n",
    "    \"\"\"\n",
    "\n",
    "    aggregations = []\n",
    "    for agg, col in aggs_to_perform:\n",
    "\n",
    "        # builds the string for aggregation\n",
    "        statement = f\"{agg.upper()}({col}) as {agg}_{col}\"\n",
    "        aggregations.append(statement)\n",
    "\n",
    "    full_statement = \",\\n\".join(aggregations)\n",
    "\n",
    "    # uses string interpolation to build the full query statement\n",
    "    QUERY = Q_LAYOUT.format(group_col, full_statement, group_col)\n",
    "\n",
    "    # registers the dataframe as temporary table:\n",
    "    df.registerTempTable(\"df\")\n",
    "    df_grouped = spark.sql(QUERY)\n",
    "\n",
    "    # rounds values:\n",
    "    for column in df_grouped.columns:\n",
    "        df_grouped = df_grouped.withColumn(column, F.round(F.col(column), 1))\n",
    "\n",
    "    return df_grouped\n",
    "\n",
    "\n",
    "######### Text Processing Functions ########\n",
    "@udf(\"string\")\n",
    "def normalize_text(text):\n",
    "    \"\"\"Helper function to normalize text data to ASCII and lower case, removing spaces\n",
    "\n",
    "    Args:\n",
    "        text (string): the string that needs to be normalized\n",
    "\n",
    "    Returns:\n",
    "        text (string): cleaned up string\n",
    "\n",
    "    \"\"\"\n",
    "    regex = r\"[^a-zA-Z0-9]+\"\n",
    "\n",
    "    if text is not None:\n",
    "\n",
    "        text = str(text)\n",
    "        text = text.lower()\n",
    "        text = re.sub(regex, \" \", text)\n",
    "        text = text.strip()\n",
    "        text = str(\n",
    "            unicodedata.normalize(\"NFKD\", text).encode(\"ASCII\", \"ignore\"), \"utf-8\"\n",
    "        )\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_null_columns(df, normalize=False):\n",
    "    \"\"\"Helper function to print the number of null records for each column of a PySpark DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pyspark.sql.dataframe.DataFrame): a PySpark Dataframe object\n",
    "\n",
    "    Returns:\n",
    "        None -> prints to standard out\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if normalize:\n",
    "        total = df.count()\n",
    "\n",
    "        df_nulls = df.select(\n",
    "            [\n",
    "                (F.sum(F.when(F.col(column).isNull(), 1).otherwise(0)) / total).alias(\n",
    "                    column\n",
    "                )\n",
    "                for column in df.columns\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        df_nulls = df.select(\n",
    "            [\n",
    "                F.sum(F.when(F.col(column).isNull(), 1).otherwise(0)).alias(column)\n",
    "                for column in df.columns\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # displaying the results to standard out\n",
    "    df_nulls.show(1, truncate=False, vertical=True)\n",
    "\n",
    "\n",
    "@udf(\"boolean\")\n",
    "def is_set_or_pack(text):\n",
    "\n",
    "    # description entries to match:\n",
    "    set_descriptions = {\"set\", \"set of\", \"pack\", \"pack of\", \"box\", \"box of\"}\n",
    "\n",
    "    if text is not None:\n",
    "        text = str(text)\n",
    "\n",
    "        if text in set_descriptions:\n",
    "            return True\n",
    "\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "@udf(\"integer\")\n",
    "def get_unit_size(text):\n",
    "\n",
    "    if text is not None:\n",
    "        check_if_digit = len(re.findall(r\"(\\d+)\", text)) > 0\n",
    "\n",
    "        if check_if_digit:\n",
    "            set_size = int(re.findall(r\"(\\d+)\", text)[0])\n",
    "            return set_size\n",
    "\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "@udf(\"boolean\")\n",
    "def has_non_digits_only(text):\n",
    "    \"\"\"Function to match entries in the dataset that are purely non-digit characters\n",
    "\n",
    "    Args:\n",
    "        text (str): string containing the invoice code\n",
    "\n",
    "    Returns:\n",
    "        boolean: whether the text contains non-digit characters and is not related to cancellations\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if text is not None:\n",
    "        condition = all(character.isalpha() for character in text)\n",
    "\n",
    "        if condition:\n",
    "            return True\n",
    "\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"# loading the raw dataset:\\ndf_clean = (spark\\n              .read\\n              .parquet('../data/processed/tb_ecommerce.parquet'))\";\n",
       "                var nbb_formatted_code = \"# loading the raw dataset:\\ndf_clean = spark.read.parquet(\\\"../data/processed/tb_ecommerce.parquet\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loading the raw dataset:\n",
    "df_clean = spark.read.parquet(\"../data/processed/tb_ecommerce.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"# instantiating the SQL Context:\\nsql_context = SQLContext(spark.sparkContext)\\n\\ndf_clean.createOrReplaceTempView('tb_ecommerce')\";\n",
       "                var nbb_formatted_code = \"# instantiating the SQL Context:\\nsql_context = SQLContext(spark.sparkContext)\\n\\ndf_clean.createOrReplaceTempView(\\\"tb_ecommerce\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# instantiating the SQL Context:\n",
    "sql_context = SQLContext(spark.sparkContext)\n",
    "\n",
    "df_clean.createOrReplaceTempView(\"tb_ecommerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Implement Invoice features\n",
    "Just like the raw dataset, I have devised a plan for features in the hypothesis map below.\n",
    "\n",
    "<img src=\"../reports/figures/Invoice Entity.png\" alt = \"Invoice Entity Map\" style = \"width:1182px; height=702px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Invoice statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"QUERY_INVOICE_ITEMS = \\\"\\\"\\\"\\nSELECT\\n    -- group unit\\n    invoice_no,\\n    \\n    -- unique information about the invoice\\n    FIRST(customer_id) as customer_id,\\n    FIRST(country) as customer_country,\\n    FIRST(invoice_date) as date,\\n    CONCAT_WS('; ', COLLECT_LIST(description)) as invoice_items,\\n    \\n    -- basket features\\n    SUM(ABS(quantity)) as basket_size,\\n    COUNT(DISTINCT description) as basket_diversity,\\n    \\n    -- payment features\\n    ROUND(SUM(ABS(total_item_price)), 2) as total_paid,\\n    \\n    -- discounted items features\\n    ROUND(AVG(CAST(is_discounted_item as LONG)), 3) as pct_sale_items,\\n    SUM(CAST(is_discounted_item as LONG)) as sale_items,\\n    SUM(CASE WHEN is_discounted_item == true THEN ABS(quantity) ELSE 0 END) as total_quantity_sale,\\n    ROUND(SUM(CASE WHEN is_discounted_item == true THEN ABS(total_item_price) ELSE 0 END), 2) as total_paid_sale,\\n    \\n    -- returned and cancelled item features\\n    SUM(CAST(is_cancelled as LONG)) as cancelled_items,\\n    SUM(CASE WHEN is_cancelled == true THEN ABS(quantity) ELSE 0 END) as total_quantity_cancelled,\\n    ROUND(SUM(CASE WHEN is_cancelled == true THEN ABS(total_item_price) ELSE 0 END), 2) as total_paid_cancelled,\\n    SUM(CAST(is_return as LONG)) as returned_items,\\n    SUM(CASE WHEN is_return == true THEN ABS(quantity) ELSE 0 END) as total_quantity_returned,\\n    ROUND(SUM(CASE WHEN is_return == true THEN ABS(total_item_price) ELSE 0 END), 2) as total_paid_returned,\\n    \\n    -- free items features\\n    ROUND(AVG(CAST(is_free_item as LONG)), 3) as pct_free_items,\\n    SUM(CAST(is_free_item as LONG)) as free_items,\\n    SUM(CASE WHEN is_free_item == true THEN ABS(quantity) ELSE 0 END) as total_quantity_free,\\n    \\n    -- fees, postage and manuals\\n   ROUND(SUM(CASE WHEN is_postage == true THEN ABS(total_item_price) ELSE 0 END), 2) as total_paid_postage,\\n   ROUND(SUM(CASE WHEN is_manual == true THEN ABS(total_item_price) ELSE 0 END), 2) as total_paid_manual,\\n   ROUND(SUM(CASE WHEN is_fee == true THEN ABS(total_item_price) ELSE 0 END), 2) as total_paid_fee,\\n   ROUND(SUM(CASE WHEN is_discount == true THEN ABS(total_item_price) ELSE 0 END), 2) as total_discounts\\n    \\nFROM tb_ecommerce\\nGROUP BY invoice_no\\n\\\"\\\"\\\"\\n\\n# performing the grouping operation\\ndf_invoice = spark.sql(QUERY_INVOICE_ITEMS)\";\n",
       "                var nbb_formatted_code = \"QUERY_INVOICE_ITEMS = \\\"\\\"\\\"\\nSELECT\\n    -- group unit\\n    invoice_no,\\n    \\n    -- unique information about the invoice\\n    FIRST(customer_id) as customer_id,\\n    FIRST(country) as customer_country,\\n    FIRST(invoice_date) as date,\\n    CONCAT_WS('; ', COLLECT_LIST(description)) as invoice_items,\\n    \\n    -- basket features\\n    SUM(ABS(quantity)) as basket_size,\\n    COUNT(DISTINCT description) as basket_diversity,\\n    \\n    -- payment features\\n    ROUND(SUM(ABS(total_item_price)), 2) as total_paid,\\n    \\n    -- discounted items features\\n    ROUND(AVG(CAST(is_discounted_item as LONG)), 3) as pct_sale_items,\\n    SUM(CAST(is_discounted_item as LONG)) as sale_items,\\n    SUM(CASE WHEN is_discounted_item == true THEN ABS(quantity) ELSE 0 END) as total_quantity_sale,\\n    ROUND(SUM(CASE WHEN is_discounted_item == true THEN ABS(total_item_price) ELSE 0 END), 2) as total_paid_sale,\\n    \\n    -- returned and cancelled item features\\n    SUM(CAST(is_cancelled as LONG)) as cancelled_items,\\n    SUM(CASE WHEN is_cancelled == true THEN ABS(quantity) ELSE 0 END) as total_quantity_cancelled,\\n    ROUND(SUM(CASE WHEN is_cancelled == true THEN ABS(total_item_price) ELSE 0 END), 2) as total_paid_cancelled,\\n    SUM(CAST(is_return as LONG)) as returned_items,\\n    SUM(CASE WHEN is_return == true THEN ABS(quantity) ELSE 0 END) as total_quantity_returned,\\n    ROUND(SUM(CASE WHEN is_return == true THEN ABS(total_item_price) ELSE 0 END), 2) as total_paid_returned,\\n    \\n    -- free items features\\n    ROUND(AVG(CAST(is_free_item as LONG)), 3) as pct_free_items,\\n    SUM(CAST(is_free_item as LONG)) as free_items,\\n    SUM(CASE WHEN is_free_item == true THEN ABS(quantity) ELSE 0 END) as total_quantity_free,\\n    \\n    -- fees, postage and manuals\\n   ROUND(SUM(CASE WHEN is_postage == true THEN ABS(total_item_price) ELSE 0 END), 2) as total_paid_postage,\\n   ROUND(SUM(CASE WHEN is_manual == true THEN ABS(total_item_price) ELSE 0 END), 2) as total_paid_manual,\\n   ROUND(SUM(CASE WHEN is_fee == true THEN ABS(total_item_price) ELSE 0 END), 2) as total_paid_fee,\\n   ROUND(SUM(CASE WHEN is_discount == true THEN ABS(total_item_price) ELSE 0 END), 2) as total_discounts\\n    \\nFROM tb_ecommerce\\nGROUP BY invoice_no\\n\\\"\\\"\\\"\\n\\n# performing the grouping operation\\ndf_invoice = spark.sql(QUERY_INVOICE_ITEMS)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "QUERY_INVOICE_ITEMS = \"\"\"\n",
    "SELECT\n",
    "    -- group unit\n",
    "    invoice_no,\n",
    "    \n",
    "    -- unique information about the invoice\n",
    "    FIRST(customer_id) as customer_id,\n",
    "    FIRST(country) as customer_country,\n",
    "    FIRST(invoice_date) as date,\n",
    "    CONCAT_WS('; ', COLLECT_LIST(description)) as invoice_items,\n",
    "    \n",
    "    -- basket features\n",
    "    SUM(ABS(quantity)) as basket_size,\n",
    "    COUNT(DISTINCT description) as basket_diversity,\n",
    "    \n",
    "    -- payment features\n",
    "    ROUND(SUM(ABS(total_item_price)), 2) as total_paid,\n",
    "    \n",
    "    -- discounted items features\n",
    "    ROUND(AVG(CAST(is_discounted_item as LONG)), 3) as pct_sale_items,\n",
    "    SUM(CAST(is_discounted_item as LONG)) as sale_items,\n",
    "    SUM(CASE WHEN is_discounted_item == true THEN ABS(quantity) ELSE 0 END) as total_quantity_sale,\n",
    "    ROUND(SUM(CASE WHEN is_discounted_item == true THEN ABS(total_item_price) ELSE 0 END), 2) as total_paid_sale,\n",
    "    \n",
    "    -- returned and cancelled item features\n",
    "    SUM(CAST(is_cancelled as LONG)) as cancelled_items,\n",
    "    SUM(CASE WHEN is_cancelled == true THEN ABS(quantity) ELSE 0 END) as total_quantity_cancelled,\n",
    "    ROUND(SUM(CASE WHEN is_cancelled == true THEN ABS(total_item_price) ELSE 0 END), 2) as total_paid_cancelled,\n",
    "    SUM(CAST(is_return as LONG)) as returned_items,\n",
    "    SUM(CASE WHEN is_return == true THEN ABS(quantity) ELSE 0 END) as total_quantity_returned,\n",
    "    ROUND(SUM(CASE WHEN is_return == true THEN ABS(total_item_price) ELSE 0 END), 2) as total_paid_returned,\n",
    "    \n",
    "    -- free items features\n",
    "    ROUND(AVG(CAST(is_free_item as LONG)), 3) as pct_free_items,\n",
    "    SUM(CAST(is_free_item as LONG)) as free_items,\n",
    "    SUM(CASE WHEN is_free_item == true THEN ABS(quantity) ELSE 0 END) as total_quantity_free,\n",
    "    \n",
    "    -- fees, postage and manuals\n",
    "   ROUND(SUM(CASE WHEN is_postage == true THEN ABS(total_item_price) ELSE 0 END), 2) as total_paid_postage,\n",
    "   ROUND(SUM(CASE WHEN is_manual == true THEN ABS(total_item_price) ELSE 0 END), 2) as total_paid_manual,\n",
    "   ROUND(SUM(CASE WHEN is_fee == true THEN ABS(total_item_price) ELSE 0 END), 2) as total_paid_fee,\n",
    "   ROUND(SUM(CASE WHEN is_discount == true THEN ABS(total_item_price) ELSE 0 END), 2) as total_discounts\n",
    "    \n",
    "FROM tb_ecommerce\n",
    "GROUP BY invoice_no\n",
    "\"\"\"\n",
    "\n",
    "# performing the grouping operation\n",
    "df_invoice = spark.sql(QUERY_INVOICE_ITEMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, I will introduce few more features related to important products in a certain invoice, such as the most expensive and cheapest item of the invoice, as well as the most representative item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Invoice product features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"Q_ITEMS_INFORMATION = \\\"\\\"\\\"\\nwith expensive as (\\nSELECT \\n    invoice_no,\\n    'product_most_expensive' as window_type,\\n    description\\n    FROM (\\n    SELECT \\n        invoice_no,\\n        description,\\n        ROW_NUMBER() OVER(PARTITION BY invoice_no, description ORDER BY unit_price DESC) as row_idx\\n    FROM tb_ecommerce\\n    ) as temp\\n    WHERE row_idx = 1\\n),\\n\\ncheapest as (\\n\\nSELECT \\n    invoice_no,\\n    'product_cheapest' as window_type,\\n    description\\n    FROM (\\n    SELECT \\n        invoice_no,\\n        description,\\n        ROW_NUMBER() OVER(PARTITION BY invoice_no, description ORDER BY unit_price ASC) as row_idx\\n    FROM tb_ecommerce\\n    ) as temp\\n    WHERE row_idx = 1\\n),\\n\\nmost_spent as (\\n\\nSELECT \\n    invoice_no,\\n    'product_most_spent' as window_type,\\n    description\\n    FROM (\\n    SELECT \\n        invoice_no,\\n        description,\\n        ROW_NUMBER() OVER(PARTITION BY invoice_no, description ORDER BY ABS(total_item_price) DESC) as row_idx\\n    FROM tb_ecommerce\\n    ) as temp\\n    WHERE row_idx = 1\\n),\\n\\nmost_bought as (\\n\\nSELECT \\n    invoice_no,\\n    'product_most_bought' as window_type,\\n    description\\n    FROM (\\n    SELECT \\n        invoice_no,\\n        description,\\n        ROW_NUMBER() OVER(PARTITION BY invoice_no, description ORDER BY ABS(quantity) DESC) as row_idx\\n    FROM tb_ecommerce\\n    ) as temp\\n    WHERE row_idx = 1\\n)\\n\\nSELECT invoice_no, window_type, description FROM expensive\\nUNION\\nSELECT invoice_no, window_type, description FROM cheapest\\nUNION\\nSELECT invoice_no, window_type, description FROM most_spent\\nUNION\\nSELECT invoice_no, window_type, description FROM most_bought\\n\\\"\\\"\\\"\\n\\ndf_items_info = spark.sql(Q_ITEMS_INFORMATION)\";\n",
       "                var nbb_formatted_code = \"Q_ITEMS_INFORMATION = \\\"\\\"\\\"\\nwith expensive as (\\nSELECT \\n    invoice_no,\\n    'product_most_expensive' as window_type,\\n    description\\n    FROM (\\n    SELECT \\n        invoice_no,\\n        description,\\n        ROW_NUMBER() OVER(PARTITION BY invoice_no, description ORDER BY unit_price DESC) as row_idx\\n    FROM tb_ecommerce\\n    ) as temp\\n    WHERE row_idx = 1\\n),\\n\\ncheapest as (\\n\\nSELECT \\n    invoice_no,\\n    'product_cheapest' as window_type,\\n    description\\n    FROM (\\n    SELECT \\n        invoice_no,\\n        description,\\n        ROW_NUMBER() OVER(PARTITION BY invoice_no, description ORDER BY unit_price ASC) as row_idx\\n    FROM tb_ecommerce\\n    ) as temp\\n    WHERE row_idx = 1\\n),\\n\\nmost_spent as (\\n\\nSELECT \\n    invoice_no,\\n    'product_most_spent' as window_type,\\n    description\\n    FROM (\\n    SELECT \\n        invoice_no,\\n        description,\\n        ROW_NUMBER() OVER(PARTITION BY invoice_no, description ORDER BY ABS(total_item_price) DESC) as row_idx\\n    FROM tb_ecommerce\\n    ) as temp\\n    WHERE row_idx = 1\\n),\\n\\nmost_bought as (\\n\\nSELECT \\n    invoice_no,\\n    'product_most_bought' as window_type,\\n    description\\n    FROM (\\n    SELECT \\n        invoice_no,\\n        description,\\n        ROW_NUMBER() OVER(PARTITION BY invoice_no, description ORDER BY ABS(quantity) DESC) as row_idx\\n    FROM tb_ecommerce\\n    ) as temp\\n    WHERE row_idx = 1\\n)\\n\\nSELECT invoice_no, window_type, description FROM expensive\\nUNION\\nSELECT invoice_no, window_type, description FROM cheapest\\nUNION\\nSELECT invoice_no, window_type, description FROM most_spent\\nUNION\\nSELECT invoice_no, window_type, description FROM most_bought\\n\\\"\\\"\\\"\\n\\ndf_items_info = spark.sql(Q_ITEMS_INFORMATION)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Q_ITEMS_INFORMATION = \"\"\"\n",
    "with expensive as (\n",
    "SELECT \n",
    "    invoice_no,\n",
    "    'product_most_expensive' as window_type,\n",
    "    description\n",
    "    FROM (\n",
    "    SELECT \n",
    "        invoice_no,\n",
    "        description,\n",
    "        ROW_NUMBER() OVER(PARTITION BY invoice_no, description ORDER BY unit_price DESC) as row_idx\n",
    "    FROM tb_ecommerce\n",
    "    ) as temp\n",
    "    WHERE row_idx = 1\n",
    "),\n",
    "\n",
    "cheapest as (\n",
    "\n",
    "SELECT \n",
    "    invoice_no,\n",
    "    'product_cheapest' as window_type,\n",
    "    description\n",
    "    FROM (\n",
    "    SELECT \n",
    "        invoice_no,\n",
    "        description,\n",
    "        ROW_NUMBER() OVER(PARTITION BY invoice_no, description ORDER BY unit_price ASC) as row_idx\n",
    "    FROM tb_ecommerce\n",
    "    ) as temp\n",
    "    WHERE row_idx = 1\n",
    "),\n",
    "\n",
    "most_spent as (\n",
    "\n",
    "SELECT \n",
    "    invoice_no,\n",
    "    'product_most_spent' as window_type,\n",
    "    description\n",
    "    FROM (\n",
    "    SELECT \n",
    "        invoice_no,\n",
    "        description,\n",
    "        ROW_NUMBER() OVER(PARTITION BY invoice_no, description ORDER BY ABS(total_item_price) DESC) as row_idx\n",
    "    FROM tb_ecommerce\n",
    "    ) as temp\n",
    "    WHERE row_idx = 1\n",
    "),\n",
    "\n",
    "most_bought as (\n",
    "\n",
    "SELECT \n",
    "    invoice_no,\n",
    "    'product_most_bought' as window_type,\n",
    "    description\n",
    "    FROM (\n",
    "    SELECT \n",
    "        invoice_no,\n",
    "        description,\n",
    "        ROW_NUMBER() OVER(PARTITION BY invoice_no, description ORDER BY ABS(quantity) DESC) as row_idx\n",
    "    FROM tb_ecommerce\n",
    "    ) as temp\n",
    "    WHERE row_idx = 1\n",
    ")\n",
    "\n",
    "SELECT invoice_no, window_type, description FROM expensive\n",
    "UNION\n",
    "SELECT invoice_no, window_type, description FROM cheapest\n",
    "UNION\n",
    "SELECT invoice_no, window_type, description FROM most_spent\n",
    "UNION\n",
    "SELECT invoice_no, window_type, description FROM most_bought\n",
    "\"\"\"\n",
    "\n",
    "df_items_info = spark.sql(Q_ITEMS_INFORMATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"# pivotting the item information dataframe to get the wide-format we need\\ndf_items_pivot = (df_items_info\\n                     .groupby('invoice_no')\\n                     .pivot('window_type')\\n                     .agg(F.first('description')))\";\n",
       "                var nbb_formatted_code = \"# pivotting the item information dataframe to get the wide-format we need\\ndf_items_pivot = (\\n    df_items_info.groupby(\\\"invoice_no\\\").pivot(\\\"window_type\\\").agg(F.first(\\\"description\\\"))\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pivotting the item information dataframe to get the wide-format we need\n",
    "df_items_pivot = (\n",
    "    df_items_info.groupby(\"invoice_no\").pivot(\"window_type\").agg(F.first(\"description\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"# joining the results back into the invoice dataframe:\\ndf_invoice = (df_invoice\\n                 .join(df_items_pivot,\\n                       on = ['invoice_no'],\\n                       how = 'left'))\";\n",
       "                var nbb_formatted_code = \"# joining the results back into the invoice dataframe:\\ndf_invoice = df_invoice.join(df_items_pivot, on=[\\\"invoice_no\\\"], how=\\\"left\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# joining the results back into the invoice dataframe:\n",
    "df_invoice = df_invoice.join(df_items_pivot, on=[\"invoice_no\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"# joining the subset of previously defined date-related attributes:\\ncols_to_remove = [\\n    'description',\\n    'stock_code',\\n    'quantity',\\n    'unit_price',\\n    'customer_id',\\n    'country',\\n    'is_missing_customer_id',\\n    'total_item_price',\\n    'retail_price',\\n    'min_unit_price',\\n    'avg_unit_price',\\n    'median_unit_price',\\n    'max_unit_price',\\n    'is_discounted_item',\\n    'has_non_digit',\\n    'is_postage',\\n    'is_manual',\\n    'is_discount',\\n    'is_fee',\\n    'is_cancelled',\\n    'is_return',\\n    'is_free_item',\\n    'temp_id'\\n]\\n\\ndf_dates = df_clean.drop(*cols_to_remove)\";\n",
       "                var nbb_formatted_code = \"# joining the subset of previously defined date-related attributes:\\ncols_to_remove = [\\n    \\\"description\\\",\\n    \\\"stock_code\\\",\\n    \\\"quantity\\\",\\n    \\\"unit_price\\\",\\n    \\\"customer_id\\\",\\n    \\\"country\\\",\\n    \\\"is_missing_customer_id\\\",\\n    \\\"total_item_price\\\",\\n    \\\"retail_price\\\",\\n    \\\"min_unit_price\\\",\\n    \\\"avg_unit_price\\\",\\n    \\\"median_unit_price\\\",\\n    \\\"max_unit_price\\\",\\n    \\\"is_discounted_item\\\",\\n    \\\"has_non_digit\\\",\\n    \\\"is_postage\\\",\\n    \\\"is_manual\\\",\\n    \\\"is_discount\\\",\\n    \\\"is_fee\\\",\\n    \\\"is_cancelled\\\",\\n    \\\"is_return\\\",\\n    \\\"is_free_item\\\",\\n    \\\"temp_id\\\",\\n]\\n\\ndf_dates = df_clean.drop(*cols_to_remove)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# joining the subset of previously defined date-related attributes:\n",
    "cols_to_remove = [\n",
    "    \"description\",\n",
    "    \"stock_code\",\n",
    "    \"quantity\",\n",
    "    \"unit_price\",\n",
    "    \"customer_id\",\n",
    "    \"country\",\n",
    "    \"is_missing_customer_id\",\n",
    "    \"total_item_price\",\n",
    "    \"retail_price\",\n",
    "    \"min_unit_price\",\n",
    "    \"avg_unit_price\",\n",
    "    \"median_unit_price\",\n",
    "    \"max_unit_price\",\n",
    "    \"is_discounted_item\",\n",
    "    \"has_non_digit\",\n",
    "    \"is_postage\",\n",
    "    \"is_manual\",\n",
    "    \"is_discount\",\n",
    "    \"is_fee\",\n",
    "    \"is_cancelled\",\n",
    "    \"is_return\",\n",
    "    \"is_free_item\",\n",
    "    \"temp_id\",\n",
    "]\n",
    "\n",
    "df_dates = df_clean.drop(*cols_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"# the resulting dataframe:\\ndf_invoice = (df_invoice\\n                 .join(df_dates,\\n                       how = 'left',\\n                       on = ['invoice_no'])\\n                 .drop_duplicates(subset = ['invoice_no',\\n                                            'invoice_date']))\";\n",
       "                var nbb_formatted_code = \"# the resulting dataframe:\\ndf_invoice = df_invoice.join(df_dates, how=\\\"left\\\", on=[\\\"invoice_no\\\"]).drop_duplicates(\\n    subset=[\\\"invoice_no\\\", \\\"invoice_date\\\"]\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the resulting dataframe:\n",
    "df_invoice = df_invoice.join(df_dates, how=\"left\", on=[\"invoice_no\"]).drop_duplicates(\n",
    "    subset=[\"invoice_no\", \"invoice_date\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Aggregated and relative features\n",
    "Finally, I will add a few more features related to proportions occupied by certain types of products (free items, returns, et cetera)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- invoice_no: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- customer_country: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- invoice_items: string (nullable = false)\n",
      " |-- basket_size: double (nullable = true)\n",
      " |-- basket_diversity: long (nullable = false)\n",
      " |-- total_paid: double (nullable = true)\n",
      " |-- pct_sale_items: double (nullable = true)\n",
      " |-- sale_items: long (nullable = true)\n",
      " |-- total_quantity_sale: double (nullable = true)\n",
      " |-- total_paid_sale: double (nullable = true)\n",
      " |-- cancelled_items: long (nullable = true)\n",
      " |-- total_quantity_cancelled: double (nullable = true)\n",
      " |-- total_paid_cancelled: double (nullable = true)\n",
      " |-- returned_items: long (nullable = true)\n",
      " |-- total_quantity_returned: double (nullable = true)\n",
      " |-- total_paid_returned: double (nullable = true)\n",
      " |-- pct_free_items: double (nullable = true)\n",
      " |-- free_items: long (nullable = true)\n",
      " |-- total_quantity_free: double (nullable = true)\n",
      " |-- total_paid_postage: double (nullable = true)\n",
      " |-- total_paid_manual: double (nullable = true)\n",
      " |-- total_paid_fee: double (nullable = true)\n",
      " |-- total_discounts: double (nullable = true)\n",
      " |-- product_cheapest: string (nullable = true)\n",
      " |-- product_most_bought: string (nullable = true)\n",
      " |-- product_most_expensive: string (nullable = true)\n",
      " |-- product_most_spent: string (nullable = true)\n",
      " |-- invoice_date: date (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- day_of_month: integer (nullable = true)\n",
      " |-- day_of_year: integer (nullable = true)\n",
      " |-- week_of_year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- quarter: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- is_commercial_holiday: boolean (nullable = true)\n",
      " |-- is_commercial_holiday_week: boolean (nullable = true)\n",
      " |-- is_commercial_holiday_month: boolean (nullable = true)\n",
      " |-- is_bank_holiday: boolean (nullable = true)\n",
      " |-- is_bank_holiday_week: boolean (nullable = true)\n",
      " |-- is_bank_holiday_month: boolean (nullable = true)\n",
      " |-- days_to_next_commercial_holiday: integer (nullable = true)\n",
      " |-- days_to_next_bank_holiday: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"df_invoice.printSchema()\";\n",
       "                var nbb_formatted_code = \"df_invoice.printSchema()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_invoice.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"# pct of discounted items:\\ndf_invoice = (df_invoice\\n                 .withColumn('pct_paid_sale_items',\\n                             F.round(F.col('total_paid_sale') / F.col('total_paid'), 2)))\\n\\ndf_invoice = (df_invoice\\n                 .withColumn('pct_paid_shipping_and_fees',\\n                             F.round((F.col('total_paid_postage')\\n                                      + F.col('total_paid_fee'))\\n                                     / F.col('total_paid'), 2)))\\n\\ndf_invoice = (df_invoice\\n                 .withColumn('pct_paid_manuals',\\n                             F.round((F.col('total_paid_manual'))\\n                                     / F.col('total_paid'), 2)))\";\n",
       "                var nbb_formatted_code = \"# pct of discounted items:\\ndf_invoice = df_invoice.withColumn(\\n    \\\"pct_paid_sale_items\\\", F.round(F.col(\\\"total_paid_sale\\\") / F.col(\\\"total_paid\\\"), 2)\\n)\\n\\ndf_invoice = df_invoice.withColumn(\\n    \\\"pct_paid_shipping_and_fees\\\",\\n    F.round(\\n        (F.col(\\\"total_paid_postage\\\") + F.col(\\\"total_paid_fee\\\")) / F.col(\\\"total_paid\\\"), 2\\n    ),\\n)\\n\\ndf_invoice = df_invoice.withColumn(\\n    \\\"pct_paid_manuals\\\", F.round((F.col(\\\"total_paid_manual\\\")) / F.col(\\\"total_paid\\\"), 2)\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pct of discounted items:\n",
    "df_invoice = df_invoice.withColumn(\n",
    "    \"pct_paid_sale_items\", F.round(F.col(\"total_paid_sale\") / F.col(\"total_paid\"), 2)\n",
    ")\n",
    "\n",
    "df_invoice = df_invoice.withColumn(\n",
    "    \"pct_paid_shipping_and_fees\",\n",
    "    F.round(\n",
    "        (F.col(\"total_paid_postage\") + F.col(\"total_paid_fee\")) / F.col(\"total_paid\"), 2\n",
    "    ),\n",
    ")\n",
    "\n",
    "df_invoice = df_invoice.withColumn(\n",
    "    \"pct_paid_manuals\", F.round((F.col(\"total_paid_manual\")) / F.col(\"total_paid\"), 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Cleaning up the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 16;\n",
       "                var nbb_unformatted_code = \"# let's put columns into a more readable format (sort them alphabetically)\\n\\npivot_cols = [\\n    'invoice_no',\\n    'customer_id',\\n    'customer_country',\\n    'date',\\n    'invoice_items'\\n]\\n\\ncurrent_cols = df_invoice.columns\\n\\nremaining_cols = sorted(list(filter(lambda col: col not in pivot_cols, current_cols)))\\n\\n# adding final set of columns:\\ncols_selected = pivot_cols + remaining_cols\";\n",
       "                var nbb_formatted_code = \"# let's put columns into a more readable format (sort them alphabetically)\\n\\npivot_cols = [\\\"invoice_no\\\", \\\"customer_id\\\", \\\"customer_country\\\", \\\"date\\\", \\\"invoice_items\\\"]\\n\\ncurrent_cols = df_invoice.columns\\n\\nremaining_cols = sorted(list(filter(lambda col: col not in pivot_cols, current_cols)))\\n\\n# adding final set of columns:\\ncols_selected = pivot_cols + remaining_cols\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's put columns into a more readable format (sort them alphabetically)\n",
    "\n",
    "pivot_cols = [\"invoice_no\", \"customer_id\", \"customer_country\", \"date\", \"invoice_items\"]\n",
    "\n",
    "current_cols = df_invoice.columns\n",
    "\n",
    "remaining_cols = sorted(list(filter(lambda col: col not in pivot_cols, current_cols)))\n",
    "\n",
    "# adding final set of columns:\n",
    "cols_selected = pivot_cols + remaining_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 17;\n",
       "                var nbb_unformatted_code = \"# selecting the columns in order:\\ndf_invoice = (df_invoice\\n                 .select(*cols_selected))\";\n",
       "                var nbb_formatted_code = \"# selecting the columns in order:\\ndf_invoice = df_invoice.select(*cols_selected)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# selecting the columns in order:\n",
    "df_invoice = df_invoice.select(*cols_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23191"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"# verifying number of entries:\\ndf_invoice.count()\";\n",
       "                var nbb_formatted_code = \"# verifying number of entries:\\ndf_invoice.count()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# verifying number of entries:\n",
    "df_invoice.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>invoice_no</th><th>customer_id</th><th>customer_country</th><th>date</th><th>invoice_items</th><th>basket_diversity</th><th>basket_size</th><th>cancelled_items</th><th>day_of_month</th><th>day_of_week</th><th>day_of_year</th><th>days_to_next_bank_holiday</th><th>days_to_next_commercial_holiday</th><th>free_items</th><th>invoice_date</th><th>is_bank_holiday</th><th>is_bank_holiday_month</th><th>is_bank_holiday_week</th><th>is_commercial_holiday</th><th>is_commercial_holiday_month</th><th>is_commercial_holiday_week</th><th>month</th><th>pct_free_items</th><th>pct_paid_manuals</th><th>pct_paid_sale_items</th><th>pct_paid_shipping_and_fees</th><th>pct_sale_items</th><th>product_cheapest</th><th>product_most_bought</th><th>product_most_expensive</th><th>product_most_spent</th><th>quarter</th><th>returned_items</th><th>sale_items</th><th>total_discounts</th><th>total_paid</th><th>total_paid_cancelled</th><th>total_paid_fee</th><th>total_paid_manual</th><th>total_paid_postage</th><th>total_paid_returned</th><th>total_paid_sale</th><th>total_quantity_cancelled</th><th>total_quantity_free</th><th>total_quantity_returned</th><th>total_quantity_sale</th><th>week_of_year</th><th>year</th></tr>\n",
       "<tr><td>536938</td><td>14680</td><td>united kingdom</td><td>2016-12-01</td><td>green 3 piece pol...</td><td>14</td><td>464.0</td><td>0</td><td>1</td><td>5</td><td>336</td><td>24</td><td>25</td><td>2</td><td>2016-12-01</td><td>false</td><td>true</td><td>false</td><td>false</td><td>true</td><td>false</td><td>12</td><td>0.143</td><td>0.0</td><td>0.87</td><td>0.0</td><td>0.714</td><td>white skull hot w...</td><td>jumbo bag baroque...</td><td>jumbo bag pink po...</td><td>red 3 piece retro...</td><td>4</td><td>0</td><td>10</td><td>0.0</td><td>1680.88</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1466.88</td><td>0.0</td><td>40.0</td><td>0.0</td><td>376.0</td><td>48</td><td>2016</td></tr>\n",
       "<tr><td>537691</td><td>13842</td><td>united kingdom</td><td>2016-12-06</td><td>3 hook photo shel...</td><td>20</td><td>163.0</td><td>0</td><td>6</td><td>3</td><td>341</td><td>19</td><td>20</td><td>4</td><td>2016-12-06</td><td>false</td><td>true</td><td>false</td><td>false</td><td>true</td><td>false</td><td>12</td><td>0.2</td><td>0.0</td><td>0.13</td><td>0.0</td><td>0.1</td><td>heart of wicker s...</td><td>wood s 3 cabinet ...</td><td>party bunting</td><td>party bunting</td><td>4</td><td>0</td><td>2</td><td>0.0</td><td>310.57</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>39.15</td><td>0.0</td><td>64.0</td><td>0.0</td><td>7.0</td><td>49</td><td>2016</td></tr>\n",
       "<tr><td>538184</td><td>17880</td><td>united kingdom</td><td>2016-12-08</td><td>empire design ros...</td><td>26</td><td>314.0</td><td>0</td><td>8</td><td>5</td><td>343</td><td>17</td><td>18</td><td>5</td><td>2016-12-08</td><td>false</td><td>true</td><td>false</td><td>false</td><td>true</td><td>false</td><td>12</td><td>0.192</td><td>0.0</td><td>0.08</td><td>0.0</td><td>0.077</td><td>retrospot giant t...</td><td>doormat welcome t...</td><td>calendar paper cu...</td><td>calendar paper cu...</td><td>4</td><td>0</td><td>2</td><td>0.0</td><td>458.92</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>35.58</td><td>0.0</td><td>120.0</td><td>0.0</td><td>18.0</td><td>49</td><td>2016</td></tr>\n",
       "<tr><td>538517</td><td>17618</td><td>united kingdom</td><td>2016-12-10</td><td>wooden box of dom...</td><td>41</td><td>161.0</td><td>0</td><td>10</td><td>7</td><td>345</td><td>15</td><td>16</td><td>8</td><td>2016-12-10</td><td>false</td><td>true</td><td>false</td><td>false</td><td>true</td><td>false</td><td>12</td><td>0.151</td><td>0.0</td><td>0.08</td><td>0.0</td><td>0.038</td><td>grey floral feltc...</td><td>feltcraft christm...</td><td>clothes pegs retr...</td><td>vintage cream dog...</td><td>4</td><td>0</td><td>2</td><td>0.0</td><td>320.28</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>25.76</td><td>0.0</td><td>43.0</td><td>0.0</td><td>8.0</td><td>49</td><td>2016</td></tr>\n",
       "<tr><td>538879</td><td>15392</td><td>united kingdom</td><td>2016-12-12</td><td>christmas gingham...</td><td>19</td><td>402.0</td><td>0</td><td>12</td><td>2</td><td>347</td><td>13</td><td>14</td><td>12</td><td>2016-12-12</td><td>false</td><td>true</td><td>false</td><td>false</td><td>true</td><td>false</td><td>12</td><td>0.632</td><td>0.0</td><td>0.04</td><td>0.0</td><td>0.053</td><td>danish rose round...</td><td>pack of 72 retros...</td><td>card billboard font</td><td>card billboard font</td><td>4</td><td>0</td><td>1</td><td>0.0</td><td>338.98</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>13.6</td><td>0.0</td><td>316.0</td><td>0.0</td><td>16.0</td><td>50</td><td>2016</td></tr>\n",
       "</table>\n",
       "only showing top 5 rows\n"
      ],
      "text/plain": [
       "+----------+-----------+----------------+----------+--------------------+----------------+-----------+---------------+------------+-----------+-----------+-------------------------+-------------------------------+----------+------------+---------------+---------------------+--------------------+---------------------+---------------------------+--------------------------+-----+--------------+----------------+-------------------+--------------------------+--------------+--------------------+--------------------+----------------------+--------------------+-------+--------------+----------+---------------+----------+--------------------+--------------+-----------------+------------------+-------------------+---------------+------------------------+-------------------+-----------------------+-------------------+------------+----+\n",
       "|invoice_no|customer_id|customer_country|      date|       invoice_items|basket_diversity|basket_size|cancelled_items|day_of_month|day_of_week|day_of_year|days_to_next_bank_holiday|days_to_next_commercial_holiday|free_items|invoice_date|is_bank_holiday|is_bank_holiday_month|is_bank_holiday_week|is_commercial_holiday|is_commercial_holiday_month|is_commercial_holiday_week|month|pct_free_items|pct_paid_manuals|pct_paid_sale_items|pct_paid_shipping_and_fees|pct_sale_items|    product_cheapest| product_most_bought|product_most_expensive|  product_most_spent|quarter|returned_items|sale_items|total_discounts|total_paid|total_paid_cancelled|total_paid_fee|total_paid_manual|total_paid_postage|total_paid_returned|total_paid_sale|total_quantity_cancelled|total_quantity_free|total_quantity_returned|total_quantity_sale|week_of_year|year|\n",
       "+----------+-----------+----------------+----------+--------------------+----------------+-----------+---------------+------------+-----------+-----------+-------------------------+-------------------------------+----------+------------+---------------+---------------------+--------------------+---------------------+---------------------------+--------------------------+-----+--------------+----------------+-------------------+--------------------------+--------------+--------------------+--------------------+----------------------+--------------------+-------+--------------+----------+---------------+----------+--------------------+--------------+-----------------+------------------+-------------------+---------------+------------------------+-------------------+-----------------------+-------------------+------------+----+\n",
       "|    536938|      14680|  united kingdom|2016-12-01|green 3 piece pol...|              14|      464.0|              0|           1|          5|        336|                       24|                             25|         2|  2016-12-01|          false|                 true|               false|                false|                       true|                     false|   12|         0.143|             0.0|               0.87|                       0.0|         0.714|white skull hot w...|jumbo bag baroque...|  jumbo bag pink po...|red 3 piece retro...|      4|             0|        10|            0.0|   1680.88|                 0.0|           0.0|              0.0|               0.0|                0.0|        1466.88|                     0.0|               40.0|                    0.0|              376.0|          48|2016|\n",
       "|    537691|      13842|  united kingdom|2016-12-06|3 hook photo shel...|              20|      163.0|              0|           6|          3|        341|                       19|                             20|         4|  2016-12-06|          false|                 true|               false|                false|                       true|                     false|   12|           0.2|             0.0|               0.13|                       0.0|           0.1|heart of wicker s...|wood s 3 cabinet ...|         party bunting|       party bunting|      4|             0|         2|            0.0|    310.57|                 0.0|           0.0|              0.0|               0.0|                0.0|          39.15|                     0.0|               64.0|                    0.0|                7.0|          49|2016|\n",
       "|    538184|      17880|  united kingdom|2016-12-08|empire design ros...|              26|      314.0|              0|           8|          5|        343|                       17|                             18|         5|  2016-12-08|          false|                 true|               false|                false|                       true|                     false|   12|         0.192|             0.0|               0.08|                       0.0|         0.077|retrospot giant t...|doormat welcome t...|  calendar paper cu...|calendar paper cu...|      4|             0|         2|            0.0|    458.92|                 0.0|           0.0|              0.0|               0.0|                0.0|          35.58|                     0.0|              120.0|                    0.0|               18.0|          49|2016|\n",
       "|    538517|      17618|  united kingdom|2016-12-10|wooden box of dom...|              41|      161.0|              0|          10|          7|        345|                       15|                             16|         8|  2016-12-10|          false|                 true|               false|                false|                       true|                     false|   12|         0.151|             0.0|               0.08|                       0.0|         0.038|grey floral feltc...|feltcraft christm...|  clothes pegs retr...|vintage cream dog...|      4|             0|         2|            0.0|    320.28|                 0.0|           0.0|              0.0|               0.0|                0.0|          25.76|                     0.0|               43.0|                    0.0|                8.0|          49|2016|\n",
       "|    538879|      15392|  united kingdom|2016-12-12|christmas gingham...|              19|      402.0|              0|          12|          2|        347|                       13|                             14|        12|  2016-12-12|          false|                 true|               false|                false|                       true|                     false|   12|         0.632|             0.0|               0.04|                       0.0|         0.053|danish rose round...|pack of 72 retros...|   card billboard font| card billboard font|      4|             0|         1|            0.0|    338.98|                 0.0|           0.0|              0.0|               0.0|                0.0|           13.6|                     0.0|              316.0|                    0.0|               16.0|          50|2016|\n",
       "+----------+-----------+----------------+----------+--------------------+----------------+-----------+---------------+------------+-----------+-----------+-------------------------+-------------------------------+----------+------------+---------------+---------------------+--------------------+---------------------+---------------------------+--------------------------+-----+--------------+----------------+-------------------+--------------------------+--------------+--------------------+--------------------+----------------------+--------------------+-------+--------------+----------+---------------+----------+--------------------+--------------+-----------------+------------------+-------------------+---------------+------------------------+-------------------+-----------------------+-------------------+------------+----+\n",
       "only showing top 5 rows"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 19;\n",
       "                var nbb_unformatted_code = \"# visualizing the results:\\ndf_invoice\";\n",
       "                var nbb_formatted_code = \"# visualizing the results:\\ndf_invoice\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualizing the results:\n",
    "df_invoice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Saving the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Writing to the filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 20;\n",
       "                var nbb_unformatted_code = \"# saving the enhanced raw data as parquet in the processed step of the pipeline\\nPROCESSED_DATA_DIR = '../data/processed'\\n\\n\\n# using the helper function to save the file:\\nsave_to_filesystem(df_invoice, \\n                   PROCESSED_DATA_DIR,\\n                   'tb_invoice',\\n                   'tb_invoice.parquet')\";\n",
       "                var nbb_formatted_code = \"# saving the enhanced raw data as parquet in the processed step of the pipeline\\nPROCESSED_DATA_DIR = \\\"../data/processed\\\"\\n\\n\\n# using the helper function to save the file:\\nsave_to_filesystem(df_invoice, PROCESSED_DATA_DIR, \\\"tb_invoice\\\", \\\"tb_invoice.parquet\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# saving the enhanced raw data as parquet in the processed step of the pipeline\n",
    "PROCESSED_DATA_DIR = \"../data/processed\"\n",
    "\n",
    "\n",
    "# using the helper function to save the file:\n",
    "save_to_filesystem(df_invoice, PROCESSED_DATA_DIR, \"tb_invoice\", \"tb_invoice.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
