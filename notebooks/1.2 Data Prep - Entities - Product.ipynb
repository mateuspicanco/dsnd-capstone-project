{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep - Product Entity\n",
    "For products, much like invoices, we can summarize purchasing statistics for each individual product to have a sense of how it performs given certain situations (time of year, whether it is at a discount or not). \n",
    "\n",
    "Additionally, I will add features related to the position of the product in respect to the whole, such as the product's popularity in terms of number of purchases, total spent on, amongst other facts.\n",
    "\n",
    "In essence, I am implementing an \"embedding\" of sorts, as I am representing the nominal entity (the name of a product) using a continuous value that has some inherent order to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install inflection nb_black >> ../configs/package_installation.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\\n%load_ext autoreload \\n%autoreload 2\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\\n%load_ext autoreload\\n%autoreload 2\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"# PySpark dependencies:\\nimport pyspark\\nfrom pyspark import SparkConf\\nfrom pyspark import SparkContext\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql import SQLContext\\nimport pyspark.sql.functions as F\\nfrom pyspark.sql.functions import udf\\nfrom pyspark.sql.types import *\\nfrom pyspark.sql.window import Window\\n\\n# database utilities:\\nimport pandas as pd\\n\\n# other relevant libraries:\\nimport warnings\\nimport inflection\\nimport unicodedata\\nfrom datetime import datetime, timedelta\\nimport json \\nimport re\\nimport os\\nfrom glob import glob\\nimport shutil\\nimport itertools\\n\\n# setting global parameters for visualizations:\\nwarnings.filterwarnings('ignore')\\npd.set_option(\\\"display.precision\\\", 4)\\npd.set_option(\\\"display.float_format\\\", lambda x: '%.2f' % x)\";\n",
       "                var nbb_formatted_code = \"# PySpark dependencies:\\nimport pyspark\\nfrom pyspark import SparkConf\\nfrom pyspark import SparkContext\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql import SQLContext\\nimport pyspark.sql.functions as F\\nfrom pyspark.sql.functions import udf\\nfrom pyspark.sql.types import *\\nfrom pyspark.sql.window import Window\\n\\n# database utilities:\\nimport pandas as pd\\n\\n# other relevant libraries:\\nimport warnings\\nimport inflection\\nimport unicodedata\\nfrom datetime import datetime, timedelta\\nimport json\\nimport re\\nimport os\\nfrom glob import glob\\nimport shutil\\nimport itertools\\n\\n# setting global parameters for visualizations:\\nwarnings.filterwarnings(\\\"ignore\\\")\\npd.set_option(\\\"display.precision\\\", 4)\\npd.set_option(\\\"display.float_format\\\", lambda x: \\\"%.2f\\\" % x)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PySpark dependencies:\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# database utilities:\n",
    "import pandas as pd\n",
    "\n",
    "# other relevant libraries:\n",
    "import warnings\n",
    "import inflection\n",
    "import unicodedata\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from glob import glob\n",
    "import shutil\n",
    "import itertools\n",
    "\n",
    "# setting global parameters for visualizations:\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "pd.set_option(\"display.float_format\", lambda x: \"%.2f\" % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Building Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"# loading the configurations needed for Spark\\ndef init_spark(app_name):\\n\\n    spark = (\\n        SparkSession.builder.appName(app_name)\\n        .config(\\\"spark.files.overwrite\\\", \\\"true\\\")\\n        .config(\\\"spark.sql.repl.eagerEval.enabled\\\", True)\\n        .config(\\\"spark.sql.repl.eagerEval.maxNumRows\\\", 5)\\n        .config(\\\"spark.sql.legacy.timeParserPolicy\\\", \\\"LEGACY\\\")\\n        .config(\\\"spark.sql.parquet.compression.codec\\\", \\\"gzip\\\")\\n        .enableHiveSupport()\\n        .getOrCreate()\\n    )\\n\\n    return spark\\n\\n\\n# init the spark session:\\nspark = init_spark(\\\"Product Preparation\\\")\";\n",
       "                var nbb_formatted_code = \"# loading the configurations needed for Spark\\ndef init_spark(app_name):\\n\\n    spark = (\\n        SparkSession.builder.appName(app_name)\\n        .config(\\\"spark.files.overwrite\\\", \\\"true\\\")\\n        .config(\\\"spark.sql.repl.eagerEval.enabled\\\", True)\\n        .config(\\\"spark.sql.repl.eagerEval.maxNumRows\\\", 5)\\n        .config(\\\"spark.sql.legacy.timeParserPolicy\\\", \\\"LEGACY\\\")\\n        .config(\\\"spark.sql.parquet.compression.codec\\\", \\\"gzip\\\")\\n        .enableHiveSupport()\\n        .getOrCreate()\\n    )\\n\\n    return spark\\n\\n\\n# init the spark session:\\nspark = init_spark(\\\"Product Preparation\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loading the configurations needed for Spark\n",
    "def init_spark(app_name):\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder.appName(app_name)\n",
    "        .config(\"spark.files.overwrite\", \"true\")\n",
    "        .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "        .config(\"spark.sql.repl.eagerEval.maxNumRows\", 5)\n",
    "        .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "        .config(\"spark.sql.parquet.compression.codec\", \"gzip\")\n",
    "        .enableHiveSupport()\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "    return spark\n",
    "\n",
    "\n",
    "# init the spark session:\n",
    "spark = init_spark(\"Product Preparation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://84a2e6cc83b5:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Product Preparation</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f40445a7cd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"# verifying the spark session:\\nspark\";\n",
       "                var nbb_formatted_code = \"# verifying the spark session:\\nspark\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# verifying the spark session:\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"def get_pivot_product_info(df, feature_col):\\n    \\\"\\\"\\\"Helper function to reproduce the pivotting and aggregation procedure for generic columns\\n    \\\"\\\"\\\"\\n    df_pivot = (df.groupby('customer_id')\\n                  .pivot('type_of_product')\\n                  .agg(\\n                      F.max(F.col(feature_col))\\n                  ))\\n    \\n    new_cols = [f'{col}_{feature_col}' for col in df_pivot.columns[1:]]\\n    \\n    for idx, col in enumerate(new_cols, start=1):\\n        df_pivot = (df_pivot\\n                       .withColumnRenamed(\\n                           df_pivot.columns[idx],\\n                           col\\n                       ))\\n        \\n    return df_pivot\\n\\ndef save_to_filesystem(df, target_path, parquet_path, filename):\\n    \\\"\\\"\\\"Helper function to save pyspark dataframes as parquets in a way that is similar to writing to local files\\n\\n    Args:\\n        df (pyspark.sql.dataframe.DataFrame): dataframe to be saved\\n        target_path (str): path that will store the file\\n        filename (str): name of the resulting file\\n\\n    Returns:\\n        None\\n    \\\"\\\"\\\"\\n    PARQUET_FILE = f\\\"{target_path}/{parquet_path}\\\"\\n    OUTPUT_FILE = f\\\"{target_path}/{filename}\\\"\\n\\n    if os.path.exists(PARQUET_FILE):\\n        shutil.rmtree(\\n            PARQUET_FILE\\n        )  # if the directory already exists, remove it (throws error if not)\\n\\n    # saves the dataframe:\\n    df.coalesce(1).write.save(PARQUET_FILE)\\n\\n    # retrieves file resulting from the saving procedure:\\n    original_file = glob(f\\\"{PARQUET_FILE}/*.parquet\\\")[0]\\n\\n    # renames the resulting file and saves it to the target directory:\\n    os.rename(original_file, OUTPUT_FILE)\\n\\n    shutil.rmtree(PARQUET_FILE)\\n\\n    return True\\n\\n\\ndef apply_category_map(category_map):\\n    \\\"\\\"\\\"Helper function to convert strings given a map\\n\\n    Note:\\n        This function uses the function generator scheme, much like the PySpark code\\n\\n    Args:\\n        original_category (str): the original category name\\n        category_map (dict): the hash table or dictionary for converting the values:\\n\\n    Returns:\\n        new_category (str): the resulting category\\n\\n    \\\"\\\"\\\"\\n\\n    def func(row):\\n        try:\\n            result = category_map[row]\\n        except:\\n            result = None\\n        return result\\n\\n    return F.udf(func)\\n\\n\\ndef get_datetime_features(df, time_col):\\n    \\\"\\\"\\\"Function to extract time-based features from pyspark dataframes\\n\\n    Args:\\n        df (pyspark.sql.dataframe.DataFrame): the original dataframe that needs to be enriched\\n        time_col (str): the string name of the column containing the date object\\n\\n    Returns:\\n        df (pyspark.sql.dataframe.DataFrame): resulting pyspark dataframe with the added features\\n            -> See list of attribute the source code for the attributes\\n\\n    \\\"\\\"\\\"\\n\\n    # applying date-related functions:\\n\\n    # day-level attributes:\\n    df = df.withColumn(\\\"day_of_week\\\", F.dayofweek(F.col(time_col)))\\n\\n    df = df.withColumn(\\\"day_of_month\\\", F.dayofmonth(F.col(time_col)))\\n\\n    df = df.withColumn(\\\"day_of_year\\\", F.dayofyear(F.col(time_col)))\\n\\n    # week-level attributes:\\n    df = df.withColumn(\\\"week_of_year\\\", F.weekofyear(F.col(time_col)))\\n\\n    # month-level attributes:\\n    df = df.withColumn(\\\"month\\\", F.month(F.col(time_col)))\\n\\n    df = df.withColumn(\\\"quarter\\\", F.quarter(F.col(time_col)))\\n\\n    # year-level attributes:\\n    df = df.withColumn(\\\"year\\\", F.year(F.col(time_col)))\\n\\n    return df\\n\\n\\ndef bulk_aggregate(df, group_col, aggs, target_cols):\\n    \\\"\\\"\\\"Wrapper function to apply multiple aggregations when performing group bys\\n\\n    It utilizes the spark's SQL Context and string interpolation to perform the aggregation using SQL syntax.\\n\\n    Args:\\n        df (pyspark.sql.dataframe.DataFrame): dataframe with raw data\\n        group_col (str): the column that will be used for grouping\\n        aggs (list): list of aggregations that want to be made (must be the same name as pyspark.sql.functions)\\n        target_cols (str): columns in which aggregations will be performed\\n\\n    Returns:\\n        df_grouped (pyspark.sql.dataframe.DataFrame): dataframe with the grouped data\\n    \\\"\\\"\\\"\\n\\n    # buils the cartersian product of the lists\\n    aggs_to_perform = itertools.product(aggs, target_cols)\\n\\n    Q_LAYOUT = \\\"\\\"\\\"\\n    SELECT\\n        {},\\n        {}\\n        FROM df\\n        GROUP BY {}\\n    \\\"\\\"\\\"\\n\\n    aggregations = []\\n    for agg, col in aggs_to_perform:\\n\\n        # builds the string for aggregation\\n        statement = f\\\"{agg.upper()}({col}) as {agg}_{col}\\\"\\n        aggregations.append(statement)\\n\\n    full_statement = \\\",\\\\n\\\".join(aggregations)\\n\\n    # uses string interpolation to build the full query statement\\n    QUERY = Q_LAYOUT.format(group_col, full_statement, group_col)\\n\\n    # registers the dataframe as temporary table:\\n    df.registerTempTable(\\\"df\\\")\\n    df_grouped = spark.sql(QUERY)\\n\\n    # rounds values:\\n    for column in df_grouped.columns:\\n        df_grouped = df_grouped.withColumn(column, F.round(F.col(column), 1))\\n\\n    return df_grouped\\n\\n\\n######### Text Processing Functions ########\\n@udf(\\\"string\\\")\\ndef normalize_text(text):\\n    \\\"\\\"\\\"Helper function to normalize text data to ASCII and lower case, removing spaces\\n\\n    Args:\\n        text (string): the string that needs to be normalized\\n\\n    Returns:\\n        text (string): cleaned up string\\n\\n    \\\"\\\"\\\"\\n    regex = r\\\"[^a-zA-Z0-9]+\\\"\\n\\n    if text is not None:\\n\\n        text = str(text)\\n        text = text.lower()\\n        text = re.sub(regex, \\\" \\\", text)\\n        text = text.strip()\\n        text = str(\\n            unicodedata.normalize(\\\"NFKD\\\", text).encode(\\\"ASCII\\\", \\\"ignore\\\"), \\\"utf-8\\\"\\n        )\\n\\n    return text\\n\\n\\ndef get_null_columns(df, normalize=False):\\n    \\\"\\\"\\\"Helper function to print the number of null records for each column of a PySpark DataFrame.\\n\\n    Args:\\n        df (pyspark.sql.dataframe.DataFrame): a PySpark Dataframe object\\n\\n    Returns:\\n        None -> prints to standard out\\n\\n    \\\"\\\"\\\"\\n\\n    if normalize:\\n        total = df.count()\\n\\n        df_nulls = df.select(\\n            [\\n                (F.sum(F.when(F.col(column).isNull(), 1).otherwise(0)) / total).alias(\\n                    column\\n                )\\n                for column in df.columns\\n            ]\\n        )\\n\\n    else:\\n        df_nulls = df.select(\\n            [\\n                F.sum(F.when(F.col(column).isNull(), 1).otherwise(0)).alias(column)\\n                for column in df.columns\\n            ]\\n        )\\n\\n    # displaying the results to standard out\\n    df_nulls.show(1, truncate=False, vertical=True)\\n\\n\\n@udf(\\\"boolean\\\")\\ndef is_set_or_pack(text):\\n\\n    # description entries to match:\\n    set_descriptions = {\\\"set\\\", \\\"set of\\\", \\\"pack\\\", \\\"pack of\\\", \\\"box\\\", \\\"box of\\\"}\\n\\n    if text is not None:\\n        text = str(text)\\n\\n        if text in set_descriptions:\\n            return True\\n\\n        else:\\n            return False\\n\\n    else:\\n        return False\\n\\n\\n@udf(\\\"integer\\\")\\ndef get_unit_size(text):\\n\\n    if text is not None:\\n        check_if_digit = len(re.findall(r\\\"(\\\\d+)\\\", text)) > 0\\n\\n        if check_if_digit:\\n            set_size = int(re.findall(r\\\"(\\\\d+)\\\", text)[0])\\n            return set_size\\n\\n        else:\\n            return 1\\n\\n    else:\\n        return 1\\n\\n\\n@udf(\\\"boolean\\\")\\ndef has_non_digits_only(text):\\n    \\\"\\\"\\\"Function to match entries in the dataset that are purely non-digit characters\\n\\n    Args:\\n        text (str): string containing the invoice code\\n\\n    Returns:\\n        boolean: whether the text contains non-digit characters and is not related to cancellations\\n\\n    \\\"\\\"\\\"\\n\\n    if text is not None:\\n        condition = all(character.isalpha() for character in text)\\n\\n        if condition:\\n            return True\\n\\n        else:\\n            return False\\n\\n    else:\\n        return False\";\n",
       "                var nbb_formatted_code = \"def get_pivot_product_info(df, feature_col):\\n    \\\"\\\"\\\"Helper function to reproduce the pivotting and aggregation procedure for generic columns\\\"\\\"\\\"\\n    df_pivot = (\\n        df.groupby(\\\"customer_id\\\")\\n        .pivot(\\\"type_of_product\\\")\\n        .agg(F.max(F.col(feature_col)))\\n    )\\n\\n    new_cols = [f\\\"{col}_{feature_col}\\\" for col in df_pivot.columns[1:]]\\n\\n    for idx, col in enumerate(new_cols, start=1):\\n        df_pivot = df_pivot.withColumnRenamed(df_pivot.columns[idx], col)\\n\\n    return df_pivot\\n\\n\\ndef save_to_filesystem(df, target_path, parquet_path, filename):\\n    \\\"\\\"\\\"Helper function to save pyspark dataframes as parquets in a way that is similar to writing to local files\\n\\n    Args:\\n        df (pyspark.sql.dataframe.DataFrame): dataframe to be saved\\n        target_path (str): path that will store the file\\n        filename (str): name of the resulting file\\n\\n    Returns:\\n        None\\n    \\\"\\\"\\\"\\n    PARQUET_FILE = f\\\"{target_path}/{parquet_path}\\\"\\n    OUTPUT_FILE = f\\\"{target_path}/{filename}\\\"\\n\\n    if os.path.exists(PARQUET_FILE):\\n        shutil.rmtree(\\n            PARQUET_FILE\\n        )  # if the directory already exists, remove it (throws error if not)\\n\\n    # saves the dataframe:\\n    df.coalesce(1).write.save(PARQUET_FILE)\\n\\n    # retrieves file resulting from the saving procedure:\\n    original_file = glob(f\\\"{PARQUET_FILE}/*.parquet\\\")[0]\\n\\n    # renames the resulting file and saves it to the target directory:\\n    os.rename(original_file, OUTPUT_FILE)\\n\\n    shutil.rmtree(PARQUET_FILE)\\n\\n    return True\\n\\n\\ndef apply_category_map(category_map):\\n    \\\"\\\"\\\"Helper function to convert strings given a map\\n\\n    Note:\\n        This function uses the function generator scheme, much like the PySpark code\\n\\n    Args:\\n        original_category (str): the original category name\\n        category_map (dict): the hash table or dictionary for converting the values:\\n\\n    Returns:\\n        new_category (str): the resulting category\\n\\n    \\\"\\\"\\\"\\n\\n    def func(row):\\n        try:\\n            result = category_map[row]\\n        except:\\n            result = None\\n        return result\\n\\n    return F.udf(func)\\n\\n\\ndef get_datetime_features(df, time_col):\\n    \\\"\\\"\\\"Function to extract time-based features from pyspark dataframes\\n\\n    Args:\\n        df (pyspark.sql.dataframe.DataFrame): the original dataframe that needs to be enriched\\n        time_col (str): the string name of the column containing the date object\\n\\n    Returns:\\n        df (pyspark.sql.dataframe.DataFrame): resulting pyspark dataframe with the added features\\n            -> See list of attribute the source code for the attributes\\n\\n    \\\"\\\"\\\"\\n\\n    # applying date-related functions:\\n\\n    # day-level attributes:\\n    df = df.withColumn(\\\"day_of_week\\\", F.dayofweek(F.col(time_col)))\\n\\n    df = df.withColumn(\\\"day_of_month\\\", F.dayofmonth(F.col(time_col)))\\n\\n    df = df.withColumn(\\\"day_of_year\\\", F.dayofyear(F.col(time_col)))\\n\\n    # week-level attributes:\\n    df = df.withColumn(\\\"week_of_year\\\", F.weekofyear(F.col(time_col)))\\n\\n    # month-level attributes:\\n    df = df.withColumn(\\\"month\\\", F.month(F.col(time_col)))\\n\\n    df = df.withColumn(\\\"quarter\\\", F.quarter(F.col(time_col)))\\n\\n    # year-level attributes:\\n    df = df.withColumn(\\\"year\\\", F.year(F.col(time_col)))\\n\\n    return df\\n\\n\\ndef bulk_aggregate(df, group_col, aggs, target_cols):\\n    \\\"\\\"\\\"Wrapper function to apply multiple aggregations when performing group bys\\n\\n    It utilizes the spark's SQL Context and string interpolation to perform the aggregation using SQL syntax.\\n\\n    Args:\\n        df (pyspark.sql.dataframe.DataFrame): dataframe with raw data\\n        group_col (str): the column that will be used for grouping\\n        aggs (list): list of aggregations that want to be made (must be the same name as pyspark.sql.functions)\\n        target_cols (str): columns in which aggregations will be performed\\n\\n    Returns:\\n        df_grouped (pyspark.sql.dataframe.DataFrame): dataframe with the grouped data\\n    \\\"\\\"\\\"\\n\\n    # buils the cartersian product of the lists\\n    aggs_to_perform = itertools.product(aggs, target_cols)\\n\\n    Q_LAYOUT = \\\"\\\"\\\"\\n    SELECT\\n        {},\\n        {}\\n        FROM df\\n        GROUP BY {}\\n    \\\"\\\"\\\"\\n\\n    aggregations = []\\n    for agg, col in aggs_to_perform:\\n\\n        # builds the string for aggregation\\n        statement = f\\\"{agg.upper()}({col}) as {agg}_{col}\\\"\\n        aggregations.append(statement)\\n\\n    full_statement = \\\",\\\\n\\\".join(aggregations)\\n\\n    # uses string interpolation to build the full query statement\\n    QUERY = Q_LAYOUT.format(group_col, full_statement, group_col)\\n\\n    # registers the dataframe as temporary table:\\n    df.registerTempTable(\\\"df\\\")\\n    df_grouped = spark.sql(QUERY)\\n\\n    # rounds values:\\n    for column in df_grouped.columns:\\n        df_grouped = df_grouped.withColumn(column, F.round(F.col(column), 1))\\n\\n    return df_grouped\\n\\n\\n######### Text Processing Functions ########\\n@udf(\\\"string\\\")\\ndef normalize_text(text):\\n    \\\"\\\"\\\"Helper function to normalize text data to ASCII and lower case, removing spaces\\n\\n    Args:\\n        text (string): the string that needs to be normalized\\n\\n    Returns:\\n        text (string): cleaned up string\\n\\n    \\\"\\\"\\\"\\n    regex = r\\\"[^a-zA-Z0-9]+\\\"\\n\\n    if text is not None:\\n\\n        text = str(text)\\n        text = text.lower()\\n        text = re.sub(regex, \\\" \\\", text)\\n        text = text.strip()\\n        text = str(\\n            unicodedata.normalize(\\\"NFKD\\\", text).encode(\\\"ASCII\\\", \\\"ignore\\\"), \\\"utf-8\\\"\\n        )\\n\\n    return text\\n\\n\\ndef get_null_columns(df, normalize=False):\\n    \\\"\\\"\\\"Helper function to print the number of null records for each column of a PySpark DataFrame.\\n\\n    Args:\\n        df (pyspark.sql.dataframe.DataFrame): a PySpark Dataframe object\\n\\n    Returns:\\n        None -> prints to standard out\\n\\n    \\\"\\\"\\\"\\n\\n    if normalize:\\n        total = df.count()\\n\\n        df_nulls = df.select(\\n            [\\n                (F.sum(F.when(F.col(column).isNull(), 1).otherwise(0)) / total).alias(\\n                    column\\n                )\\n                for column in df.columns\\n            ]\\n        )\\n\\n    else:\\n        df_nulls = df.select(\\n            [\\n                F.sum(F.when(F.col(column).isNull(), 1).otherwise(0)).alias(column)\\n                for column in df.columns\\n            ]\\n        )\\n\\n    # displaying the results to standard out\\n    df_nulls.show(1, truncate=False, vertical=True)\\n\\n\\n@udf(\\\"boolean\\\")\\ndef is_set_or_pack(text):\\n\\n    # description entries to match:\\n    set_descriptions = {\\\"set\\\", \\\"set of\\\", \\\"pack\\\", \\\"pack of\\\", \\\"box\\\", \\\"box of\\\"}\\n\\n    if text is not None:\\n        text = str(text)\\n\\n        if text in set_descriptions:\\n            return True\\n\\n        else:\\n            return False\\n\\n    else:\\n        return False\\n\\n\\n@udf(\\\"integer\\\")\\ndef get_unit_size(text):\\n\\n    if text is not None:\\n        check_if_digit = len(re.findall(r\\\"(\\\\d+)\\\", text)) > 0\\n\\n        if check_if_digit:\\n            set_size = int(re.findall(r\\\"(\\\\d+)\\\", text)[0])\\n            return set_size\\n\\n        else:\\n            return 1\\n\\n    else:\\n        return 1\\n\\n\\n@udf(\\\"boolean\\\")\\ndef has_non_digits_only(text):\\n    \\\"\\\"\\\"Function to match entries in the dataset that are purely non-digit characters\\n\\n    Args:\\n        text (str): string containing the invoice code\\n\\n    Returns:\\n        boolean: whether the text contains non-digit characters and is not related to cancellations\\n\\n    \\\"\\\"\\\"\\n\\n    if text is not None:\\n        condition = all(character.isalpha() for character in text)\\n\\n        if condition:\\n            return True\\n\\n        else:\\n            return False\\n\\n    else:\\n        return False\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_pivot_product_info(df, feature_col):\n",
    "    \"\"\"Helper function to reproduce the pivotting and aggregation procedure for generic columns\"\"\"\n",
    "    df_pivot = (\n",
    "        df.groupby(\"customer_id\")\n",
    "        .pivot(\"type_of_product\")\n",
    "        .agg(F.max(F.col(feature_col)))\n",
    "    )\n",
    "\n",
    "    new_cols = [f\"{col}_{feature_col}\" for col in df_pivot.columns[1:]]\n",
    "\n",
    "    for idx, col in enumerate(new_cols, start=1):\n",
    "        df_pivot = df_pivot.withColumnRenamed(df_pivot.columns[idx], col)\n",
    "\n",
    "    return df_pivot\n",
    "\n",
    "\n",
    "def save_to_filesystem(df, target_path, parquet_path, filename):\n",
    "    \"\"\"Helper function to save pyspark dataframes as parquets in a way that is similar to writing to local files\n",
    "\n",
    "    Args:\n",
    "        df (pyspark.sql.dataframe.DataFrame): dataframe to be saved\n",
    "        target_path (str): path that will store the file\n",
    "        filename (str): name of the resulting file\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    PARQUET_FILE = f\"{target_path}/{parquet_path}\"\n",
    "    OUTPUT_FILE = f\"{target_path}/{filename}\"\n",
    "\n",
    "    if os.path.exists(PARQUET_FILE):\n",
    "        shutil.rmtree(\n",
    "            PARQUET_FILE\n",
    "        )  # if the directory already exists, remove it (throws error if not)\n",
    "\n",
    "    # saves the dataframe:\n",
    "    df.coalesce(1).write.save(PARQUET_FILE)\n",
    "\n",
    "    # retrieves file resulting from the saving procedure:\n",
    "    original_file = glob(f\"{PARQUET_FILE}/*.parquet\")[0]\n",
    "\n",
    "    # renames the resulting file and saves it to the target directory:\n",
    "    os.rename(original_file, OUTPUT_FILE)\n",
    "\n",
    "    shutil.rmtree(PARQUET_FILE)\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def apply_category_map(category_map):\n",
    "    \"\"\"Helper function to convert strings given a map\n",
    "\n",
    "    Note:\n",
    "        This function uses the function generator scheme, much like the PySpark code\n",
    "\n",
    "    Args:\n",
    "        original_category (str): the original category name\n",
    "        category_map (dict): the hash table or dictionary for converting the values:\n",
    "\n",
    "    Returns:\n",
    "        new_category (str): the resulting category\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def func(row):\n",
    "        try:\n",
    "            result = category_map[row]\n",
    "        except:\n",
    "            result = None\n",
    "        return result\n",
    "\n",
    "    return F.udf(func)\n",
    "\n",
    "\n",
    "def get_datetime_features(df, time_col):\n",
    "    \"\"\"Function to extract time-based features from pyspark dataframes\n",
    "\n",
    "    Args:\n",
    "        df (pyspark.sql.dataframe.DataFrame): the original dataframe that needs to be enriched\n",
    "        time_col (str): the string name of the column containing the date object\n",
    "\n",
    "    Returns:\n",
    "        df (pyspark.sql.dataframe.DataFrame): resulting pyspark dataframe with the added features\n",
    "            -> See list of attribute the source code for the attributes\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # applying date-related functions:\n",
    "\n",
    "    # day-level attributes:\n",
    "    df = df.withColumn(\"day_of_week\", F.dayofweek(F.col(time_col)))\n",
    "\n",
    "    df = df.withColumn(\"day_of_month\", F.dayofmonth(F.col(time_col)))\n",
    "\n",
    "    df = df.withColumn(\"day_of_year\", F.dayofyear(F.col(time_col)))\n",
    "\n",
    "    # week-level attributes:\n",
    "    df = df.withColumn(\"week_of_year\", F.weekofyear(F.col(time_col)))\n",
    "\n",
    "    # month-level attributes:\n",
    "    df = df.withColumn(\"month\", F.month(F.col(time_col)))\n",
    "\n",
    "    df = df.withColumn(\"quarter\", F.quarter(F.col(time_col)))\n",
    "\n",
    "    # year-level attributes:\n",
    "    df = df.withColumn(\"year\", F.year(F.col(time_col)))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def bulk_aggregate(df, group_col, aggs, target_cols):\n",
    "    \"\"\"Wrapper function to apply multiple aggregations when performing group bys\n",
    "\n",
    "    It utilizes the spark's SQL Context and string interpolation to perform the aggregation using SQL syntax.\n",
    "\n",
    "    Args:\n",
    "        df (pyspark.sql.dataframe.DataFrame): dataframe with raw data\n",
    "        group_col (str): the column that will be used for grouping\n",
    "        aggs (list): list of aggregations that want to be made (must be the same name as pyspark.sql.functions)\n",
    "        target_cols (str): columns in which aggregations will be performed\n",
    "\n",
    "    Returns:\n",
    "        df_grouped (pyspark.sql.dataframe.DataFrame): dataframe with the grouped data\n",
    "    \"\"\"\n",
    "\n",
    "    # buils the cartersian product of the lists\n",
    "    aggs_to_perform = itertools.product(aggs, target_cols)\n",
    "\n",
    "    Q_LAYOUT = \"\"\"\n",
    "    SELECT\n",
    "        {},\n",
    "        {}\n",
    "        FROM df\n",
    "        GROUP BY {}\n",
    "    \"\"\"\n",
    "\n",
    "    aggregations = []\n",
    "    for agg, col in aggs_to_perform:\n",
    "\n",
    "        # builds the string for aggregation\n",
    "        statement = f\"{agg.upper()}({col}) as {agg}_{col}\"\n",
    "        aggregations.append(statement)\n",
    "\n",
    "    full_statement = \",\\n\".join(aggregations)\n",
    "\n",
    "    # uses string interpolation to build the full query statement\n",
    "    QUERY = Q_LAYOUT.format(group_col, full_statement, group_col)\n",
    "\n",
    "    # registers the dataframe as temporary table:\n",
    "    df.registerTempTable(\"df\")\n",
    "    df_grouped = spark.sql(QUERY)\n",
    "\n",
    "    # rounds values:\n",
    "    for column in df_grouped.columns:\n",
    "        df_grouped = df_grouped.withColumn(column, F.round(F.col(column), 1))\n",
    "\n",
    "    return df_grouped\n",
    "\n",
    "\n",
    "######### Text Processing Functions ########\n",
    "@udf(\"string\")\n",
    "def normalize_text(text):\n",
    "    \"\"\"Helper function to normalize text data to ASCII and lower case, removing spaces\n",
    "\n",
    "    Args:\n",
    "        text (string): the string that needs to be normalized\n",
    "\n",
    "    Returns:\n",
    "        text (string): cleaned up string\n",
    "\n",
    "    \"\"\"\n",
    "    regex = r\"[^a-zA-Z0-9]+\"\n",
    "\n",
    "    if text is not None:\n",
    "\n",
    "        text = str(text)\n",
    "        text = text.lower()\n",
    "        text = re.sub(regex, \" \", text)\n",
    "        text = text.strip()\n",
    "        text = str(\n",
    "            unicodedata.normalize(\"NFKD\", text).encode(\"ASCII\", \"ignore\"), \"utf-8\"\n",
    "        )\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_null_columns(df, normalize=False):\n",
    "    \"\"\"Helper function to print the number of null records for each column of a PySpark DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pyspark.sql.dataframe.DataFrame): a PySpark Dataframe object\n",
    "\n",
    "    Returns:\n",
    "        None -> prints to standard out\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if normalize:\n",
    "        total = df.count()\n",
    "\n",
    "        df_nulls = df.select(\n",
    "            [\n",
    "                (F.sum(F.when(F.col(column).isNull(), 1).otherwise(0)) / total).alias(\n",
    "                    column\n",
    "                )\n",
    "                for column in df.columns\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        df_nulls = df.select(\n",
    "            [\n",
    "                F.sum(F.when(F.col(column).isNull(), 1).otherwise(0)).alias(column)\n",
    "                for column in df.columns\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # displaying the results to standard out\n",
    "    df_nulls.show(1, truncate=False, vertical=True)\n",
    "\n",
    "\n",
    "@udf(\"boolean\")\n",
    "def is_set_or_pack(text):\n",
    "\n",
    "    # description entries to match:\n",
    "    set_descriptions = {\"set\", \"set of\", \"pack\", \"pack of\", \"box\", \"box of\"}\n",
    "\n",
    "    if text is not None:\n",
    "        text = str(text)\n",
    "\n",
    "        if text in set_descriptions:\n",
    "            return True\n",
    "\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "@udf(\"integer\")\n",
    "def get_unit_size(text):\n",
    "\n",
    "    if text is not None:\n",
    "        check_if_digit = len(re.findall(r\"(\\d+)\", text)) > 0\n",
    "\n",
    "        if check_if_digit:\n",
    "            set_size = int(re.findall(r\"(\\d+)\", text)[0])\n",
    "            return set_size\n",
    "\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "@udf(\"boolean\")\n",
    "def has_non_digits_only(text):\n",
    "    \"\"\"Function to match entries in the dataset that are purely non-digit characters\n",
    "\n",
    "    Args:\n",
    "        text (str): string containing the invoice code\n",
    "\n",
    "    Returns:\n",
    "        boolean: whether the text contains non-digit characters and is not related to cancellations\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if text is not None:\n",
    "        condition = all(character.isalpha() for character in text)\n",
    "\n",
    "        if condition:\n",
    "            return True\n",
    "\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading and Inspecting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"# loading the raw dataset:\\ndf_clean = (spark\\n              .read\\n              .parquet('../data/processed/tb_ecommerce.parquet'))\";\n",
       "                var nbb_formatted_code = \"# loading the raw dataset:\\ndf_clean = spark.read.parquet(\\\"../data/processed/tb_ecommerce.parquet\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loading the raw dataset:\n",
    "df_clean = spark.read.parquet(\"../data/processed/tb_ecommerce.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"# instantiating the SQL Context:\\nsql_context = SQLContext(spark)\\n\\n# generating a temporary view of the raw dataframe:\\nsql_context.registerDataFrameAsTable(df_clean, \\n                                     \\\"tb_ecommerce\\\")\";\n",
       "                var nbb_formatted_code = \"# instantiating the SQL Context:\\nsql_context = SQLContext(spark)\\n\\n# generating a temporary view of the raw dataframe:\\nsql_context.registerDataFrameAsTable(df_clean, \\\"tb_ecommerce\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# instantiating the SQL Context:\n",
    "sql_context = SQLContext(spark)\n",
    "\n",
    "# generating a temporary view of the raw dataframe:\n",
    "sql_context.registerDataFrameAsTable(df_clean, \"tb_ecommerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- invoice_no: string (nullable = true)\n",
      " |-- invoice_date: date (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- stock_code: string (nullable = true)\n",
      " |-- quantity: string (nullable = true)\n",
      " |-- unit_price: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- is_missing_customer_id: boolean (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- day_of_month: integer (nullable = true)\n",
      " |-- day_of_year: integer (nullable = true)\n",
      " |-- week_of_year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- quarter: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- is_commercial_holiday: boolean (nullable = true)\n",
      " |-- is_commercial_holiday_week: boolean (nullable = true)\n",
      " |-- is_commercial_holiday_month: boolean (nullable = true)\n",
      " |-- is_bank_holiday: boolean (nullable = true)\n",
      " |-- is_bank_holiday_week: boolean (nullable = true)\n",
      " |-- is_bank_holiday_month: boolean (nullable = true)\n",
      " |-- is_cancelled: boolean (nullable = true)\n",
      " |-- is_return: boolean (nullable = true)\n",
      " |-- is_free_item: boolean (nullable = true)\n",
      " |-- total_item_price: double (nullable = true)\n",
      " |-- retail_price: string (nullable = true)\n",
      " |-- min_unit_price: double (nullable = true)\n",
      " |-- avg_unit_price: double (nullable = true)\n",
      " |-- median_unit_price: double (nullable = true)\n",
      " |-- max_unit_price: double (nullable = true)\n",
      " |-- is_discounted_item: boolean (nullable = true)\n",
      " |-- has_non_digit: boolean (nullable = true)\n",
      " |-- is_postage: boolean (nullable = true)\n",
      " |-- is_manual: boolean (nullable = true)\n",
      " |-- is_discount: boolean (nullable = true)\n",
      " |-- is_fee: boolean (nullable = true)\n",
      " |-- days_to_next_commercial_holiday: integer (nullable = true)\n",
      " |-- days_to_next_bank_holiday: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"# checking the table's schema:\\ndf_clean.printSchema()\";\n",
       "                var nbb_formatted_code = \"# checking the table's schema:\\ndf_clean.printSchema()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# checking the table's schema:\n",
    "df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>invoice_no</th><th>invoice_date</th><th>description</th><th>stock_code</th><th>quantity</th><th>unit_price</th><th>customer_id</th><th>country</th><th>is_missing_customer_id</th><th>day_of_week</th><th>day_of_month</th><th>day_of_year</th><th>week_of_year</th><th>month</th><th>quarter</th><th>year</th><th>is_commercial_holiday</th><th>is_commercial_holiday_week</th><th>is_commercial_holiday_month</th><th>is_bank_holiday</th><th>is_bank_holiday_week</th><th>is_bank_holiday_month</th><th>is_cancelled</th><th>is_return</th><th>is_free_item</th><th>total_item_price</th><th>retail_price</th><th>min_unit_price</th><th>avg_unit_price</th><th>median_unit_price</th><th>max_unit_price</th><th>is_discounted_item</th><th>has_non_digit</th><th>is_postage</th><th>is_manual</th><th>is_discount</th><th>is_fee</th><th>days_to_next_commercial_holiday</th><th>days_to_next_bank_holiday</th></tr>\n",
       "<tr><td>536938</td><td>2016-12-01</td><td>green 3 piece pol...</td><td>84997A</td><td>24</td><td>3.75</td><td>14680</td><td>united kingdom</td><td>false</td><td>5</td><td>1</td><td>336</td><td>48</td><td>12</td><td>4</td><td>2016</td><td>false</td><td>false</td><td>true</td><td>false</td><td>false</td><td>true</td><td>false</td><td>false</td><td>false</td><td>90.0</td><td>3.75</td><td>3.39</td><td>4.340506329113922</td><td>3.75</td><td>8.29</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>25</td><td>24</td></tr>\n",
       "<tr><td>536938</td><td>2016-12-01</td><td>jumbo bag pink po...</td><td>22386</td><td>20</td><td>1.95</td><td>14680</td><td>united kingdom</td><td>false</td><td>5</td><td>1</td><td>336</td><td>48</td><td>12</td><td>4</td><td>2016</td><td>false</td><td>false</td><td>true</td><td>false</td><td>false</td><td>true</td><td>false</td><td>false</td><td>false</td><td>39.0</td><td>2.08</td><td>1.65</td><td>2.5887769784172683</td><td>2.08</td><td>5.06</td><td>true</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>25</td><td>24</td></tr>\n",
       "<tr><td>536938</td><td>2016-12-01</td><td>white skull hot w...</td><td>21479</td><td>72</td><td>3.39</td><td>14680</td><td>united kingdom</td><td>false</td><td>5</td><td>1</td><td>336</td><td>48</td><td>12</td><td>4</td><td>2016</td><td>false</td><td>false</td><td>true</td><td>false</td><td>false</td><td>true</td><td>false</td><td>false</td><td>false</td><td>244.08</td><td>4.25</td><td>0.0</td><td>4.9614864864864865</td><td>4.25</td><td>9.13</td><td>true</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>25</td><td>24</td></tr>\n",
       "<tr><td>536938</td><td>2016-12-01</td><td>victorian sewing ...</td><td>21258</td><td>24</td><td>10.95</td><td>14680</td><td>united kingdom</td><td>false</td><td>5</td><td>1</td><td>336</td><td>48</td><td>12</td><td>4</td><td>2016</td><td>false</td><td>false</td><td>true</td><td>false</td><td>false</td><td>true</td><td>false</td><td>false</td><td>false</td><td>262.79999999999995</td><td>12.75</td><td>0.0</td><td>16.494450549450555</td><td>12.75</td><td>25.49</td><td>true</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>25</td><td>24</td></tr>\n",
       "<tr><td>536938</td><td>2016-12-01</td><td>red retrospot cha...</td><td>20724</td><td>20</td><td>0.85</td><td>14680</td><td>united kingdom</td><td>false</td><td>5</td><td>1</td><td>336</td><td>48</td><td>12</td><td>4</td><td>2016</td><td>false</td><td>false</td><td>true</td><td>false</td><td>false</td><td>true</td><td>false</td><td>false</td><td>true</td><td>17.0</td><td>0.85</td><td>0.0</td><td>1.1348314606741545</td><td>0.85</td><td>5.63</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>false</td><td>25</td><td>24</td></tr>\n",
       "</table>\n",
       "only showing top 5 rows\n"
      ],
      "text/plain": [
       "+----------+------------+--------------------+----------+--------+----------+-----------+--------------+----------------------+-----------+------------+-----------+------------+-----+-------+----+---------------------+--------------------------+---------------------------+---------------+--------------------+---------------------+------------+---------+------------+------------------+------------+--------------+------------------+-----------------+--------------+------------------+-------------+----------+---------+-----------+------+-------------------------------+-------------------------+\n",
       "|invoice_no|invoice_date|         description|stock_code|quantity|unit_price|customer_id|       country|is_missing_customer_id|day_of_week|day_of_month|day_of_year|week_of_year|month|quarter|year|is_commercial_holiday|is_commercial_holiday_week|is_commercial_holiday_month|is_bank_holiday|is_bank_holiday_week|is_bank_holiday_month|is_cancelled|is_return|is_free_item|  total_item_price|retail_price|min_unit_price|    avg_unit_price|median_unit_price|max_unit_price|is_discounted_item|has_non_digit|is_postage|is_manual|is_discount|is_fee|days_to_next_commercial_holiday|days_to_next_bank_holiday|\n",
       "+----------+------------+--------------------+----------+--------+----------+-----------+--------------+----------------------+-----------+------------+-----------+------------+-----+-------+----+---------------------+--------------------------+---------------------------+---------------+--------------------+---------------------+------------+---------+------------+------------------+------------+--------------+------------------+-----------------+--------------+------------------+-------------+----------+---------+-----------+------+-------------------------------+-------------------------+\n",
       "|    536938|  2016-12-01|green 3 piece pol...|    84997A|      24|      3.75|      14680|united kingdom|                 false|          5|           1|        336|          48|   12|      4|2016|                false|                     false|                       true|          false|               false|                 true|       false|    false|       false|              90.0|        3.75|          3.39| 4.340506329113922|             3.75|          8.29|             false|        false|     false|    false|      false| false|                             25|                       24|\n",
       "|    536938|  2016-12-01|jumbo bag pink po...|     22386|      20|      1.95|      14680|united kingdom|                 false|          5|           1|        336|          48|   12|      4|2016|                false|                     false|                       true|          false|               false|                 true|       false|    false|       false|              39.0|        2.08|          1.65|2.5887769784172683|             2.08|          5.06|              true|        false|     false|    false|      false| false|                             25|                       24|\n",
       "|    536938|  2016-12-01|white skull hot w...|     21479|      72|      3.39|      14680|united kingdom|                 false|          5|           1|        336|          48|   12|      4|2016|                false|                     false|                       true|          false|               false|                 true|       false|    false|       false|            244.08|        4.25|           0.0|4.9614864864864865|             4.25|          9.13|              true|        false|     false|    false|      false| false|                             25|                       24|\n",
       "|    536938|  2016-12-01|victorian sewing ...|     21258|      24|     10.95|      14680|united kingdom|                 false|          5|           1|        336|          48|   12|      4|2016|                false|                     false|                       true|          false|               false|                 true|       false|    false|       false|262.79999999999995|       12.75|           0.0|16.494450549450555|            12.75|         25.49|              true|        false|     false|    false|      false| false|                             25|                       24|\n",
       "|    536938|  2016-12-01|red retrospot cha...|     20724|      20|      0.85|      14680|united kingdom|                 false|          5|           1|        336|          48|   12|      4|2016|                false|                     false|                       true|          false|               false|                 true|       false|    false|        true|              17.0|        0.85|           0.0|1.1348314606741545|             0.85|          5.63|             false|        false|     false|    false|      false| false|                             25|                       24|\n",
       "+----------+------------+--------------------+----------+--------+----------+-----------+--------------+----------------------+-----------+------------+-----------+------------+-----+-------+----+---------------------+--------------------------+---------------------------+---------------+--------------------+---------------------+------------+---------+------------+------------------+------------+--------------+------------------+-----------------+--------------+------------------+-------------+----------+---------+-----------+------+-------------------------------+-------------------------+\n",
       "only showing top 5 rows"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"# visualizing a few entries:\\ndf_clean\";\n",
       "                var nbb_formatted_code = \"# visualizing a few entries:\\ndf_clean\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualizing a few entries:\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 16;\n",
       "                var nbb_unformatted_code = \"# generating a few summary values:\\ntotal_revenue = (df_clean\\n                     .select(F.sum(F.col('total_item_price'))\\n                              .alias('total_revenue'))\\n                     .collect()[0]\\n                     ['total_revenue'])\\n\\ntotal_items_sold = (df_clean\\n                     .select(F.sum(F.col('quantity'))\\n                              .alias('total_items_sold'))\\n                     .collect()[0]\\n                     ['total_items_sold'])\";\n",
       "                var nbb_formatted_code = \"# generating a few summary values:\\ntotal_revenue = df_clean.select(\\n    F.sum(F.col(\\\"total_item_price\\\")).alias(\\\"total_revenue\\\")\\n).collect()[0][\\\"total_revenue\\\"]\\n\\ntotal_items_sold = df_clean.select(\\n    F.sum(F.col(\\\"quantity\\\")).alias(\\\"total_items_sold\\\")\\n).collect()[0][\\\"total_items_sold\\\"]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generating a few summary values:\n",
    "total_revenue = df_clean.select(\n",
    "    F.sum(F.col(\"total_item_price\")).alias(\"total_revenue\")\n",
    ").collect()[0][\"total_revenue\"]\n",
    "\n",
    "total_items_sold = df_clean.select(\n",
    "    F.sum(F.col(\"quantity\")).alias(\"total_items_sold\")\n",
    ").collect()[0][\"total_items_sold\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Implementing Product Features\n",
    "I will follow the hypothesis/features planning below to implement relevant features from a product perspective.\n",
    "\n",
    "<img src=\"../reports/figures/Product Entity.png\" alt = \"Product Entity Map\" style = \"width:1182px; height=702px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Product Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 17;\n",
       "                var nbb_unformatted_code = \"# product metadata (price, description, et cetera)\\ndf_product_info = (df_clean\\n                      .groupby('stock_code')\\n                      .agg(\\n                          F.first(F.col('description')).alias('product_description'),\\n                          F.length(F.first(F.col('description'))).alias('description_length'),\\n                          F.first(F.col('retail_price')).alias('retail_price'),\\n                          F.first(F.abs(F.col('min_unit_price'))).alias('min_product_unit_price'),\\n                          F.first(F.abs(F.col('max_unit_price'))).alias('max_product_unit_price'),\\n                          F.first(F.abs(F.col('avg_unit_price'))).alias('avg_product_unit_price'),\\n                          F.first(F.abs(F.col('median_unit_price'))).alias('median_product_unit_price'),\\n                          F.sum(F.abs(F.col('total_item_price'))).alias('total_spent_on'),\\n                          (F.sum(F.abs(F.col('total_item_price'))) / F.lit(total_revenue)).alias('total_contribution_revenue'),\\n                          F.sum(F.abs(F.col('quantity'))).alias('total_units_sold'),\\n                          (F.sum(F.abs(F.col('quantity'))) / F.lit(total_items_sold)).alias('total_contribution_units')\\n                      ))\";\n",
       "                var nbb_formatted_code = \"# product metadata (price, description, et cetera)\\ndf_product_info = df_clean.groupby(\\\"stock_code\\\").agg(\\n    F.first(F.col(\\\"description\\\")).alias(\\\"product_description\\\"),\\n    F.length(F.first(F.col(\\\"description\\\"))).alias(\\\"description_length\\\"),\\n    F.first(F.col(\\\"retail_price\\\")).alias(\\\"retail_price\\\"),\\n    F.first(F.abs(F.col(\\\"min_unit_price\\\"))).alias(\\\"min_product_unit_price\\\"),\\n    F.first(F.abs(F.col(\\\"max_unit_price\\\"))).alias(\\\"max_product_unit_price\\\"),\\n    F.first(F.abs(F.col(\\\"avg_unit_price\\\"))).alias(\\\"avg_product_unit_price\\\"),\\n    F.first(F.abs(F.col(\\\"median_unit_price\\\"))).alias(\\\"median_product_unit_price\\\"),\\n    F.sum(F.abs(F.col(\\\"total_item_price\\\"))).alias(\\\"total_spent_on\\\"),\\n    (F.sum(F.abs(F.col(\\\"total_item_price\\\"))) / F.lit(total_revenue)).alias(\\n        \\\"total_contribution_revenue\\\"\\n    ),\\n    F.sum(F.abs(F.col(\\\"quantity\\\"))).alias(\\\"total_units_sold\\\"),\\n    (F.sum(F.abs(F.col(\\\"quantity\\\"))) / F.lit(total_items_sold)).alias(\\n        \\\"total_contribution_units\\\"\\n    ),\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# product metadata (price, description, et cetera)\n",
    "df_product_info = df_clean.groupby(\"stock_code\").agg(\n",
    "    F.first(F.col(\"description\")).alias(\"product_description\"),\n",
    "    F.length(F.first(F.col(\"description\"))).alias(\"description_length\"),\n",
    "    F.first(F.col(\"retail_price\")).alias(\"retail_price\"),\n",
    "    F.first(F.abs(F.col(\"min_unit_price\"))).alias(\"min_product_unit_price\"),\n",
    "    F.first(F.abs(F.col(\"max_unit_price\"))).alias(\"max_product_unit_price\"),\n",
    "    F.first(F.abs(F.col(\"avg_unit_price\"))).alias(\"avg_product_unit_price\"),\n",
    "    F.first(F.abs(F.col(\"median_unit_price\"))).alias(\"median_product_unit_price\"),\n",
    "    F.sum(F.abs(F.col(\"total_item_price\"))).alias(\"total_spent_on\"),\n",
    "    (F.sum(F.abs(F.col(\"total_item_price\"))) / F.lit(total_revenue)).alias(\n",
    "        \"total_contribution_revenue\"\n",
    "    ),\n",
    "    F.sum(F.abs(F.col(\"quantity\"))).alias(\"total_units_sold\"),\n",
    "    (F.sum(F.abs(F.col(\"quantity\"))) / F.lit(total_items_sold)).alias(\n",
    "        \"total_contribution_units\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Sets, packs and boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"# extracting the type of product (set or pack instead of invidiual item)\\ndf_product_info = (df_product_info\\n                      .withColumn('is_set_or_pack',\\n                                   is_set_or_pack(F.col('product_description'))))\";\n",
       "                var nbb_formatted_code = \"# extracting the type of product (set or pack instead of invidiual item)\\ndf_product_info = df_product_info.withColumn(\\n    \\\"is_set_or_pack\\\", is_set_or_pack(F.col(\\\"product_description\\\"))\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extracting the type of product (set or pack instead of invidiual item)\n",
    "df_product_info = df_product_info.withColumn(\n",
    "    \"is_set_or_pack\", is_set_or_pack(F.col(\"product_description\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 19;\n",
       "                var nbb_unformatted_code = \"# unit size for the products:\\ndf_product_info = (df_product_info\\n                      .withColumn('unit_size',\\n                                   get_unit_size(F.col('product_description'))))\";\n",
       "                var nbb_formatted_code = \"# unit size for the products:\\ndf_product_info = df_product_info.withColumn(\\n    \\\"unit_size\\\", get_unit_size(F.col(\\\"product_description\\\"))\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# unit size for the products:\n",
    "df_product_info = df_product_info.withColumn(\n",
    "    \"unit_size\", get_unit_size(F.col(\"product_description\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- stock_code: string (nullable = true)\n",
      " |-- product_description: string (nullable = true)\n",
      " |-- description_length: integer (nullable = true)\n",
      " |-- retail_price: string (nullable = true)\n",
      " |-- min_product_unit_price: double (nullable = true)\n",
      " |-- max_product_unit_price: double (nullable = true)\n",
      " |-- avg_product_unit_price: double (nullable = true)\n",
      " |-- median_product_unit_price: double (nullable = true)\n",
      " |-- total_spent_on: double (nullable = true)\n",
      " |-- total_contribution_revenue: double (nullable = true)\n",
      " |-- total_units_sold: double (nullable = true)\n",
      " |-- total_contribution_units: double (nullable = true)\n",
      " |-- is_set_or_pack: boolean (nullable = true)\n",
      " |-- unit_size: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 20;\n",
       "                var nbb_unformatted_code = \"df_product_info.printSchema()\";\n",
       "                var nbb_formatted_code = \"df_product_info.printSchema()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_product_info.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Product Price Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 21;\n",
       "                var nbb_unformatted_code = \"# window function for ranking based on specific columns:\\nretail_price_window = (Window\\n                          .orderBy('retail_price'))\\n\\ndf_temp = (df_product_info\\n              .withColumn('retail_price_rank',\\n                          F.percent_rank()\\n                           .over(retail_price_window)))\\n\\ndf_product_price = (df_temp\\n                       .groupby('stock_code')\\n                       .agg(\\n                           F.first(F.col('product_description')).alias('product_description'),\\n                           F.round(F.max(F.col('retail_price_rank')), 3).alias('retail_price_score')\\n                       ))\";\n",
       "                var nbb_formatted_code = \"# window function for ranking based on specific columns:\\nretail_price_window = Window.orderBy(\\\"retail_price\\\")\\n\\ndf_temp = df_product_info.withColumn(\\n    \\\"retail_price_rank\\\", F.percent_rank().over(retail_price_window)\\n)\\n\\ndf_product_price = df_temp.groupby(\\\"stock_code\\\").agg(\\n    F.first(F.col(\\\"product_description\\\")).alias(\\\"product_description\\\"),\\n    F.round(F.max(F.col(\\\"retail_price_rank\\\")), 3).alias(\\\"retail_price_score\\\"),\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# window function for ranking based on specific columns:\n",
    "retail_price_window = Window.orderBy(\"retail_price\")\n",
    "\n",
    "df_temp = df_product_info.withColumn(\n",
    "    \"retail_price_rank\", F.percent_rank().over(retail_price_window)\n",
    ")\n",
    "\n",
    "df_product_price = df_temp.groupby(\"stock_code\").agg(\n",
    "    F.first(F.col(\"product_description\")).alias(\"product_description\"),\n",
    "    F.round(F.max(F.col(\"retail_price_rank\")), 3).alias(\"retail_price_score\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Product Spending Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 23;\n",
       "                var nbb_unformatted_code = \"spent_on_window = (Window\\n                       .orderBy('total_spent_on'))\\n\\nunits_sold_window = (Window\\n                          .orderBy('total_units_sold'))\\n\\ndf_temp = (df_product_info\\n              .withColumn('spent_on_rank',\\n                          F.percent_rank()\\n                           .over(spent_on_window)))\\n\\ndf_temp = (df_temp\\n              .withColumn('units_sold_rank',\\n                          F.percent_rank()\\n                           .over(units_sold_window)))\\n\\ndf_product_spending = (df_temp\\n                           .groupby('stock_code')\\n                           .agg(\\n                               F.first(F.col('product_description')).alias('product_description'),\\n                               F.round(F.max(F.col('spent_on_rank')), 3).alias('revenue_score'),\\n                               F.round(F.max(F.col('units_sold_rank')), 3).alias('quantity_sold_score')\\n                           ))\";\n",
       "                var nbb_formatted_code = \"spent_on_window = Window.orderBy(\\\"total_spent_on\\\")\\n\\nunits_sold_window = Window.orderBy(\\\"total_units_sold\\\")\\n\\ndf_temp = df_product_info.withColumn(\\n    \\\"spent_on_rank\\\", F.percent_rank().over(spent_on_window)\\n)\\n\\ndf_temp = df_temp.withColumn(\\n    \\\"units_sold_rank\\\", F.percent_rank().over(units_sold_window)\\n)\\n\\ndf_product_spending = df_temp.groupby(\\\"stock_code\\\").agg(\\n    F.first(F.col(\\\"product_description\\\")).alias(\\\"product_description\\\"),\\n    F.round(F.max(F.col(\\\"spent_on_rank\\\")), 3).alias(\\\"revenue_score\\\"),\\n    F.round(F.max(F.col(\\\"units_sold_rank\\\")), 3).alias(\\\"quantity_sold_score\\\"),\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spent_on_window = Window.orderBy(\"total_spent_on\")\n",
    "\n",
    "units_sold_window = Window.orderBy(\"total_units_sold\")\n",
    "\n",
    "df_temp = df_product_info.withColumn(\n",
    "    \"spent_on_rank\", F.percent_rank().over(spent_on_window)\n",
    ")\n",
    "\n",
    "df_temp = df_temp.withColumn(\n",
    "    \"units_sold_rank\", F.percent_rank().over(units_sold_window)\n",
    ")\n",
    "\n",
    "df_product_spending = df_temp.groupby(\"stock_code\").agg(\n",
    "    F.first(F.col(\"product_description\")).alias(\"product_description\"),\n",
    "    F.round(F.max(F.col(\"spent_on_rank\")), 3).alias(\"revenue_score\"),\n",
    "    F.round(F.max(F.col(\"units_sold_rank\")), 3).alias(\"quantity_sold_score\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Preparing the Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 25;\n",
       "                var nbb_unformatted_code = \"df_product = (df_product_info\\n                 .join(\\n                     df_product_price,\\n                     on = ['stock_code', 'product_description'],\\n                     how = 'left'\\n                 ))\\n\\ndf_product = (df_product\\n                 .join(\\n                     df_product_spending,\\n                     on = ['stock_code', 'product_description'],\\n                     how = 'left'\\n                 ))\\n\\ndf_product = (df_product\\n                 .drop_duplicates(subset=['stock_code',\\n                                          'product_description']))\";\n",
       "                var nbb_formatted_code = \"df_product = df_product_info.join(\\n    df_product_price, on=[\\\"stock_code\\\", \\\"product_description\\\"], how=\\\"left\\\"\\n)\\n\\ndf_product = df_product.join(\\n    df_product_spending, on=[\\\"stock_code\\\", \\\"product_description\\\"], how=\\\"left\\\"\\n)\\n\\ndf_product = df_product.drop_duplicates(subset=[\\\"stock_code\\\", \\\"product_description\\\"])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_product = df_product_info.join(\n",
    "    df_product_price, on=[\"stock_code\", \"product_description\"], how=\"left\"\n",
    ")\n",
    "\n",
    "df_product = df_product.join(\n",
    "    df_product_spending, on=[\"stock_code\", \"product_description\"], how=\"left\"\n",
    ")\n",
    "\n",
    "df_product = df_product.drop_duplicates(subset=[\"stock_code\", \"product_description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Saving the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the enhanced raw data as parquet in the processed step of the pipeline\n",
    "PROCESSED_DATA_DIR = '../data/processed'\n",
    "\n",
    "\n",
    "# using the helper function to save the file:\n",
    "save_to_filesystem(df_product, \n",
    "                   PROCESSED_DATA_DIR,\n",
    "                   'tb_product',\n",
    "                   'tb_product.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
