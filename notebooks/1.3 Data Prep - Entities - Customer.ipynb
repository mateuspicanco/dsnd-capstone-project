{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep - Customer Entity\n",
    "The `customer` entity represents the features grouped at the single customer level and thus are the most important for performing customer segmentation and clustering tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install inflection >> ../configs/package_installation.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nb_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext nb_black\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\\n%load_ext autoreload \\n%autoreload 2\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\\n%load_ext autoreload\\n%autoreload 2\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"# PySpark dependencies:\\nimport pyspark\\nfrom pyspark import SparkConf\\nfrom pyspark import SparkContext\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql import SQLContext\\nimport pyspark.sql.functions as F\\nfrom pyspark.sql.functions import udf\\nfrom pyspark.sql.types import *\\nfrom pyspark.sql.window import Window\\n\\n# database utilities:\\nfrom sqlalchemy import create_engine\\nimport sqlite3 as db\\nimport pandas as pd\\n\\n# other relevant libraries:\\nimport warnings\\nimport inflection\\nimport unicodedata\\nfrom datetime import datetime, timedelta\\nimport json\\nimport re\\nimport os\\nfrom glob import glob\\nimport shutil\\nimport itertools\\n\\n# setting global parameters for visualizations:\\nwarnings.filterwarnings(\\\"ignore\\\")\\npd.set_option(\\\"display.precision\\\", 4)\\npd.set_option(\\\"display.float_format\\\", lambda x: \\\"%.2f\\\" % x)\";\n",
       "                var nbb_formatted_code = \"# PySpark dependencies:\\nimport pyspark\\nfrom pyspark import SparkConf\\nfrom pyspark import SparkContext\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql import SQLContext\\nimport pyspark.sql.functions as F\\nfrom pyspark.sql.functions import udf\\nfrom pyspark.sql.types import *\\nfrom pyspark.sql.window import Window\\n\\n# database utilities:\\nfrom sqlalchemy import create_engine\\nimport sqlite3 as db\\nimport pandas as pd\\n\\n# other relevant libraries:\\nimport warnings\\nimport inflection\\nimport unicodedata\\nfrom datetime import datetime, timedelta\\nimport json\\nimport re\\nimport os\\nfrom glob import glob\\nimport shutil\\nimport itertools\\n\\n# setting global parameters for visualizations:\\nwarnings.filterwarnings(\\\"ignore\\\")\\npd.set_option(\\\"display.precision\\\", 4)\\npd.set_option(\\\"display.float_format\\\", lambda x: \\\"%.2f\\\" % x)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PySpark dependencies:\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# database utilities:\n",
    "from sqlalchemy import create_engine\n",
    "import sqlite3 as db\n",
    "import pandas as pd\n",
    "\n",
    "# other relevant libraries:\n",
    "import warnings\n",
    "import inflection\n",
    "import unicodedata\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from glob import glob\n",
    "import shutil\n",
    "import itertools\n",
    "\n",
    "# setting global parameters for visualizations:\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "pd.set_option(\"display.float_format\", lambda x: \"%.2f\" % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Building Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"# loading the configurations needed for Spark\\ndef init_spark(app_name):\\n\\n    spark = (\\n        SparkSession.builder.appName(app_name)\\n        .config(\\\"spark.files.overwrite\\\", \\\"true\\\")\\n        .config(\\\"spark.sql.repl.eagerEval.enabled\\\", True)\\n        .config(\\\"spark.sql.repl.eagerEval.maxNumRows\\\", 5)\\n        .config(\\\"spark.sql.legacy.timeParserPolicy\\\", \\\"LEGACY\\\")\\n        .config(\\\"spark.sql.parquet.compression.codec\\\", \\\"gzip\\\")\\n        .enableHiveSupport()\\n        .getOrCreate()\\n    )\\n\\n    return spark\\n\\n\\n# init the spark session:\\nspark = init_spark(\\\"Customer Preparation\\\")\";\n",
       "                var nbb_formatted_code = \"# loading the configurations needed for Spark\\ndef init_spark(app_name):\\n\\n    spark = (\\n        SparkSession.builder.appName(app_name)\\n        .config(\\\"spark.files.overwrite\\\", \\\"true\\\")\\n        .config(\\\"spark.sql.repl.eagerEval.enabled\\\", True)\\n        .config(\\\"spark.sql.repl.eagerEval.maxNumRows\\\", 5)\\n        .config(\\\"spark.sql.legacy.timeParserPolicy\\\", \\\"LEGACY\\\")\\n        .config(\\\"spark.sql.parquet.compression.codec\\\", \\\"gzip\\\")\\n        .enableHiveSupport()\\n        .getOrCreate()\\n    )\\n\\n    return spark\\n\\n\\n# init the spark session:\\nspark = init_spark(\\\"Customer Preparation\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loading the configurations needed for Spark\n",
    "def init_spark(app_name):\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder.appName(app_name)\n",
    "        .config(\"spark.files.overwrite\", \"true\")\n",
    "        .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "        .config(\"spark.sql.repl.eagerEval.maxNumRows\", 5)\n",
    "        .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "        .config(\"spark.sql.parquet.compression.codec\", \"gzip\")\n",
    "        .enableHiveSupport()\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "    return spark\n",
    "\n",
    "\n",
    "# init the spark session:\n",
    "spark = init_spark(\"Customer Preparation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://84a2e6cc83b5:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Customer Preparation</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd59e25ef70>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"# verifying the spark session:\\nspark\";\n",
       "                var nbb_formatted_code = \"# verifying the spark session:\\nspark\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# verifying the spark session:\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"def get_pivot_product_info(df, feature_col):\\n    \\\"\\\"\\\"Helper function to reproduce the pivotting and aggregation procedure for generic columns\\\"\\\"\\\"\\n    df_pivot = (\\n        df.groupby(\\\"customer_id\\\")\\n        .pivot(\\\"type_of_product\\\")\\n        .agg(F.max(F.col(feature_col)))\\n    )\\n\\n    new_cols = [f\\\"{col}_{feature_col}\\\" for col in df_pivot.columns[1:]]\\n\\n    for idx, col in enumerate(new_cols, start=1):\\n        df_pivot = df_pivot.withColumnRenamed(df_pivot.columns[idx], col)\\n\\n    return df_pivot\\n\\n\\ndef save_to_filesystem(df, target_path, parquet_path, filename):\\n    \\\"\\\"\\\"Helper function to save pyspark dataframes as parquets in a way that is similar to writing to local files\\n\\n    Args:\\n        df (pyspark.sql.dataframe.DataFrame): dataframe to be saved\\n        target_path (str): path that will store the file\\n        filename (str): name of the resulting file\\n\\n    Returns:\\n        None\\n    \\\"\\\"\\\"\\n    PARQUET_FILE = f\\\"{target_path}/{parquet_path}\\\"\\n    OUTPUT_FILE = f\\\"{target_path}/{filename}\\\"\\n\\n    if os.path.exists(PARQUET_FILE):\\n        shutil.rmtree(\\n            PARQUET_FILE\\n        )  # if the directory already exists, remove it (throws error if not)\\n\\n    # saves the dataframe:\\n    df.coalesce(1).write.save(PARQUET_FILE)\\n\\n    # retrieves file resulting from the saving procedure:\\n    original_file = glob(f\\\"{PARQUET_FILE}/*.parquet\\\")[0]\\n\\n    # renames the resulting file and saves it to the target directory:\\n    os.rename(original_file, OUTPUT_FILE)\\n\\n    shutil.rmtree(PARQUET_FILE)\\n\\n    return True\\n\\n\\ndef apply_category_map(category_map):\\n    \\\"\\\"\\\"Helper function to convert strings given a map\\n\\n    Note:\\n        This function uses the function generator scheme, much like the PySpark code\\n\\n    Args:\\n        original_category (str): the original category name\\n        category_map (dict): the hash table or dictionary for converting the values:\\n\\n    Returns:\\n        new_category (str): the resulting category\\n\\n    \\\"\\\"\\\"\\n\\n    def func(row):\\n        try:\\n            result = category_map[row]\\n        except:\\n            result = None\\n        return result\\n\\n    return F.udf(func)\\n\\n\\ndef get_datetime_features(df, time_col):\\n    \\\"\\\"\\\"Function to extract time-based features from pyspark dataframes\\n\\n    Args:\\n        df (pyspark.sql.dataframe.DataFrame): the original dataframe that needs to be enriched\\n        time_col (str): the string name of the column containing the date object\\n\\n    Returns:\\n        df (pyspark.sql.dataframe.DataFrame): resulting pyspark dataframe with the added features\\n            -> See list of attribute the source code for the attributes\\n\\n    \\\"\\\"\\\"\\n\\n    # applying date-related functions:\\n\\n    # day-level attributes:\\n    df = df.withColumn(\\\"day_of_week\\\", F.dayofweek(F.col(time_col)))\\n\\n    df = df.withColumn(\\\"day_of_month\\\", F.dayofmonth(F.col(time_col)))\\n\\n    df = df.withColumn(\\\"day_of_year\\\", F.dayofyear(F.col(time_col)))\\n\\n    # week-level attributes:\\n    df = df.withColumn(\\\"week_of_year\\\", F.weekofyear(F.col(time_col)))\\n\\n    # month-level attributes:\\n    df = df.withColumn(\\\"month\\\", F.month(F.col(time_col)))\\n\\n    df = df.withColumn(\\\"quarter\\\", F.quarter(F.col(time_col)))\\n\\n    # year-level attributes:\\n    df = df.withColumn(\\\"year\\\", F.year(F.col(time_col)))\\n\\n    return df\\n\\n\\ndef bulk_aggregate(df, group_col, aggs, target_cols):\\n    \\\"\\\"\\\"Wrapper function to apply multiple aggregations when performing group bys\\n\\n    It utilizes the spark's SQL Context and string interpolation to perform the aggregation using SQL syntax.\\n\\n    Args:\\n        df (pyspark.sql.dataframe.DataFrame): dataframe with raw data\\n        group_col (str): the column that will be used for grouping\\n        aggs (list): list of aggregations that want to be made (must be the same name as pyspark.sql.functions)\\n        target_cols (str): columns in which aggregations will be performed\\n\\n    Returns:\\n        df_grouped (pyspark.sql.dataframe.DataFrame): dataframe with the grouped data\\n    \\\"\\\"\\\"\\n\\n    # buils the cartersian product of the lists\\n    aggs_to_perform = itertools.product(aggs, target_cols)\\n\\n    Q_LAYOUT = \\\"\\\"\\\"\\n    SELECT\\n        {},\\n        {}\\n        FROM df\\n        GROUP BY {}\\n    \\\"\\\"\\\"\\n\\n    aggregations = []\\n    for agg, col in aggs_to_perform:\\n\\n        # builds the string for aggregation\\n        statement = f\\\"{agg.upper()}({col}) as {agg}_{col}\\\"\\n        aggregations.append(statement)\\n\\n    full_statement = \\\",\\\\n\\\".join(aggregations)\\n\\n    # uses string interpolation to build the full query statement\\n    QUERY = Q_LAYOUT.format(group_col, full_statement, group_col)\\n\\n    # registers the dataframe as temporary table:\\n    df.registerTempTable(\\\"df\\\")\\n    df_grouped = spark.sql(QUERY)\\n\\n    # rounds values:\\n    for column in df_grouped.columns:\\n        df_grouped = df_grouped.withColumn(column, F.round(F.col(column), 1))\\n\\n    return df_grouped\\n\\n\\n######### Text Processing Functions ########\\n@udf(\\\"string\\\")\\ndef normalize_text(text):\\n    \\\"\\\"\\\"Helper function to normalize text data to ASCII and lower case, removing spaces\\n\\n    Args:\\n        text (string): the string that needs to be normalized\\n\\n    Returns:\\n        text (string): cleaned up string\\n\\n    \\\"\\\"\\\"\\n    regex = r\\\"[^a-zA-Z0-9]+\\\"\\n\\n    if text is not None:\\n\\n        text = str(text)\\n        text = text.lower()\\n        text = re.sub(regex, \\\" \\\", text)\\n        text = text.strip()\\n        text = str(\\n            unicodedata.normalize(\\\"NFKD\\\", text).encode(\\\"ASCII\\\", \\\"ignore\\\"), \\\"utf-8\\\"\\n        )\\n\\n    return text\\n\\n\\ndef get_null_columns(df, normalize=False):\\n    \\\"\\\"\\\"Helper function to print the number of null records for each column of a PySpark DataFrame.\\n\\n    Args:\\n        df (pyspark.sql.dataframe.DataFrame): a PySpark Dataframe object\\n\\n    Returns:\\n        None -> prints to standard out\\n\\n    \\\"\\\"\\\"\\n\\n    if normalize:\\n        total = df.count()\\n\\n        df_nulls = df.select(\\n            [\\n                (F.sum(F.when(F.col(column).isNull(), 1).otherwise(0)) / total).alias(\\n                    column\\n                )\\n                for column in df.columns\\n            ]\\n        )\\n\\n    else:\\n        df_nulls = df.select(\\n            [\\n                F.sum(F.when(F.col(column).isNull(), 1).otherwise(0)).alias(column)\\n                for column in df.columns\\n            ]\\n        )\\n\\n    # displaying the results to standard out\\n    df_nulls.show(1, truncate=False, vertical=True)\\n\\n\\n@udf(\\\"boolean\\\")\\ndef is_set_or_pack(text):\\n\\n    # description entries to match:\\n    set_descriptions = {\\\"set\\\", \\\"set of\\\", \\\"pack\\\", \\\"pack of\\\", \\\"box\\\", \\\"box of\\\"}\\n\\n    if text is not None:\\n        text = str(text)\\n\\n        if text in set_descriptions:\\n            return True\\n\\n        else:\\n            return False\\n\\n    else:\\n        return False\\n\\n\\n@udf(\\\"integer\\\")\\ndef get_unit_size(text):\\n\\n    if text is not None:\\n        check_if_digit = len(re.findall(r\\\"(\\\\d+)\\\", text)) > 0\\n\\n        if check_if_digit:\\n            set_size = int(re.findall(r\\\"(\\\\d+)\\\", text)[0])\\n            return set_size\\n\\n        else:\\n            return 1\\n\\n    else:\\n        return 1\\n\\n\\n@udf(\\\"boolean\\\")\\ndef has_non_digits_only(text):\\n    \\\"\\\"\\\"Function to match entries in the dataset that are purely non-digit characters\\n\\n    Args:\\n        text (str): string containing the invoice code\\n\\n    Returns:\\n        boolean: whether the text contains non-digit characters and is not related to cancellations\\n\\n    \\\"\\\"\\\"\\n\\n    if text is not None:\\n        condition = all(character.isalpha() for character in text)\\n\\n        if condition:\\n            return True\\n\\n        else:\\n            return False\\n\\n    else:\\n        return False\";\n",
       "                var nbb_formatted_code = \"def get_pivot_product_info(df, feature_col):\\n    \\\"\\\"\\\"Helper function to reproduce the pivotting and aggregation procedure for generic columns\\\"\\\"\\\"\\n    df_pivot = (\\n        df.groupby(\\\"customer_id\\\")\\n        .pivot(\\\"type_of_product\\\")\\n        .agg(F.max(F.col(feature_col)))\\n    )\\n\\n    new_cols = [f\\\"{col}_{feature_col}\\\" for col in df_pivot.columns[1:]]\\n\\n    for idx, col in enumerate(new_cols, start=1):\\n        df_pivot = df_pivot.withColumnRenamed(df_pivot.columns[idx], col)\\n\\n    return df_pivot\\n\\n\\ndef save_to_filesystem(df, target_path, parquet_path, filename):\\n    \\\"\\\"\\\"Helper function to save pyspark dataframes as parquets in a way that is similar to writing to local files\\n\\n    Args:\\n        df (pyspark.sql.dataframe.DataFrame): dataframe to be saved\\n        target_path (str): path that will store the file\\n        filename (str): name of the resulting file\\n\\n    Returns:\\n        None\\n    \\\"\\\"\\\"\\n    PARQUET_FILE = f\\\"{target_path}/{parquet_path}\\\"\\n    OUTPUT_FILE = f\\\"{target_path}/{filename}\\\"\\n\\n    if os.path.exists(PARQUET_FILE):\\n        shutil.rmtree(\\n            PARQUET_FILE\\n        )  # if the directory already exists, remove it (throws error if not)\\n\\n    # saves the dataframe:\\n    df.coalesce(1).write.save(PARQUET_FILE)\\n\\n    # retrieves file resulting from the saving procedure:\\n    original_file = glob(f\\\"{PARQUET_FILE}/*.parquet\\\")[0]\\n\\n    # renames the resulting file and saves it to the target directory:\\n    os.rename(original_file, OUTPUT_FILE)\\n\\n    shutil.rmtree(PARQUET_FILE)\\n\\n    return True\\n\\n\\ndef apply_category_map(category_map):\\n    \\\"\\\"\\\"Helper function to convert strings given a map\\n\\n    Note:\\n        This function uses the function generator scheme, much like the PySpark code\\n\\n    Args:\\n        original_category (str): the original category name\\n        category_map (dict): the hash table or dictionary for converting the values:\\n\\n    Returns:\\n        new_category (str): the resulting category\\n\\n    \\\"\\\"\\\"\\n\\n    def func(row):\\n        try:\\n            result = category_map[row]\\n        except:\\n            result = None\\n        return result\\n\\n    return F.udf(func)\\n\\n\\ndef get_datetime_features(df, time_col):\\n    \\\"\\\"\\\"Function to extract time-based features from pyspark dataframes\\n\\n    Args:\\n        df (pyspark.sql.dataframe.DataFrame): the original dataframe that needs to be enriched\\n        time_col (str): the string name of the column containing the date object\\n\\n    Returns:\\n        df (pyspark.sql.dataframe.DataFrame): resulting pyspark dataframe with the added features\\n            -> See list of attribute the source code for the attributes\\n\\n    \\\"\\\"\\\"\\n\\n    # applying date-related functions:\\n\\n    # day-level attributes:\\n    df = df.withColumn(\\\"day_of_week\\\", F.dayofweek(F.col(time_col)))\\n\\n    df = df.withColumn(\\\"day_of_month\\\", F.dayofmonth(F.col(time_col)))\\n\\n    df = df.withColumn(\\\"day_of_year\\\", F.dayofyear(F.col(time_col)))\\n\\n    # week-level attributes:\\n    df = df.withColumn(\\\"week_of_year\\\", F.weekofyear(F.col(time_col)))\\n\\n    # month-level attributes:\\n    df = df.withColumn(\\\"month\\\", F.month(F.col(time_col)))\\n\\n    df = df.withColumn(\\\"quarter\\\", F.quarter(F.col(time_col)))\\n\\n    # year-level attributes:\\n    df = df.withColumn(\\\"year\\\", F.year(F.col(time_col)))\\n\\n    return df\\n\\n\\ndef bulk_aggregate(df, group_col, aggs, target_cols):\\n    \\\"\\\"\\\"Wrapper function to apply multiple aggregations when performing group bys\\n\\n    It utilizes the spark's SQL Context and string interpolation to perform the aggregation using SQL syntax.\\n\\n    Args:\\n        df (pyspark.sql.dataframe.DataFrame): dataframe with raw data\\n        group_col (str): the column that will be used for grouping\\n        aggs (list): list of aggregations that want to be made (must be the same name as pyspark.sql.functions)\\n        target_cols (str): columns in which aggregations will be performed\\n\\n    Returns:\\n        df_grouped (pyspark.sql.dataframe.DataFrame): dataframe with the grouped data\\n    \\\"\\\"\\\"\\n\\n    # buils the cartersian product of the lists\\n    aggs_to_perform = itertools.product(aggs, target_cols)\\n\\n    Q_LAYOUT = \\\"\\\"\\\"\\n    SELECT\\n        {},\\n        {}\\n        FROM df\\n        GROUP BY {}\\n    \\\"\\\"\\\"\\n\\n    aggregations = []\\n    for agg, col in aggs_to_perform:\\n\\n        # builds the string for aggregation\\n        statement = f\\\"{agg.upper()}({col}) as {agg}_{col}\\\"\\n        aggregations.append(statement)\\n\\n    full_statement = \\\",\\\\n\\\".join(aggregations)\\n\\n    # uses string interpolation to build the full query statement\\n    QUERY = Q_LAYOUT.format(group_col, full_statement, group_col)\\n\\n    # registers the dataframe as temporary table:\\n    df.registerTempTable(\\\"df\\\")\\n    df_grouped = spark.sql(QUERY)\\n\\n    # rounds values:\\n    for column in df_grouped.columns:\\n        df_grouped = df_grouped.withColumn(column, F.round(F.col(column), 1))\\n\\n    return df_grouped\\n\\n\\n######### Text Processing Functions ########\\n@udf(\\\"string\\\")\\ndef normalize_text(text):\\n    \\\"\\\"\\\"Helper function to normalize text data to ASCII and lower case, removing spaces\\n\\n    Args:\\n        text (string): the string that needs to be normalized\\n\\n    Returns:\\n        text (string): cleaned up string\\n\\n    \\\"\\\"\\\"\\n    regex = r\\\"[^a-zA-Z0-9]+\\\"\\n\\n    if text is not None:\\n\\n        text = str(text)\\n        text = text.lower()\\n        text = re.sub(regex, \\\" \\\", text)\\n        text = text.strip()\\n        text = str(\\n            unicodedata.normalize(\\\"NFKD\\\", text).encode(\\\"ASCII\\\", \\\"ignore\\\"), \\\"utf-8\\\"\\n        )\\n\\n    return text\\n\\n\\ndef get_null_columns(df, normalize=False):\\n    \\\"\\\"\\\"Helper function to print the number of null records for each column of a PySpark DataFrame.\\n\\n    Args:\\n        df (pyspark.sql.dataframe.DataFrame): a PySpark Dataframe object\\n\\n    Returns:\\n        None -> prints to standard out\\n\\n    \\\"\\\"\\\"\\n\\n    if normalize:\\n        total = df.count()\\n\\n        df_nulls = df.select(\\n            [\\n                (F.sum(F.when(F.col(column).isNull(), 1).otherwise(0)) / total).alias(\\n                    column\\n                )\\n                for column in df.columns\\n            ]\\n        )\\n\\n    else:\\n        df_nulls = df.select(\\n            [\\n                F.sum(F.when(F.col(column).isNull(), 1).otherwise(0)).alias(column)\\n                for column in df.columns\\n            ]\\n        )\\n\\n    # displaying the results to standard out\\n    df_nulls.show(1, truncate=False, vertical=True)\\n\\n\\n@udf(\\\"boolean\\\")\\ndef is_set_or_pack(text):\\n\\n    # description entries to match:\\n    set_descriptions = {\\\"set\\\", \\\"set of\\\", \\\"pack\\\", \\\"pack of\\\", \\\"box\\\", \\\"box of\\\"}\\n\\n    if text is not None:\\n        text = str(text)\\n\\n        if text in set_descriptions:\\n            return True\\n\\n        else:\\n            return False\\n\\n    else:\\n        return False\\n\\n\\n@udf(\\\"integer\\\")\\ndef get_unit_size(text):\\n\\n    if text is not None:\\n        check_if_digit = len(re.findall(r\\\"(\\\\d+)\\\", text)) > 0\\n\\n        if check_if_digit:\\n            set_size = int(re.findall(r\\\"(\\\\d+)\\\", text)[0])\\n            return set_size\\n\\n        else:\\n            return 1\\n\\n    else:\\n        return 1\\n\\n\\n@udf(\\\"boolean\\\")\\ndef has_non_digits_only(text):\\n    \\\"\\\"\\\"Function to match entries in the dataset that are purely non-digit characters\\n\\n    Args:\\n        text (str): string containing the invoice code\\n\\n    Returns:\\n        boolean: whether the text contains non-digit characters and is not related to cancellations\\n\\n    \\\"\\\"\\\"\\n\\n    if text is not None:\\n        condition = all(character.isalpha() for character in text)\\n\\n        if condition:\\n            return True\\n\\n        else:\\n            return False\\n\\n    else:\\n        return False\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_pivot_product_info(df, feature_col):\n",
    "    \"\"\"Helper function to reproduce the pivotting and aggregation procedure for generic columns\"\"\"\n",
    "    df_pivot = (\n",
    "        df.groupby(\"customer_id\")\n",
    "        .pivot(\"type_of_product\")\n",
    "        .agg(F.max(F.col(feature_col)))\n",
    "    )\n",
    "\n",
    "    new_cols = [f\"{col}_{feature_col}\" for col in df_pivot.columns[1:]]\n",
    "\n",
    "    for idx, col in enumerate(new_cols, start=1):\n",
    "        df_pivot = df_pivot.withColumnRenamed(df_pivot.columns[idx], col)\n",
    "\n",
    "    return df_pivot\n",
    "\n",
    "\n",
    "def save_to_filesystem(df, target_path, parquet_path, filename):\n",
    "    \"\"\"Helper function to save pyspark dataframes as parquets in a way that is similar to writing to local files\n",
    "\n",
    "    Args:\n",
    "        df (pyspark.sql.dataframe.DataFrame): dataframe to be saved\n",
    "        target_path (str): path that will store the file\n",
    "        filename (str): name of the resulting file\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    PARQUET_FILE = f\"{target_path}/{parquet_path}\"\n",
    "    OUTPUT_FILE = f\"{target_path}/{filename}\"\n",
    "\n",
    "    if os.path.exists(PARQUET_FILE):\n",
    "        shutil.rmtree(\n",
    "            PARQUET_FILE\n",
    "        )  # if the directory already exists, remove it (throws error if not)\n",
    "\n",
    "    # saves the dataframe:\n",
    "    df.coalesce(1).write.save(PARQUET_FILE)\n",
    "\n",
    "    # retrieves file resulting from the saving procedure:\n",
    "    original_file = glob(f\"{PARQUET_FILE}/*.parquet\")[0]\n",
    "\n",
    "    # renames the resulting file and saves it to the target directory:\n",
    "    os.rename(original_file, OUTPUT_FILE)\n",
    "\n",
    "    shutil.rmtree(PARQUET_FILE)\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def apply_category_map(category_map):\n",
    "    \"\"\"Helper function to convert strings given a map\n",
    "\n",
    "    Note:\n",
    "        This function uses the function generator scheme, much like the PySpark code\n",
    "\n",
    "    Args:\n",
    "        original_category (str): the original category name\n",
    "        category_map (dict): the hash table or dictionary for converting the values:\n",
    "\n",
    "    Returns:\n",
    "        new_category (str): the resulting category\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def func(row):\n",
    "        try:\n",
    "            result = category_map[row]\n",
    "        except:\n",
    "            result = None\n",
    "        return result\n",
    "\n",
    "    return F.udf(func)\n",
    "\n",
    "\n",
    "def get_datetime_features(df, time_col):\n",
    "    \"\"\"Function to extract time-based features from pyspark dataframes\n",
    "\n",
    "    Args:\n",
    "        df (pyspark.sql.dataframe.DataFrame): the original dataframe that needs to be enriched\n",
    "        time_col (str): the string name of the column containing the date object\n",
    "\n",
    "    Returns:\n",
    "        df (pyspark.sql.dataframe.DataFrame): resulting pyspark dataframe with the added features\n",
    "            -> See list of attribute the source code for the attributes\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # applying date-related functions:\n",
    "\n",
    "    # day-level attributes:\n",
    "    df = df.withColumn(\"day_of_week\", F.dayofweek(F.col(time_col)))\n",
    "\n",
    "    df = df.withColumn(\"day_of_month\", F.dayofmonth(F.col(time_col)))\n",
    "\n",
    "    df = df.withColumn(\"day_of_year\", F.dayofyear(F.col(time_col)))\n",
    "\n",
    "    # week-level attributes:\n",
    "    df = df.withColumn(\"week_of_year\", F.weekofyear(F.col(time_col)))\n",
    "\n",
    "    # month-level attributes:\n",
    "    df = df.withColumn(\"month\", F.month(F.col(time_col)))\n",
    "\n",
    "    df = df.withColumn(\"quarter\", F.quarter(F.col(time_col)))\n",
    "\n",
    "    # year-level attributes:\n",
    "    df = df.withColumn(\"year\", F.year(F.col(time_col)))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def bulk_aggregate(df, group_col, aggs, target_cols):\n",
    "    \"\"\"Wrapper function to apply multiple aggregations when performing group bys\n",
    "\n",
    "    It utilizes the spark's SQL Context and string interpolation to perform the aggregation using SQL syntax.\n",
    "\n",
    "    Args:\n",
    "        df (pyspark.sql.dataframe.DataFrame): dataframe with raw data\n",
    "        group_col (str): the column that will be used for grouping\n",
    "        aggs (list): list of aggregations that want to be made (must be the same name as pyspark.sql.functions)\n",
    "        target_cols (str): columns in which aggregations will be performed\n",
    "\n",
    "    Returns:\n",
    "        df_grouped (pyspark.sql.dataframe.DataFrame): dataframe with the grouped data\n",
    "    \"\"\"\n",
    "\n",
    "    # buils the cartersian product of the lists\n",
    "    aggs_to_perform = itertools.product(aggs, target_cols)\n",
    "\n",
    "    Q_LAYOUT = \"\"\"\n",
    "    SELECT\n",
    "        {},\n",
    "        {}\n",
    "        FROM df\n",
    "        GROUP BY {}\n",
    "    \"\"\"\n",
    "\n",
    "    aggregations = []\n",
    "    for agg, col in aggs_to_perform:\n",
    "\n",
    "        # builds the string for aggregation\n",
    "        statement = f\"{agg.upper()}({col}) as {agg}_{col}\"\n",
    "        aggregations.append(statement)\n",
    "\n",
    "    full_statement = \",\\n\".join(aggregations)\n",
    "\n",
    "    # uses string interpolation to build the full query statement\n",
    "    QUERY = Q_LAYOUT.format(group_col, full_statement, group_col)\n",
    "\n",
    "    # registers the dataframe as temporary table:\n",
    "    df.registerTempTable(\"df\")\n",
    "    df_grouped = spark.sql(QUERY)\n",
    "\n",
    "    # rounds values:\n",
    "    for column in df_grouped.columns:\n",
    "        df_grouped = df_grouped.withColumn(column, F.round(F.col(column), 1))\n",
    "\n",
    "    return df_grouped\n",
    "\n",
    "\n",
    "######### Text Processing Functions ########\n",
    "@udf(\"string\")\n",
    "def normalize_text(text):\n",
    "    \"\"\"Helper function to normalize text data to ASCII and lower case, removing spaces\n",
    "\n",
    "    Args:\n",
    "        text (string): the string that needs to be normalized\n",
    "\n",
    "    Returns:\n",
    "        text (string): cleaned up string\n",
    "\n",
    "    \"\"\"\n",
    "    regex = r\"[^a-zA-Z0-9]+\"\n",
    "\n",
    "    if text is not None:\n",
    "\n",
    "        text = str(text)\n",
    "        text = text.lower()\n",
    "        text = re.sub(regex, \" \", text)\n",
    "        text = text.strip()\n",
    "        text = str(\n",
    "            unicodedata.normalize(\"NFKD\", text).encode(\"ASCII\", \"ignore\"), \"utf-8\"\n",
    "        )\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_null_columns(df, normalize=False):\n",
    "    \"\"\"Helper function to print the number of null records for each column of a PySpark DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pyspark.sql.dataframe.DataFrame): a PySpark Dataframe object\n",
    "\n",
    "    Returns:\n",
    "        None -> prints to standard out\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if normalize:\n",
    "        total = df.count()\n",
    "\n",
    "        df_nulls = df.select(\n",
    "            [\n",
    "                (F.sum(F.when(F.col(column).isNull(), 1).otherwise(0)) / total).alias(\n",
    "                    column\n",
    "                )\n",
    "                for column in df.columns\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        df_nulls = df.select(\n",
    "            [\n",
    "                F.sum(F.when(F.col(column).isNull(), 1).otherwise(0)).alias(column)\n",
    "                for column in df.columns\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # displaying the results to standard out\n",
    "    df_nulls.show(1, truncate=False, vertical=True)\n",
    "\n",
    "\n",
    "@udf(\"boolean\")\n",
    "def is_set_or_pack(text):\n",
    "\n",
    "    # description entries to match:\n",
    "    set_descriptions = {\"set\", \"set of\", \"pack\", \"pack of\", \"box\", \"box of\"}\n",
    "\n",
    "    if text is not None:\n",
    "        text = str(text)\n",
    "\n",
    "        if text in set_descriptions:\n",
    "            return True\n",
    "\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "@udf(\"integer\")\n",
    "def get_unit_size(text):\n",
    "\n",
    "    if text is not None:\n",
    "        check_if_digit = len(re.findall(r\"(\\d+)\", text)) > 0\n",
    "\n",
    "        if check_if_digit:\n",
    "            set_size = int(re.findall(r\"(\\d+)\", text)[0])\n",
    "            return set_size\n",
    "\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "@udf(\"boolean\")\n",
    "def has_non_digits_only(text):\n",
    "    \"\"\"Function to match entries in the dataset that are purely non-digit characters\n",
    "\n",
    "    Args:\n",
    "        text (str): string containing the invoice code\n",
    "\n",
    "    Returns:\n",
    "        boolean: whether the text contains non-digit characters and is not related to cancellations\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if text is not None:\n",
    "        condition = all(character.isalpha() for character in text)\n",
    "\n",
    "        if condition:\n",
    "            return True\n",
    "\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementing Customer Features\n",
    "The customer entity for this project is the one that will be used for the customer segmentation task itself. The following map illustrates the reasoning for the features we will generate.\n",
    "\n",
    "<img src=\"../reports/figures/Customer Entity.png\" alt = \"Customer Entity Mapp\" style = \"width:1182px; height=702px;\">\n",
    "\n",
    "Given that we don't have many of these features and informations regarding the customers, we will generate what is possible from the datasets available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"# loading the invoice dataset from the parquet file:\\nPROCESSED_DATA_DIR = '../data/processed/'\\n\\ndf_invoice = (spark\\n                  .read\\n                  .parquet(PROCESSED_DATA_DIR\\n                           + 'tb_invoice.parquet'))\\n\\ndf_product = (spark\\n                 .read\\n                 .parquet(PROCESSED_DATA_DIR\\n                          + 'tb_product.parquet'))\\n\\ndf_ecommerce = (spark\\n                 .read\\n                 .parquet(PROCESSED_DATA_DIR\\n                          + 'tb_ecommerce.parquet'))\";\n",
       "                var nbb_formatted_code = \"# loading the invoice dataset from the parquet file:\\nPROCESSED_DATA_DIR = \\\"../data/processed/\\\"\\n\\ndf_invoice = spark.read.parquet(PROCESSED_DATA_DIR + \\\"tb_invoice.parquet\\\")\\n\\ndf_product = spark.read.parquet(PROCESSED_DATA_DIR + \\\"tb_product.parquet\\\")\\n\\ndf_ecommerce = spark.read.parquet(PROCESSED_DATA_DIR + \\\"tb_ecommerce.parquet\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loading the invoice dataset from the parquet file:\n",
    "PROCESSED_DATA_DIR = \"../data/processed/\"\n",
    "\n",
    "df_invoice = spark.read.parquet(PROCESSED_DATA_DIR + \"tb_invoice.parquet\")\n",
    "\n",
    "df_product = spark.read.parquet(PROCESSED_DATA_DIR + \"tb_product.parquet\")\n",
    "\n",
    "df_ecommerce = spark.read.parquet(PROCESSED_DATA_DIR + \"tb_ecommerce.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"# instantiating the SQL Context:\\nsql_context = SQLContext(spark.sparkContext)\";\n",
       "                var nbb_formatted_code = \"# instantiating the SQL Context:\\nsql_context = SQLContext(spark.sparkContext)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# instantiating the SQL Context:\n",
    "sql_context = SQLContext(spark.sparkContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 RFM and Key metrics \n",
    "The Recency, Frequency and Monetary value variables are often used for baseline customer segmentation models using the RFM strategy. I will start by generating these elements as features. These are defined as below:\n",
    "\n",
    "1. **Recency**: the time in days since the last purchase of the client;\n",
    "2. **Frequency**: the number of times the customer made a purchase;\n",
    "3. **Monetary Value**: the total money spent by the customer;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"# for recency of all purchases, we will consider the day following the last date of the dataset as the upper end:\\nlast_date = (df_invoice\\n                .select(F.date_add(F.max(F.col('date')), 1)\\n                         .alias('last_date'))\\n                         .collect()[0]['last_date'])\\n\\n# generating the recency from the last date as a support variable:\\ndf_invoice = (df_invoice\\n                 .withColumn('last_date', F.lit(last_date)))\";\n",
       "                var nbb_formatted_code = \"# for recency of all purchases, we will consider the day following the last date of the dataset as the upper end:\\nlast_date = df_invoice.select(\\n    F.date_add(F.max(F.col(\\\"date\\\")), 1).alias(\\\"last_date\\\")\\n).collect()[0][\\\"last_date\\\"]\\n\\n# generating the recency from the last date as a support variable:\\ndf_invoice = df_invoice.withColumn(\\\"last_date\\\", F.lit(last_date))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for recency of all purchases, we will consider the day following the last date of the dataset as the upper end:\n",
    "last_date = df_invoice.select(\n",
    "    F.date_add(F.max(F.col(\"date\")), 1).alias(\"last_date\")\n",
    ").collect()[0][\"last_date\"]\n",
    "\n",
    "# generating the recency from the last date as a support variable:\n",
    "df_invoice = df_invoice.withColumn(\"last_date\", F.lit(last_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"# adding the recency variable:\\ndf_invoice = (df_invoice\\n                 .withColumn('recency', \\n                             F.datediff(\\n                                 F.col('last_date'),\\n                                 F.col('date')\\n                             )))\";\n",
       "                var nbb_formatted_code = \"# adding the recency variable:\\ndf_invoice = df_invoice.withColumn(\\n    \\\"recency\\\", F.datediff(F.col(\\\"last_date\\\"), F.col(\\\"date\\\"))\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# adding the recency variable:\n",
    "df_invoice = df_invoice.withColumn(\n",
    "    \"recency\", F.datediff(F.col(\"last_date\"), F.col(\"date\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"# get the time period between the max and min dates:\\ntime_period = (df_invoice\\n                   .select(\\n                       F.datediff(\\n                            F.max(F.col('date')),\\n                            F.min(F.col('date'))\\n                       ).alias('total_period')\\n                   ).collect()[0]['total_period'])\";\n",
       "                var nbb_formatted_code = \"# get the time period between the max and min dates:\\ntime_period = df_invoice.select(\\n    F.datediff(F.max(F.col(\\\"date\\\")), F.min(F.col(\\\"date\\\"))).alias(\\\"total_period\\\")\\n).collect()[0][\\\"total_period\\\"]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the time period between the max and min dates:\n",
    "time_period = df_invoice.select(\n",
    "    F.datediff(F.max(F.col(\"date\")), F.min(F.col(\"date\"))).alias(\"total_period\")\n",
    ").collect()[0][\"total_period\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 16;\n",
       "                var nbb_unformatted_code = \"# generating the RFM variables:\\ndf_customer_rfm = (df_invoice\\n                      .groupby('customer_id')\\n                      .agg(\\n                          F.min(F.col('recency')).alias('recency'),\\n                          F.countDistinct(F.col('invoice_no')).alias('n_orders'),\\n                          F.round(F.sum(F.abs(F.col('total_paid'))), 2).alias('gross_revenue'),\\n                          F.round(F.sum(F.abs(F.col('total_paid_cancelled'))), 2).alias('total_cancelled')\\n                      ))\\n\\n# adding frequency (per month):\\ndf_customer_rfm = (df_customer_rfm\\n                      .withColumn('frequency',\\n                                  F.round(\\n                                      F.col('n_orders') / F.lit(30), 3),\\n                         ))\\n\\n# adding monetary value:\\ndf_customer_rfm = (df_customer_rfm\\n                      .withColumn(\\n                          'monetary_value',\\n                          F.col('gross_revenue') - F.col('total_cancelled')\\n                      ))\\n\\n# adding average ticket:\\ndf_customer_rfm = (df_customer_rfm\\n                      .withColumn(\\n                          'average_ticket',\\n                          F.round(F.col('gross_revenue') / F.col('n_orders'), 2)\\n                      ))\";\n",
       "                var nbb_formatted_code = \"# generating the RFM variables:\\ndf_customer_rfm = df_invoice.groupby(\\\"customer_id\\\").agg(\\n    F.min(F.col(\\\"recency\\\")).alias(\\\"recency\\\"),\\n    F.countDistinct(F.col(\\\"invoice_no\\\")).alias(\\\"n_orders\\\"),\\n    F.round(F.sum(F.abs(F.col(\\\"total_paid\\\"))), 2).alias(\\\"gross_revenue\\\"),\\n    F.round(F.sum(F.abs(F.col(\\\"total_paid_cancelled\\\"))), 2).alias(\\\"total_cancelled\\\"),\\n)\\n\\n# adding frequency (per month):\\ndf_customer_rfm = df_customer_rfm.withColumn(\\n    \\\"frequency\\\",\\n    F.round(F.col(\\\"n_orders\\\") / F.lit(30), 3),\\n)\\n\\n# adding monetary value:\\ndf_customer_rfm = df_customer_rfm.withColumn(\\n    \\\"monetary_value\\\", F.col(\\\"gross_revenue\\\") - F.col(\\\"total_cancelled\\\")\\n)\\n\\n# adding average ticket:\\ndf_customer_rfm = df_customer_rfm.withColumn(\\n    \\\"average_ticket\\\", F.round(F.col(\\\"gross_revenue\\\") / F.col(\\\"n_orders\\\"), 2)\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generating the RFM variables:\n",
    "df_customer_rfm = df_invoice.groupby(\"customer_id\").agg(\n",
    "    F.min(F.col(\"recency\")).alias(\"recency\"),\n",
    "    F.countDistinct(F.col(\"invoice_no\")).alias(\"n_orders\"),\n",
    "    F.round(F.sum(F.abs(F.col(\"total_paid\"))), 2).alias(\"gross_revenue\"),\n",
    "    F.round(F.sum(F.abs(F.col(\"total_paid_cancelled\"))), 2).alias(\"total_cancelled\"),\n",
    ")\n",
    "\n",
    "# adding frequency (per month):\n",
    "df_customer_rfm = df_customer_rfm.withColumn(\n",
    "    \"frequency\",\n",
    "    F.round(F.col(\"n_orders\") / F.lit(30), 3),\n",
    ")\n",
    "\n",
    "# adding monetary value:\n",
    "df_customer_rfm = df_customer_rfm.withColumn(\n",
    "    \"monetary_value\", F.col(\"gross_revenue\") - F.col(\"total_cancelled\")\n",
    ")\n",
    "\n",
    "# adding average ticket:\n",
    "df_customer_rfm = df_customer_rfm.withColumn(\n",
    "    \"average_ticket\", F.round(F.col(\"gross_revenue\") / F.col(\"n_orders\"), 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>customer_id</th><th>recency</th><th>n_orders</th><th>gross_revenue</th><th>total_cancelled</th><th>frequency</th><th>monetary_value</th><th>average_ticket</th></tr>\n",
       "<tr><td>15619</td><td>11</td><td>1</td><td>336.4</td><td>0.0</td><td>0.033</td><td>336.4</td><td>336.4</td></tr>\n",
       "<tr><td>17389</td><td>1</td><td>43</td><td>32367.28</td><td>533.6</td><td>1.433</td><td>31833.68</td><td>752.73</td></tr>\n",
       "<tr><td>18944</td><td>145</td><td>1</td><td>526.8</td><td>0.0</td><td>0.033</td><td>526.8</td><td>526.8</td></tr>\n",
       "<tr><td>12940</td><td>47</td><td>4</td><td>950.79</td><td>37.25</td><td>0.133</td><td>913.54</td><td>237.7</td></tr>\n",
       "<tr><td>14450</td><td>181</td><td>3</td><td>483.25</td><td>0.0</td><td>0.1</td><td>483.25</td><td>161.08</td></tr>\n",
       "</table>\n",
       "only showing top 5 rows\n"
      ],
      "text/plain": [
       "+-----------+-------+--------+-------------+---------------+---------+--------------+--------------+\n",
       "|customer_id|recency|n_orders|gross_revenue|total_cancelled|frequency|monetary_value|average_ticket|\n",
       "+-----------+-------+--------+-------------+---------------+---------+--------------+--------------+\n",
       "|      15619|     11|       1|        336.4|            0.0|    0.033|         336.4|         336.4|\n",
       "|      17389|      1|      43|     32367.28|          533.6|    1.433|      31833.68|        752.73|\n",
       "|      18944|    145|       1|        526.8|            0.0|    0.033|         526.8|         526.8|\n",
       "|      12940|     47|       4|       950.79|          37.25|    0.133|        913.54|         237.7|\n",
       "|      14450|    181|       3|       483.25|            0.0|      0.1|        483.25|        161.08|\n",
       "+-----------+-------+--------+-------------+---------------+---------+--------------+--------------+\n",
       "only showing top 5 rows"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 17;\n",
       "                var nbb_unformatted_code = \"# visualizing the results:\\ndf_customer_rfm\";\n",
       "                var nbb_formatted_code = \"# visualizing the results:\\ndf_customer_rfm\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualizing the results:\n",
    "df_customer_rfm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Basket Features\n",
    "Along with the basic RFM features, I will introduce some other relevant characteristics of the customer's basket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"basket_features = [\\n     'basket_size', \\n     'basket_diversity',\\n     'cancelled_items',\\n     'free_items',\\n     'returned_items',\\n     'sale_items',\\n     'total_discounts',\\n     'total_paid_fee',\\n     'total_paid_manual',\\n     'total_paid_postage',\\n     'total_paid_returned',\\n     'total_paid_sale',\\n     'total_quantity_cancelled',\\n     'total_quantity_free',\\n     'total_quantity_returned',\\n     'total_quantity_sale'\\n]\\n\\ndf_customer_basket = (df_invoice\\n                         .groupby('customer_id')\\n                         .agg(\\n                             F.round(F.avg(F.col('basket_size')), 2).alias('average_basket_size'),\\n                             F.round(F.avg(F.col('basket_diversity')),2).alias('average_basket_diversity'),\\n                             F.sum(F.col('basket_size')).cast('int').alias('total_items'),\\n                             F.sum(F.col('cancelled_items')).alias('total_cancelled_items'),\\n                             F.sum(F.col('free_items')).alias('total_free_items'),\\n                             F.sum(F.col('returned_items')).alias('total_returned_items'),\\n                             F.sum(F.col('sale_items')).alias('total_sale_items'),\\n                             F.sum(F.col('total_discounts')).alias('total_discounts_received'),\\n                             F.sum(F.col('total_paid_fee')).alias('total_paid_fees'),\\n                             F.sum(F.col('total_paid_manual')).alias('total_paid_manual'),\\n                             F.sum(F.col('total_paid_postage')).alias('total_paid_postage'),\\n                             F.sum(F.col('total_paid_returned')).alias('total_paid_returned'),\\n                             F.round(F.sum(F.col('total_paid_sale')), 2).alias('total_paid_sale'),\\n                             F.sum(F.col('total_quantity_cancelled')).alias('total_units_cancelled'),\\n                             F.sum(F.col('total_quantity_free')).alias('total_units_free'),\\n                             F.sum(F.col('total_quantity_returned')).alias('total_units_returned'),\\n                             F.sum(F.col('total_quantity_sale')).alias('total_units_sale')\\n                         ))\";\n",
       "                var nbb_formatted_code = \"basket_features = [\\n    \\\"basket_size\\\",\\n    \\\"basket_diversity\\\",\\n    \\\"cancelled_items\\\",\\n    \\\"free_items\\\",\\n    \\\"returned_items\\\",\\n    \\\"sale_items\\\",\\n    \\\"total_discounts\\\",\\n    \\\"total_paid_fee\\\",\\n    \\\"total_paid_manual\\\",\\n    \\\"total_paid_postage\\\",\\n    \\\"total_paid_returned\\\",\\n    \\\"total_paid_sale\\\",\\n    \\\"total_quantity_cancelled\\\",\\n    \\\"total_quantity_free\\\",\\n    \\\"total_quantity_returned\\\",\\n    \\\"total_quantity_sale\\\",\\n]\\n\\ndf_customer_basket = df_invoice.groupby(\\\"customer_id\\\").agg(\\n    F.round(F.avg(F.col(\\\"basket_size\\\")), 2).alias(\\\"average_basket_size\\\"),\\n    F.round(F.avg(F.col(\\\"basket_diversity\\\")), 2).alias(\\\"average_basket_diversity\\\"),\\n    F.sum(F.col(\\\"basket_size\\\")).cast(\\\"int\\\").alias(\\\"total_items\\\"),\\n    F.sum(F.col(\\\"cancelled_items\\\")).alias(\\\"total_cancelled_items\\\"),\\n    F.sum(F.col(\\\"free_items\\\")).alias(\\\"total_free_items\\\"),\\n    F.sum(F.col(\\\"returned_items\\\")).alias(\\\"total_returned_items\\\"),\\n    F.sum(F.col(\\\"sale_items\\\")).alias(\\\"total_sale_items\\\"),\\n    F.sum(F.col(\\\"total_discounts\\\")).alias(\\\"total_discounts_received\\\"),\\n    F.sum(F.col(\\\"total_paid_fee\\\")).alias(\\\"total_paid_fees\\\"),\\n    F.sum(F.col(\\\"total_paid_manual\\\")).alias(\\\"total_paid_manual\\\"),\\n    F.sum(F.col(\\\"total_paid_postage\\\")).alias(\\\"total_paid_postage\\\"),\\n    F.sum(F.col(\\\"total_paid_returned\\\")).alias(\\\"total_paid_returned\\\"),\\n    F.round(F.sum(F.col(\\\"total_paid_sale\\\")), 2).alias(\\\"total_paid_sale\\\"),\\n    F.sum(F.col(\\\"total_quantity_cancelled\\\")).alias(\\\"total_units_cancelled\\\"),\\n    F.sum(F.col(\\\"total_quantity_free\\\")).alias(\\\"total_units_free\\\"),\\n    F.sum(F.col(\\\"total_quantity_returned\\\")).alias(\\\"total_units_returned\\\"),\\n    F.sum(F.col(\\\"total_quantity_sale\\\")).alias(\\\"total_units_sale\\\"),\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "basket_features = [\n",
    "    \"basket_size\",\n",
    "    \"basket_diversity\",\n",
    "    \"cancelled_items\",\n",
    "    \"free_items\",\n",
    "    \"returned_items\",\n",
    "    \"sale_items\",\n",
    "    \"total_discounts\",\n",
    "    \"total_paid_fee\",\n",
    "    \"total_paid_manual\",\n",
    "    \"total_paid_postage\",\n",
    "    \"total_paid_returned\",\n",
    "    \"total_paid_sale\",\n",
    "    \"total_quantity_cancelled\",\n",
    "    \"total_quantity_free\",\n",
    "    \"total_quantity_returned\",\n",
    "    \"total_quantity_sale\",\n",
    "]\n",
    "\n",
    "df_customer_basket = df_invoice.groupby(\"customer_id\").agg(\n",
    "    F.round(F.avg(F.col(\"basket_size\")), 2).alias(\"average_basket_size\"),\n",
    "    F.round(F.avg(F.col(\"basket_diversity\")), 2).alias(\"average_basket_diversity\"),\n",
    "    F.sum(F.col(\"basket_size\")).cast(\"int\").alias(\"total_items\"),\n",
    "    F.sum(F.col(\"cancelled_items\")).alias(\"total_cancelled_items\"),\n",
    "    F.sum(F.col(\"free_items\")).alias(\"total_free_items\"),\n",
    "    F.sum(F.col(\"returned_items\")).alias(\"total_returned_items\"),\n",
    "    F.sum(F.col(\"sale_items\")).alias(\"total_sale_items\"),\n",
    "    F.sum(F.col(\"total_discounts\")).alias(\"total_discounts_received\"),\n",
    "    F.sum(F.col(\"total_paid_fee\")).alias(\"total_paid_fees\"),\n",
    "    F.sum(F.col(\"total_paid_manual\")).alias(\"total_paid_manual\"),\n",
    "    F.sum(F.col(\"total_paid_postage\")).alias(\"total_paid_postage\"),\n",
    "    F.sum(F.col(\"total_paid_returned\")).alias(\"total_paid_returned\"),\n",
    "    F.round(F.sum(F.col(\"total_paid_sale\")), 2).alias(\"total_paid_sale\"),\n",
    "    F.sum(F.col(\"total_quantity_cancelled\")).alias(\"total_units_cancelled\"),\n",
    "    F.sum(F.col(\"total_quantity_free\")).alias(\"total_units_free\"),\n",
    "    F.sum(F.col(\"total_quantity_returned\")).alias(\"total_units_returned\"),\n",
    "    F.sum(F.col(\"total_quantity_sale\")).alias(\"total_units_sale\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>customer_id</th><th>average_basket_size</th><th>average_basket_diversity</th><th>total_items</th><th>total_cancelled_items</th><th>total_free_items</th><th>total_returned_items</th><th>total_sale_items</th><th>total_discounts_received</th><th>total_paid_fees</th><th>total_paid_manual</th><th>total_paid_postage</th><th>total_paid_returned</th><th>total_paid_sale</th><th>total_units_cancelled</th><th>total_units_free</th><th>total_units_returned</th><th>total_units_sale</th></tr>\n",
       "<tr><td>14450</td><td>80.33</td><td>13.33</td><td>241</td><td>0</td><td>10</td><td>0</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>104.0</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>17389</td><td>181.26</td><td>5.21</td><td>7794</td><td>11</td><td>10</td><td>0</td><td>145</td><td>0.0</td><td>0.0</td><td>17.4</td><td>0.0</td><td>0.0</td><td>26569.01</td><td>182.0</td><td>222.0</td><td>0.0</td><td>6301.0</td></tr>\n",
       "<tr><td>13285</td><td>512.75</td><td>46.75</td><td>2051</td><td>0</td><td>62</td><td>0</td><td>5</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>95.7</td><td>0.0</td><td>958.0</td><td>0.0</td><td>46.0</td></tr>\n",
       "<tr><td>16503</td><td>114.2</td><td>17.2</td><td>571</td><td>2</td><td>5</td><td>0</td><td>1</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>15.6</td><td>2.0</td><td>64.0</td><td>0.0</td><td>8.0</td></tr>\n",
       "<tr><td>18654</td><td>93.0</td><td>57.0</td><td>93</td><td>0</td><td>2</td><td>0</td><td>14</td><td>0.0</td><td>0.0</td><td>0.0</td><td>175.13</td><td>0.0</td><td>361.28</td><td>0.0</td><td>2.0</td><td>0.0</td><td>14.0</td></tr>\n",
       "</table>\n",
       "only showing top 5 rows\n"
      ],
      "text/plain": [
       "+-----------+-------------------+------------------------+-----------+---------------------+----------------+--------------------+----------------+------------------------+---------------+-----------------+------------------+-------------------+---------------+---------------------+----------------+--------------------+----------------+\n",
       "|customer_id|average_basket_size|average_basket_diversity|total_items|total_cancelled_items|total_free_items|total_returned_items|total_sale_items|total_discounts_received|total_paid_fees|total_paid_manual|total_paid_postage|total_paid_returned|total_paid_sale|total_units_cancelled|total_units_free|total_units_returned|total_units_sale|\n",
       "+-----------+-------------------+------------------------+-----------+---------------------+----------------+--------------------+----------------+------------------------+---------------+-----------------+------------------+-------------------+---------------+---------------------+----------------+--------------------+----------------+\n",
       "|      14450|              80.33|                   13.33|        241|                    0|              10|                   0|               0|                     0.0|            0.0|              0.0|               0.0|                0.0|            0.0|                  0.0|           104.0|                 0.0|             0.0|\n",
       "|      17389|             181.26|                    5.21|       7794|                   11|              10|                   0|             145|                     0.0|            0.0|             17.4|               0.0|                0.0|       26569.01|                182.0|           222.0|                 0.0|          6301.0|\n",
       "|      13285|             512.75|                   46.75|       2051|                    0|              62|                   0|               5|                     0.0|            0.0|              0.0|               0.0|                0.0|           95.7|                  0.0|           958.0|                 0.0|            46.0|\n",
       "|      16503|              114.2|                    17.2|        571|                    2|               5|                   0|               1|                     0.0|            0.0|              0.0|               0.0|                0.0|           15.6|                  2.0|            64.0|                 0.0|             8.0|\n",
       "|      18654|               93.0|                    57.0|         93|                    0|               2|                   0|              14|                     0.0|            0.0|              0.0|            175.13|                0.0|         361.28|                  0.0|             2.0|                 0.0|            14.0|\n",
       "+-----------+-------------------+------------------------+-----------+---------------------+----------------+--------------------+----------------+------------------------+---------------+-----------------+------------------+-------------------+---------------+---------------------+----------------+--------------------+----------------+\n",
       "only showing top 5 rows"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 19;\n",
       "                var nbb_unformatted_code = \"# verifying the results:\\ndf_customer_basket\";\n",
       "                var nbb_formatted_code = \"# verifying the results:\\ndf_customer_basket\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# verifying the results:\n",
    "df_customer_basket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 20;\n",
       "                var nbb_unformatted_code = \"# basket features:\\nbasket_related_features = df_customer_basket.columns\";\n",
       "                var nbb_formatted_code = \"# basket features:\\nbasket_related_features = df_customer_basket.columns\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# basket features:\n",
    "basket_related_features = df_customer_basket.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Time between purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 21;\n",
       "                var nbb_unformatted_code = \"# selecting a temporary view of the invoice dataset\\ndf_temp = (df_invoice\\n              .select('customer_id', 'invoice_no', 'date'))\\n\\n\\n# defining a window function and associated column \\ntime_between_purchases = (Window\\n                             .partitionBy('customer_id')\\n                             .orderBy('date'))\\n\\ndf_temp = (df_temp\\n              .withColumn('time_between_purchases',\\n                        F.datediff(\\n                            F.col('date'),\\n                            F.lag(F.col('date'), 1)\\n                             .over(time_between_purchases)\\n                        )))\\n\\n# adding the results back into the invoice dataset:\\ndf_invoice = (df_invoice\\n                .join(\\n                    df_temp,\\n                    on = ['customer_id', 'invoice_no', 'date'],\\n                    how = 'left'\\n                ))\";\n",
       "                var nbb_formatted_code = \"# selecting a temporary view of the invoice dataset\\ndf_temp = df_invoice.select(\\\"customer_id\\\", \\\"invoice_no\\\", \\\"date\\\")\\n\\n\\n# defining a window function and associated column\\ntime_between_purchases = Window.partitionBy(\\\"customer_id\\\").orderBy(\\\"date\\\")\\n\\ndf_temp = df_temp.withColumn(\\n    \\\"time_between_purchases\\\",\\n    F.datediff(F.col(\\\"date\\\"), F.lag(F.col(\\\"date\\\"), 1).over(time_between_purchases)),\\n)\\n\\n# adding the results back into the invoice dataset:\\ndf_invoice = df_invoice.join(\\n    df_temp, on=[\\\"customer_id\\\", \\\"invoice_no\\\", \\\"date\\\"], how=\\\"left\\\"\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# selecting a temporary view of the invoice dataset\n",
    "df_temp = df_invoice.select(\"customer_id\", \"invoice_no\", \"date\")\n",
    "\n",
    "\n",
    "# defining a window function and associated column\n",
    "time_between_purchases = Window.partitionBy(\"customer_id\").orderBy(\"date\")\n",
    "\n",
    "df_temp = df_temp.withColumn(\n",
    "    \"time_between_purchases\",\n",
    "    F.datediff(F.col(\"date\"), F.lag(F.col(\"date\"), 1).over(time_between_purchases)),\n",
    ")\n",
    "\n",
    "# adding the results back into the invoice dataset:\n",
    "df_invoice = df_invoice.join(\n",
    "    df_temp, on=[\"customer_id\", \"invoice_no\", \"date\"], how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 22;\n",
       "                var nbb_unformatted_code = \"# performing the time aggregations:\\ntime_based_features = [\\n    'time_between_purchases',\\n    'days_to_next_bank_holiday',\\n    'days_to_next_commercial_holiday',\\n    'month',\\n    'week_of_year',\\n    'day_of_month',\\n    'day_of_week'\\n]\\n\\ndf_customer_time = (df_invoice\\n                       .groupby('customer_id')\\n                       .agg(\\n                           F.avg(F.col('time_between_purchases')).alias('average_time_between_purchases'),\\n                           F.avg(F.col('days_to_next_bank_holiday')).alias('average_time_to_next_bank_holiday'),\\n                           F.avg(F.col('days_to_next_commercial_holiday')).alias('average_time_to_next_commercial_holiday')\\n                       ))\\n\\n# generating a temporary view of the raw dataframe:\\nsql_context.registerDataFrameAsTable(df_invoice, \\n                                     \\\"tb_invoice\\\")\";\n",
       "                var nbb_formatted_code = \"# performing the time aggregations:\\ntime_based_features = [\\n    \\\"time_between_purchases\\\",\\n    \\\"days_to_next_bank_holiday\\\",\\n    \\\"days_to_next_commercial_holiday\\\",\\n    \\\"month\\\",\\n    \\\"week_of_year\\\",\\n    \\\"day_of_month\\\",\\n    \\\"day_of_week\\\",\\n]\\n\\ndf_customer_time = df_invoice.groupby(\\\"customer_id\\\").agg(\\n    F.avg(F.col(\\\"time_between_purchases\\\")).alias(\\\"average_time_between_purchases\\\"),\\n    F.avg(F.col(\\\"days_to_next_bank_holiday\\\")).alias(\\n        \\\"average_time_to_next_bank_holiday\\\"\\n    ),\\n    F.avg(F.col(\\\"days_to_next_commercial_holiday\\\")).alias(\\n        \\\"average_time_to_next_commercial_holiday\\\"\\n    ),\\n)\\n\\n# generating a temporary view of the raw dataframe:\\nsql_context.registerDataFrameAsTable(df_invoice, \\\"tb_invoice\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# performing the time aggregations:\n",
    "time_based_features = [\n",
    "    'time_between_purchases',\n",
    "    'days_to_next_bank_holiday',\n",
    "    'days_to_next_commercial_holiday',\n",
    "    'month',\n",
    "    'week_of_year',\n",
    "    'day_of_month',\n",
    "    'day_of_week'\n",
    "]\n",
    "\n",
    "df_customer_time = (df_invoice\n",
    "                       .groupby('customer_id')\n",
    "                       .agg(\n",
    "                           F.avg(F.col('time_between_purchases')).alias('average_time_between_purchases'),\n",
    "                           F.avg(F.col('days_to_next_bank_holiday')).alias('average_time_to_next_bank_holiday'),\n",
    "                           F.avg(F.col('days_to_next_commercial_holiday')).alias('average_time_to_next_commercial_holiday')\n",
    "                       ))\n",
    "\n",
    "# generating a temporary view of the raw dataframe:\n",
    "sql_context.registerDataFrameAsTable(df_invoice, \n",
    "                                     \"tb_invoice\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 23;\n",
       "                var nbb_unformatted_code = \"# defining window functions in sql context to generate the most relevant time periods for customers:\\nQ_MONTH_MOST_ACTIVE = \\\"\\\"\\\"\\n\\nwith months as (\\nSELECT\\n    customer_id,\\n    month as time_unit,\\n    COUNT(DISTINCT invoice_no) as n_orders\\nFROM tb_invoice\\nGROUP BY customer_id, month\\n)\\n\\nSELECT \\n    customer_id,\\n    time_unit as month_most_active\\nFROM (\\nSELECT\\n    A.*,\\n    RANK() OVER(PARTITION BY customer_id ORDER BY n_orders DESC) as rank_idx\\n    FROM months as A\\n) AS temp\\nWHERE rank_idx = 1\\n\\\"\\\"\\\"\\n\\ndf_month = spark.sql(Q_MONTH_MOST_ACTIVE)\\n\\nQ_WEEK_MOST_ACTIVE = \\\"\\\"\\\"\\n\\nwith weeks as (\\nSELECT\\n    customer_id,\\n    week_of_year as time_unit,\\n    COUNT(DISTINCT invoice_no) as n_orders\\nFROM tb_invoice\\nGROUP BY customer_id, week_of_year\\n)\\n\\nSELECT \\n    customer_id,\\n    time_unit as week_most_active\\nFROM (\\nSELECT\\n    A.*,\\n    RANK() OVER(PARTITION BY customer_id ORDER BY n_orders DESC) as rank_idx\\n    FROM weeks as A\\n) AS temp\\nWHERE rank_idx = 1\\n\\\"\\\"\\\"\\n\\ndf_week = spark.sql(Q_WEEK_MOST_ACTIVE)\\n\\nQ_DAY_OF_WEEK = \\\"\\\"\\\"\\n\\nwith days as (\\nSELECT\\n    customer_id,\\n    day_of_week as time_unit,\\n    COUNT(DISTINCT invoice_no) as n_orders\\nFROM tb_invoice\\nGROUP BY customer_id, day_of_week\\n)\\n\\nSELECT \\n    customer_id,\\n    time_unit as day_of_week_most_active\\nFROM (\\nSELECT\\n    A.*,\\n    RANK() OVER(PARTITION BY customer_id ORDER BY n_orders DESC) as rank_idx\\n    FROM days as A\\n) AS temp\\nWHERE rank_idx = 1\\n\\\"\\\"\\\"\\n\\ndf_day_of_week = spark.sql(Q_DAY_OF_WEEK)\\n\\n\\nQ_DAY_OF_MONTH = \\\"\\\"\\\"\\n\\nwith days as (\\nSELECT\\n    customer_id,\\n    day_of_month as time_unit,\\n    COUNT(DISTINCT invoice_no) as n_orders\\nFROM tb_invoice\\nGROUP BY customer_id, day_of_month\\n)\\n\\nSELECT \\n    customer_id,\\n    time_unit as day_of_month_most_active\\nFROM (\\nSELECT\\n    A.*,\\n    RANK() OVER(PARTITION BY customer_id ORDER BY n_orders DESC) as rank_idx\\n    FROM days as A\\n) AS temp\\nWHERE rank_idx = 1\\n\\\"\\\"\\\"\\n\\ndf_day_of_month = spark.sql(Q_DAY_OF_MONTH)\";\n",
       "                var nbb_formatted_code = \"# defining window functions in sql context to generate the most relevant time periods for customers:\\nQ_MONTH_MOST_ACTIVE = \\\"\\\"\\\"\\n\\nwith months as (\\nSELECT\\n    customer_id,\\n    month as time_unit,\\n    COUNT(DISTINCT invoice_no) as n_orders\\nFROM tb_invoice\\nGROUP BY customer_id, month\\n)\\n\\nSELECT \\n    customer_id,\\n    time_unit as month_most_active\\nFROM (\\nSELECT\\n    A.*,\\n    RANK() OVER(PARTITION BY customer_id ORDER BY n_orders DESC) as rank_idx\\n    FROM months as A\\n) AS temp\\nWHERE rank_idx = 1\\n\\\"\\\"\\\"\\n\\ndf_month = spark.sql(Q_MONTH_MOST_ACTIVE)\\n\\nQ_WEEK_MOST_ACTIVE = \\\"\\\"\\\"\\n\\nwith weeks as (\\nSELECT\\n    customer_id,\\n    week_of_year as time_unit,\\n    COUNT(DISTINCT invoice_no) as n_orders\\nFROM tb_invoice\\nGROUP BY customer_id, week_of_year\\n)\\n\\nSELECT \\n    customer_id,\\n    time_unit as week_most_active\\nFROM (\\nSELECT\\n    A.*,\\n    RANK() OVER(PARTITION BY customer_id ORDER BY n_orders DESC) as rank_idx\\n    FROM weeks as A\\n) AS temp\\nWHERE rank_idx = 1\\n\\\"\\\"\\\"\\n\\ndf_week = spark.sql(Q_WEEK_MOST_ACTIVE)\\n\\nQ_DAY_OF_WEEK = \\\"\\\"\\\"\\n\\nwith days as (\\nSELECT\\n    customer_id,\\n    day_of_week as time_unit,\\n    COUNT(DISTINCT invoice_no) as n_orders\\nFROM tb_invoice\\nGROUP BY customer_id, day_of_week\\n)\\n\\nSELECT \\n    customer_id,\\n    time_unit as day_of_week_most_active\\nFROM (\\nSELECT\\n    A.*,\\n    RANK() OVER(PARTITION BY customer_id ORDER BY n_orders DESC) as rank_idx\\n    FROM days as A\\n) AS temp\\nWHERE rank_idx = 1\\n\\\"\\\"\\\"\\n\\ndf_day_of_week = spark.sql(Q_DAY_OF_WEEK)\\n\\n\\nQ_DAY_OF_MONTH = \\\"\\\"\\\"\\n\\nwith days as (\\nSELECT\\n    customer_id,\\n    day_of_month as time_unit,\\n    COUNT(DISTINCT invoice_no) as n_orders\\nFROM tb_invoice\\nGROUP BY customer_id, day_of_month\\n)\\n\\nSELECT \\n    customer_id,\\n    time_unit as day_of_month_most_active\\nFROM (\\nSELECT\\n    A.*,\\n    RANK() OVER(PARTITION BY customer_id ORDER BY n_orders DESC) as rank_idx\\n    FROM days as A\\n) AS temp\\nWHERE rank_idx = 1\\n\\\"\\\"\\\"\\n\\ndf_day_of_month = spark.sql(Q_DAY_OF_MONTH)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# defining window functions in sql context to generate the most relevant time periods for customers:\n",
    "Q_MONTH_MOST_ACTIVE = \"\"\"\n",
    "\n",
    "with months as (\n",
    "SELECT\n",
    "    customer_id,\n",
    "    month as time_unit,\n",
    "    COUNT(DISTINCT invoice_no) as n_orders\n",
    "FROM tb_invoice\n",
    "GROUP BY customer_id, month\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    customer_id,\n",
    "    time_unit as month_most_active\n",
    "FROM (\n",
    "SELECT\n",
    "    A.*,\n",
    "    RANK() OVER(PARTITION BY customer_id ORDER BY n_orders DESC) as rank_idx\n",
    "    FROM months as A\n",
    ") AS temp\n",
    "WHERE rank_idx = 1\n",
    "\"\"\"\n",
    "\n",
    "df_month = spark.sql(Q_MONTH_MOST_ACTIVE)\n",
    "\n",
    "Q_WEEK_MOST_ACTIVE = \"\"\"\n",
    "\n",
    "with weeks as (\n",
    "SELECT\n",
    "    customer_id,\n",
    "    week_of_year as time_unit,\n",
    "    COUNT(DISTINCT invoice_no) as n_orders\n",
    "FROM tb_invoice\n",
    "GROUP BY customer_id, week_of_year\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    customer_id,\n",
    "    time_unit as week_most_active\n",
    "FROM (\n",
    "SELECT\n",
    "    A.*,\n",
    "    RANK() OVER(PARTITION BY customer_id ORDER BY n_orders DESC) as rank_idx\n",
    "    FROM weeks as A\n",
    ") AS temp\n",
    "WHERE rank_idx = 1\n",
    "\"\"\"\n",
    "\n",
    "df_week = spark.sql(Q_WEEK_MOST_ACTIVE)\n",
    "\n",
    "Q_DAY_OF_WEEK = \"\"\"\n",
    "\n",
    "with days as (\n",
    "SELECT\n",
    "    customer_id,\n",
    "    day_of_week as time_unit,\n",
    "    COUNT(DISTINCT invoice_no) as n_orders\n",
    "FROM tb_invoice\n",
    "GROUP BY customer_id, day_of_week\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    customer_id,\n",
    "    time_unit as day_of_week_most_active\n",
    "FROM (\n",
    "SELECT\n",
    "    A.*,\n",
    "    RANK() OVER(PARTITION BY customer_id ORDER BY n_orders DESC) as rank_idx\n",
    "    FROM days as A\n",
    ") AS temp\n",
    "WHERE rank_idx = 1\n",
    "\"\"\"\n",
    "\n",
    "df_day_of_week = spark.sql(Q_DAY_OF_WEEK)\n",
    "\n",
    "\n",
    "Q_DAY_OF_MONTH = \"\"\"\n",
    "\n",
    "with days as (\n",
    "SELECT\n",
    "    customer_id,\n",
    "    day_of_month as time_unit,\n",
    "    COUNT(DISTINCT invoice_no) as n_orders\n",
    "FROM tb_invoice\n",
    "GROUP BY customer_id, day_of_month\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    customer_id,\n",
    "    time_unit as day_of_month_most_active\n",
    "FROM (\n",
    "SELECT\n",
    "    A.*,\n",
    "    RANK() OVER(PARTITION BY customer_id ORDER BY n_orders DESC) as rank_idx\n",
    "    FROM days as A\n",
    ") AS temp\n",
    "WHERE rank_idx = 1\n",
    "\"\"\"\n",
    "\n",
    "df_day_of_month = spark.sql(Q_DAY_OF_MONTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 24;\n",
       "                var nbb_unformatted_code = \"# joining all the time segmentations:\\ndf_dates = (df_month\\n               .join(\\n                   df_week,\\n                   on = ['customer_id'],\\n                   how = 'left'\\n                )\\n               .join(\\n                   df_day_of_week,\\n                   on = ['customer_id'],\\n                   how = 'left'\\n               )\\n               .join(\\n                   df_day_of_month,\\n                   on = ['customer_id'],\\n                   how = 'left'\\n               )\\n               .drop_duplicates(subset=['customer_id']))\";\n",
       "                var nbb_formatted_code = \"# joining all the time segmentations:\\ndf_dates = (\\n    df_month.join(df_week, on=[\\\"customer_id\\\"], how=\\\"left\\\")\\n    .join(df_day_of_week, on=[\\\"customer_id\\\"], how=\\\"left\\\")\\n    .join(df_day_of_month, on=[\\\"customer_id\\\"], how=\\\"left\\\")\\n    .drop_duplicates(subset=[\\\"customer_id\\\"])\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# joining all the time segmentations:\n",
    "df_dates = (\n",
    "    df_month.join(df_week, on=[\"customer_id\"], how=\"left\")\n",
    "    .join(df_day_of_week, on=[\"customer_id\"], how=\"left\")\n",
    "    .join(df_day_of_month, on=[\"customer_id\"], how=\"left\")\n",
    "    .drop_duplicates(subset=[\"customer_id\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 25;\n",
       "                var nbb_unformatted_code = \"# joining on the full time dataframe:\\ndf_customer_time = (df_customer_time\\n                       .join(\\n                           df_dates,\\n                           on = ['customer_id'],\\n                           how = 'left'\\n                        )\\n                       .drop_duplicates(subset = ['customer_id']))\";\n",
       "                var nbb_formatted_code = \"# joining on the full time dataframe:\\ndf_customer_time = df_customer_time.join(\\n    df_dates, on=[\\\"customer_id\\\"], how=\\\"left\\\"\\n).drop_duplicates(subset=[\\\"customer_id\\\"])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# joining on the full time dataframe:\n",
    "df_customer_time = df_customer_time.join(\n",
    "    df_dates, on=[\"customer_id\"], how=\"left\"\n",
    ").drop_duplicates(subset=[\"customer_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 26;\n",
       "                var nbb_unformatted_code = \"# getting the resulting columns:\\ntime_features = df_customer_time.columns\";\n",
       "                var nbb_formatted_code = \"# getting the resulting columns:\\ntime_features = df_customer_time.columns\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# getting the resulting columns:\n",
    "time_features = df_customer_time.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Profile Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 27;\n",
       "                var nbb_unformatted_code = \"# extracting some key characteristics from customer profile:\\ndf_customer_profile = (df_invoice\\n                          .groupby(\\n                              'customer_id'\\n                          )\\n                          .agg(\\n                              F.first(F.col('customer_country')).alias('customer_country'),\\n                              F.min(F.col('date')).alias('first_purchase_date')\\n                          ))\\n\\n# check if the customer is from the UK:\\ndf_customer_profile = (df_customer_profile\\n                          .withColumn('is_foreign',\\n                                      F.when(F.col('customer_country') != 'united kingdom', True)\\n                                       .otherwise(False)\\n                                     ))\\n\\n# getting account age\\ndf_customer_profile = (df_customer_profile\\n                          .withColumn('account_age_days',\\n                                      F.datediff(\\n                                          F.lit(last_date),\\n                                          F.col('first_purchase_date')\\n                                      )))\";\n",
       "                var nbb_formatted_code = \"# extracting some key characteristics from customer profile:\\ndf_customer_profile = df_invoice.groupby(\\\"customer_id\\\").agg(\\n    F.first(F.col(\\\"customer_country\\\")).alias(\\\"customer_country\\\"),\\n    F.min(F.col(\\\"date\\\")).alias(\\\"first_purchase_date\\\"),\\n)\\n\\n# check if the customer is from the UK:\\ndf_customer_profile = df_customer_profile.withColumn(\\n    \\\"is_foreign\\\",\\n    F.when(F.col(\\\"customer_country\\\") != \\\"united kingdom\\\", True).otherwise(False),\\n)\\n\\n# getting account age\\ndf_customer_profile = df_customer_profile.withColumn(\\n    \\\"account_age_days\\\", F.datediff(F.lit(last_date), F.col(\\\"first_purchase_date\\\"))\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extracting some key characteristics from customer profile:\n",
    "df_customer_profile = df_invoice.groupby(\"customer_id\").agg(\n",
    "    F.first(F.col(\"customer_country\")).alias(\"customer_country\"),\n",
    "    F.min(F.col(\"date\")).alias(\"first_purchase_date\"),\n",
    ")\n",
    "\n",
    "# check if the customer is from the UK:\n",
    "df_customer_profile = df_customer_profile.withColumn(\n",
    "    \"is_foreign\",\n",
    "    F.when(F.col(\"customer_country\") != \"united kingdom\", True).otherwise(False),\n",
    ")\n",
    "\n",
    "# getting account age\n",
    "df_customer_profile = df_customer_profile.withColumn(\n",
    "    \"account_age_days\", F.datediff(F.lit(last_date), F.col(\"first_purchase_date\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 28;\n",
       "                var nbb_unformatted_code = \"# extracting the profile columns:\\nprofile_cols = df_customer_profile.columns\";\n",
       "                var nbb_formatted_code = \"# extracting the profile columns:\\nprofile_cols = df_customer_profile.columns\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extracting the profile columns:\n",
    "profile_cols = df_customer_profile.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Categorizing Resellers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 29;\n",
       "                var nbb_unformatted_code = \"# as demonstrated during the EDA notebook for the customer entity, we will implement the reseller feature here as well:\\nreseller_threshold = (df_customer_rfm\\n                          .approxQuantile(\\\"monetary_value\\\", \\n                                          [0.95], \\n                                          0.0001)\\n                          [0])\\n\\ndf_customer_rfm = (df_customer_rfm\\n                      .withColumn(\\n                          'is_considered_reseller',\\n                          F.when(F.col('monetary_value') >= reseller_threshold, True)\\n                           .otherwise(False)\\n                      ))\";\n",
       "                var nbb_formatted_code = \"# as demonstrated during the EDA notebook for the customer entity, we will implement the reseller feature here as well:\\nreseller_threshold = df_customer_rfm.approxQuantile(\\\"monetary_value\\\", [0.95], 0.0001)[0]\\n\\ndf_customer_rfm = df_customer_rfm.withColumn(\\n    \\\"is_considered_reseller\\\",\\n    F.when(F.col(\\\"monetary_value\\\") >= reseller_threshold, True).otherwise(False),\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# as demonstrated during the EDA notebook for the customer entity, we will implement the reseller feature here as well:\n",
    "reseller_threshold = df_customer_rfm.approxQuantile(\"monetary_value\", [0.95], 0.0001)[0]\n",
    "\n",
    "df_customer_rfm = df_customer_rfm.withColumn(\n",
    "    \"is_considered_reseller\",\n",
    "    F.when(F.col(\"monetary_value\") >= reseller_threshold, True).otherwise(False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 30;\n",
       "                var nbb_unformatted_code = \"# extracting the columns:\\nrfm_cols = df_customer_rfm.columns\";\n",
       "                var nbb_formatted_code = \"# extracting the columns:\\nrfm_cols = df_customer_rfm.columns\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extracting the columns:\n",
    "rfm_cols = df_customer_rfm.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preparing the Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 31;\n",
       "                var nbb_unformatted_code = \"# final schema would be:\\nall_cols = (profile_cols  \\n              + rfm_cols\\n              + time_features \\n              + basket_related_features)\\n\\nall_cols = list(filter(lambda x: x != 'customer_id', all_cols))\\n\\ncol_order = ['customer_id'] + all_cols\";\n",
       "                var nbb_formatted_code = \"# final schema would be:\\nall_cols = profile_cols + rfm_cols + time_features + basket_related_features\\n\\nall_cols = list(filter(lambda x: x != \\\"customer_id\\\", all_cols))\\n\\ncol_order = [\\\"customer_id\\\"] + all_cols\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# final schema would be:\n",
    "all_cols = profile_cols + rfm_cols + time_features + basket_related_features\n",
    "\n",
    "all_cols = list(filter(lambda x: x != \"customer_id\", all_cols))\n",
    "\n",
    "col_order = [\"customer_id\"] + all_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 32;\n",
       "                var nbb_unformatted_code = \"# joining all the datasets:\\ndf_customer = (df_customer_profile\\n                  .join(\\n                     df_customer_basket,\\n                     how = 'left',\\n                     on = ['customer_id']\\n                  ))\\n\\ndf_customer = (df_customer\\n                  .join(\\n                     df_customer_rfm,\\n                     how = 'left',\\n                     on = ['customer_id']\\n                  ))\\n\\ndf_customer = (df_customer\\n                  .join(\\n                     df_customer_time,\\n                     how = 'left',\\n                     on = ['customer_id']\\n                  ))\\n\\n# dropping the duplicates:\\ndf_customer = (df_customer\\n                  .drop_duplicates(subset = ['customer_id']))\";\n",
       "                var nbb_formatted_code = \"# joining all the datasets:\\ndf_customer = df_customer_profile.join(\\n    df_customer_basket, how=\\\"left\\\", on=[\\\"customer_id\\\"]\\n)\\n\\ndf_customer = df_customer.join(df_customer_rfm, how=\\\"left\\\", on=[\\\"customer_id\\\"])\\n\\ndf_customer = df_customer.join(df_customer_time, how=\\\"left\\\", on=[\\\"customer_id\\\"])\\n\\n# dropping the duplicates:\\ndf_customer = df_customer.drop_duplicates(subset=[\\\"customer_id\\\"])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# joining all the datasets:\n",
    "df_customer = df_customer_profile.join(\n",
    "    df_customer_basket, how=\"left\", on=[\"customer_id\"]\n",
    ")\n",
    "\n",
    "df_customer = df_customer.join(df_customer_rfm, how=\"left\", on=[\"customer_id\"])\n",
    "\n",
    "df_customer = df_customer.join(df_customer_time, how=\"left\", on=[\"customer_id\"])\n",
    "\n",
    "# dropping the duplicates:\n",
    "df_customer = df_customer.drop_duplicates(subset=[\"customer_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 33;\n",
       "                var nbb_unformatted_code = \"# rounding down time-based columns:\\ncols_to_floor = [\\n    'average_time_to_next_commercial_holiday',\\n    'average_time_to_next_bank_holiday',\\n    'average_time_between_purchases'\\n]\\n\\nfor col in cols_to_floor:\\n    \\n    df_customer = (df_customer\\n                      .withColumn(\\n                          col,\\n                          F.floor(F.col(col))\\n                      ))\";\n",
       "                var nbb_formatted_code = \"# rounding down time-based columns:\\ncols_to_floor = [\\n    \\\"average_time_to_next_commercial_holiday\\\",\\n    \\\"average_time_to_next_bank_holiday\\\",\\n    \\\"average_time_between_purchases\\\",\\n]\\n\\nfor col in cols_to_floor:\\n\\n    df_customer = df_customer.withColumn(col, F.floor(F.col(col)))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# rounding down time-based columns:\n",
    "cols_to_floor = [\n",
    "    \"average_time_to_next_commercial_holiday\",\n",
    "    \"average_time_to_next_bank_holiday\",\n",
    "    \"average_time_between_purchases\",\n",
    "]\n",
    "\n",
    "for col in cols_to_floor:\n",
    "\n",
    "    df_customer = df_customer.withColumn(col, F.floor(F.col(col)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 34;\n",
       "                var nbb_unformatted_code = \"# selecting the column order:\\ndf_customer = (df_customer\\n                  .select(*col_order))\";\n",
       "                var nbb_formatted_code = \"# selecting the column order:\\ndf_customer = df_customer.select(*col_order)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# selecting the column order:\n",
    "df_customer = df_customer.select(*col_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- customer_country: string (nullable = true)\n",
      " |-- first_purchase_date: date (nullable = true)\n",
      " |-- is_foreign: boolean (nullable = false)\n",
      " |-- account_age_days: integer (nullable = true)\n",
      " |-- recency: integer (nullable = true)\n",
      " |-- n_orders: long (nullable = true)\n",
      " |-- gross_revenue: double (nullable = true)\n",
      " |-- total_cancelled: double (nullable = true)\n",
      " |-- frequency: double (nullable = true)\n",
      " |-- monetary_value: double (nullable = true)\n",
      " |-- average_ticket: double (nullable = true)\n",
      " |-- is_considered_reseller: boolean (nullable = true)\n",
      " |-- average_time_between_purchases: long (nullable = true)\n",
      " |-- average_time_to_next_bank_holiday: long (nullable = true)\n",
      " |-- average_time_to_next_commercial_holiday: long (nullable = true)\n",
      " |-- month_most_active: integer (nullable = true)\n",
      " |-- week_most_active: integer (nullable = true)\n",
      " |-- day_of_week_most_active: integer (nullable = true)\n",
      " |-- day_of_month_most_active: integer (nullable = true)\n",
      " |-- average_basket_size: double (nullable = true)\n",
      " |-- average_basket_diversity: double (nullable = true)\n",
      " |-- total_items: integer (nullable = true)\n",
      " |-- total_cancelled_items: long (nullable = true)\n",
      " |-- total_free_items: long (nullable = true)\n",
      " |-- total_returned_items: long (nullable = true)\n",
      " |-- total_sale_items: long (nullable = true)\n",
      " |-- total_discounts_received: double (nullable = true)\n",
      " |-- total_paid_fees: double (nullable = true)\n",
      " |-- total_paid_manual: double (nullable = true)\n",
      " |-- total_paid_postage: double (nullable = true)\n",
      " |-- total_paid_returned: double (nullable = true)\n",
      " |-- total_paid_sale: double (nullable = true)\n",
      " |-- total_units_cancelled: double (nullable = true)\n",
      " |-- total_units_free: double (nullable = true)\n",
      " |-- total_units_returned: double (nullable = true)\n",
      " |-- total_units_sale: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 35;\n",
       "                var nbb_unformatted_code = \"# verifying the schema:\\ndf_customer.printSchema()\";\n",
       "                var nbb_formatted_code = \"# verifying the schema:\\ndf_customer.printSchema()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# verifying the schema:\n",
    "df_customer.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Saving the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the enhanced raw data as parquet in the processed step of the pipeline\n",
    "PROCESSED_DATA_DIR = '../data/processed'\n",
    "\n",
    "\n",
    "# using the helper function to save the file:\n",
    "save_to_filesystem(df_customer, \n",
    "                   PROCESSED_DATA_DIR,\n",
    "                   'tb_customer',\n",
    "                   'tb_customer.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
